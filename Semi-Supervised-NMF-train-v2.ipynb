{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All Method\n",
    "\n",
    "Train NMF for each topic separately.\n",
    "\n",
    "Use all Wiki articles as Background Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "import pickle\n",
    "\n",
    "import libs.text_preprocess as tp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - Doc \" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, theme, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print(color.CYAN + color.BOLD + categories[theme] + color.END)\n",
    "    for topic_idx, topic in enumerate(model[theme].components_):\n",
    "        if topic_idx / n_topics == 1:\n",
    "            break\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_train_results(doc_topic, doc, corpus, data):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_train_doc_topics(data, doc_topic, doc, 11) \n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "def print_test_results(doc_topic, doc, corpus):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    \n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_test_doc_topics(doc_topic, doc, 11)\n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm\n",
    "\n",
    "def export_to_excel(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put xlsx as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories)\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_excel(filepath)\n",
    "    return df\n",
    "\n",
    "def export_to_csv(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put csv as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories)\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data = data.sort_values('theme.id')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def extract_corpus(data):    \n",
    "    corpus = list(data['text'])\n",
    "    return corpus\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, extra_process = ['stem'])['TF'])) for doc in corpus]\n",
    "    return PPcorpus\n",
    "\n",
    "def train_corpus(corpus, data, brown_corpus, n_topics=3, betaloss = 'kullback-leibler', bckg_brown = False):\n",
    "    N = len(data)\n",
    "    \n",
    "    theme_counts = data.groupby(['theme.id','theme']).count().iloc[:,1]\n",
    "    pd_theme_counts = pd.DataFrame(theme_counts)\n",
    "    n_themes = len(theme_counts)\n",
    "    \n",
    "    n_top_words = 5\n",
    "    n_components = n_topics*(n_themes)\n",
    "    \n",
    "    \n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer() # optionally add maxfeatures = n_features to enforce number of features\n",
    "    t0 = time()\n",
    "    \n",
    "    W_list = []\n",
    "    \n",
    "    if bckg_brown:\n",
    "        tfidf = tfidf_vectorizer.fit_transform(corpus+brown_corpus)\n",
    "        tc_sum = 0\n",
    "        for tc in theme_counts:\n",
    "            W = np.zeros((N+len(brown_corpus),2*n_topics))\n",
    "            W[N:, n_topics:] = np.random.random((len(brown_corpus),n_topics))\n",
    "            W[tc_sum:tc_sum+tc, :] = np.random.random((tc,2*n_topics))\n",
    "\n",
    "            tc_sum += tc\n",
    "            W_list.append(W)\n",
    "    else:\n",
    "        tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "        tc_sum = 0\n",
    "        for tc in theme_counts:\n",
    "            W = np.zeros((N,2*n_topics))\n",
    "            W[:, n_topics:] = np.random.random((N,n_topics))\n",
    "            W[tc_sum:tc_sum+tc, :n_topics] = np.random.random((tc,n_topics))\n",
    "\n",
    "            tc_sum += tc\n",
    "            W_list.append(W)\n",
    "        \n",
    "    n_features = tfidf.shape[1]\n",
    "    print(n_features)\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    X = tfidf \n",
    "    nmf_list = []\n",
    "\n",
    "    for i, W in enumerate(W_list):\n",
    "        print(\"Fitting NMF for \" + str(theme_counts.index[i][1]))\n",
    "        t0 = time()\n",
    "        H = np.random.rand(2*n_topics, n_features)\n",
    "\n",
    "        nmf = NMF(n_components= 2*n_topics, solver='mu', beta_loss=betaloss,\n",
    "                  alpha=.1, l1_ratio=.5, init = 'custom')\n",
    "\n",
    "        nmf.fit_transform(X=X,W=W,H=H)\n",
    "        print(\"done in %0.2fs.\" % (time() - t0))\n",
    "\n",
    "        nmf_list.append(nmf)\n",
    "    \n",
    "    \n",
    "    return nmf_list, W_list, tfidf, tfidf_vectorizer\n",
    "    \n",
    "def evaluate_docs(docs, nmf, tfidf_test, betaloss = 'kullback-leibler'):\n",
    "    X_test = tfidf_test\n",
    "    H_test = nmf.components_\n",
    "    \n",
    "    # Fit the NMF model\n",
    "    t0 = time()\n",
    "\n",
    "    W_test = nmf.transform(X_test)\n",
    "    \n",
    "    return W_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix bad wording:  0.006991863250732422 s\n",
      "Tokenize:  0.012935161590576172 s\n",
      "Remove stopwords and Lemmatize:  1.8916518688201904 s\n",
      "\n",
      "Fix bad wording:  0.0039730072021484375 s\n",
      "Tokenize:  0.013935327529907227 s\n",
      "Remove stopwords and Lemmatize:  0.02895379066467285 s\n",
      "\n",
      "Fix bad wording:  0.0039615631103515625 s\n",
      "Tokenize:  0.0059816837310791016 s\n",
      "Remove stopwords and Lemmatize:  0.015967845916748047 s\n",
      "\n",
      "Fix bad wording:  0.006997823715209961 s\n",
      "Tokenize:  0.012973308563232422 s\n",
      "Remove stopwords and Lemmatize:  0.03392839431762695 s\n",
      "\n",
      "Fix bad wording:  0.0019452571868896484 s\n",
      "Tokenize:  0.0030264854431152344 s\n",
      "Remove stopwords and Lemmatize:  0.08374190330505371 s\n",
      "\n",
      "Fix bad wording:  0.012964963912963867 s\n",
      "Tokenize:  0.025928974151611328 s\n",
      "Remove stopwords and Lemmatize:  0.0498960018157959 s\n",
      "\n",
      "Fix bad wording:  0.000995635986328125 s\n",
      "Tokenize:  0.0029985904693603516 s\n",
      "Remove stopwords and Lemmatize:  0.005976676940917969 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.0019953250885009766 s\n",
      "\n",
      "Fix bad wording:  0.0019674301147460938 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.010999202728271484 s\n",
      "\n",
      "Fix bad wording:  0.0049855709075927734 s\n",
      "Tokenize:  0.009973764419555664 s\n",
      "Remove stopwords and Lemmatize:  0.023933887481689453 s\n",
      "\n",
      "Fix bad wording:  0.05382990837097168 s\n",
      "Tokenize:  0.10771059989929199 s\n",
      "Remove stopwords and Lemmatize:  0.2742938995361328 s\n",
      "\n",
      "Fix bad wording:  0.02642989158630371 s\n",
      "Tokenize:  0.03886890411376953 s\n",
      "Remove stopwords and Lemmatize:  0.12268352508544922 s\n",
      "\n",
      "Fix bad wording:  0.0020062923431396484 s\n",
      "Tokenize:  0.003966808319091797 s\n",
      "Remove stopwords and Lemmatize:  0.009984731674194336 s\n",
      "\n",
      "Fix bad wording:  0.005972623825073242 s\n",
      "Tokenize:  0.011969327926635742 s\n",
      "Remove stopwords and Lemmatize:  0.03291130065917969 s\n",
      "\n",
      "Fix bad wording:  0.0030002593994140625 s\n",
      "Tokenize:  0.004952669143676758 s\n",
      "Remove stopwords and Lemmatize:  0.009991645812988281 s\n",
      "\n",
      "Fix bad wording:  0.0009701251983642578 s\n",
      "Tokenize:  0.0029897689819335938 s\n",
      "Remove stopwords and Lemmatize:  0.006980419158935547 s\n",
      "\n",
      "Fix bad wording:  0.021265506744384766 s\n",
      "Tokenize:  0.03690505027770996 s\n",
      "Remove stopwords and Lemmatize:  0.09627127647399902 s\n",
      "\n",
      "Fix bad wording:  0.0029664039611816406 s\n",
      "Tokenize:  0.006011009216308594 s\n",
      "Remove stopwords and Lemmatize:  0.017934560775756836 s\n",
      "\n",
      "Fix bad wording:  0.0010046958923339844 s\n",
      "Tokenize:  0.0020279884338378906 s\n",
      "Remove stopwords and Lemmatize:  0.0019736289978027344 s\n",
      "\n",
      "Fix bad wording:  0.0009951591491699219 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0029954910278320312 s\n",
      "\n",
      "Fix bad wording:  0.014928817749023438 s\n",
      "Tokenize:  0.022966861724853516 s\n",
      "Remove stopwords and Lemmatize:  0.06681990623474121 s\n",
      "\n",
      "Fix bad wording:  0.0019927024841308594 s\n",
      "Tokenize:  0.005005598068237305 s\n",
      "Remove stopwords and Lemmatize:  0.01493525505065918 s\n",
      "\n",
      "Fix bad wording:  0.004005908966064453 s\n",
      "Tokenize:  0.00895071029663086 s\n",
      "Remove stopwords and Lemmatize:  0.026904821395874023 s\n",
      "\n",
      "Fix bad wording:  0.0029647350311279297 s\n",
      "Tokenize:  0.008008718490600586 s\n",
      "Remove stopwords and Lemmatize:  0.0169522762298584 s\n",
      "\n",
      "Fix bad wording:  0.0009982585906982422 s\n",
      "Tokenize:  0.0019676685333251953 s\n",
      "Remove stopwords and Lemmatize:  0.003970623016357422 s\n",
      "\n",
      "Fix bad wording:  0.013962507247924805 s\n",
      "Tokenize:  0.02893996238708496 s\n",
      "Remove stopwords and Lemmatize:  0.06484580039978027 s\n",
      "\n",
      "Fix bad wording:  0.0009932518005371094 s\n",
      "Tokenize:  0.0029604434967041016 s\n",
      "Remove stopwords and Lemmatize:  0.006015777587890625 s\n",
      "\n",
      "Fix bad wording:  0.004953861236572266 s\n",
      "Tokenize:  0.00997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.02596139907836914 s\n",
      "\n",
      "Fix bad wording:  0.01695418357849121 s\n",
      "Tokenize:  0.02789473533630371 s\n",
      "Remove stopwords and Lemmatize:  0.09574484825134277 s\n",
      "\n",
      "Fix bad wording:  0.007008075714111328 s\n",
      "Tokenize:  0.01398324966430664 s\n",
      "Remove stopwords and Lemmatize:  0.03484010696411133 s\n",
      "\n",
      "Fix bad wording:  0.0029931068420410156 s\n",
      "Tokenize:  0.004013776779174805 s\n",
      "Remove stopwords and Lemmatize:  0.016951560974121094 s\n",
      "\n",
      "Fix bad wording:  0.005013942718505859 s\n",
      "Tokenize:  0.006984233856201172 s\n",
      "Remove stopwords and Lemmatize:  0.018921375274658203 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0010254383087158203 s\n",
      "Remove stopwords and Lemmatize:  0.0019643306732177734 s\n",
      "\n",
      "Fix bad wording:  0.0010259151458740234 s\n",
      "Tokenize:  0.004979133605957031 s\n",
      "Remove stopwords and Lemmatize:  0.008983373641967773 s\n",
      "\n",
      "Fix bad wording:  0.001999378204345703 s\n",
      "Tokenize:  0.004988431930541992 s\n",
      "Remove stopwords and Lemmatize:  0.007006406784057617 s\n",
      "\n",
      "Fix bad wording:  0.0029914379119873047 s\n",
      "Tokenize:  0.00598597526550293 s\n",
      "Remove stopwords and Lemmatize:  0.014966964721679688 s\n",
      "\n",
      "Fix bad wording:  0.007944107055664062 s\n",
      "Tokenize:  0.01496434211730957 s\n",
      "Remove stopwords and Lemmatize:  0.03589940071105957 s\n",
      "\n",
      "Fix bad wording:  0.006979942321777344 s\n",
      "Tokenize:  0.01100015640258789 s\n",
      "Remove stopwords and Lemmatize:  0.02992701530456543 s\n",
      "\n",
      "Fix bad wording:  0.010934114456176758 s\n",
      "Tokenize:  0.016984939575195312 s\n",
      "Remove stopwords and Lemmatize:  0.052337646484375 s\n",
      "\n",
      "Fix bad wording:  0.003019571304321289 s\n",
      "Tokenize:  0.004987478256225586 s\n",
      "Remove stopwords and Lemmatize:  0.015920400619506836 s\n",
      "\n",
      "Fix bad wording:  0.028946638107299805 s\n",
      "Tokenize:  0.057849884033203125 s\n",
      "Remove stopwords and Lemmatize:  0.1545853614807129 s\n",
      "\n",
      "Fix bad wording:  0.011985063552856445 s\n",
      "Tokenize:  0.023919105529785156 s\n",
      "Remove stopwords and Lemmatize:  0.06681990623474121 s\n",
      "\n",
      "Fix bad wording:  0.012967348098754883 s\n",
      "Tokenize:  0.02094268798828125 s\n",
      "Remove stopwords and Lemmatize:  0.05886554718017578 s\n",
      "\n",
      "Fix bad wording:  0.01942300796508789 s\n",
      "Tokenize:  0.03292369842529297 s\n",
      "Remove stopwords and Lemmatize:  0.08674049377441406 s\n",
      "\n",
      "Fix bad wording:  0.012992382049560547 s\n",
      "Tokenize:  0.024919748306274414 s\n",
      "Remove stopwords and Lemmatize:  0.06285262107849121 s\n",
      "\n",
      "Fix bad wording:  0.002004384994506836 s\n",
      "Tokenize:  0.0029604434967041016 s\n",
      "Remove stopwords and Lemmatize:  0.009001493453979492 s\n",
      "\n",
      "Fix bad wording:  0.002993345260620117 s\n",
      "Tokenize:  0.005982875823974609 s\n",
      "Remove stopwords and Lemmatize:  0.0159609317779541 s\n",
      "\n",
      "Fix bad wording:  0.015956401824951172 s\n",
      "Tokenize:  0.033908843994140625 s\n",
      "Remove stopwords and Lemmatize:  0.09474444389343262 s\n",
      "\n",
      "Fix bad wording:  0.0009968280792236328 s\n",
      "Tokenize:  0.0020008087158203125 s\n",
      "Remove stopwords and Lemmatize:  0.005007028579711914 s\n",
      "\n",
      "Fix bad wording:  0.0059871673583984375 s\n",
      "Tokenize:  0.012964248657226562 s\n",
      "Remove stopwords and Lemmatize:  0.035875797271728516 s\n",
      "\n",
      "Fix bad wording:  0.001993894577026367 s\n",
      "Tokenize:  0.001024484634399414 s\n",
      "Remove stopwords and Lemmatize:  0.008983373641967773 s\n",
      "\n",
      "Fix bad wording:  0.0010013580322265625 s\n",
      "Tokenize:  0.00298309326171875 s\n",
      "Remove stopwords and Lemmatize:  0.0049855709075927734 s\n",
      "\n",
      "Fix bad wording:  0.0039937496185302734 s\n",
      "Tokenize:  0.005980253219604492 s\n",
      "Remove stopwords and Lemmatize:  0.01692652702331543 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009944438934326172 s\n",
      "Remove stopwords and Lemmatize:  0.007976531982421875 s\n",
      "\n",
      "Fix bad wording:  0.00299072265625 s\n",
      "Tokenize:  0.007981061935424805 s\n",
      "Remove stopwords and Lemmatize:  0.021913766860961914 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.008005142211914062 s\n",
      "Remove stopwords and Lemmatize:  0.02490830421447754 s\n",
      "\n",
      "Fix bad wording:  0.0009286403656005859 s\n",
      "Tokenize:  0.0019986629486083984 s\n",
      "Remove stopwords and Lemmatize:  0.007997751235961914 s\n",
      "\n",
      "Fix bad wording:  0.006960630416870117 s\n",
      "Tokenize:  0.012943744659423828 s\n",
      "Remove stopwords and Lemmatize:  0.03393149375915527 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0019910335540771484 s\n",
      "Remove stopwords and Lemmatize:  0.002991914749145508 s\n",
      "\n",
      "Fix bad wording:  0.01394343376159668 s\n",
      "Tokenize:  0.02592015266418457 s\n",
      "Remove stopwords and Lemmatize:  0.06984114646911621 s\n",
      "\n",
      "Fix bad wording:  0.00101470947265625 s\n",
      "Tokenize:  0.001972198486328125 s\n",
      "Remove stopwords and Lemmatize:  0.005984067916870117 s\n",
      "\n",
      "Fix bad wording:  0.00952005386352539 s\n",
      "Tokenize:  0.009973764419555664 s\n",
      "Remove stopwords and Lemmatize:  0.023972749710083008 s\n",
      "\n",
      "Fix bad wording:  0.00595545768737793 s\n",
      "Tokenize:  0.013989925384521484 s\n",
      "Remove stopwords and Lemmatize:  0.0338900089263916 s\n",
      "\n",
      "Fix bad wording:  0.004986286163330078 s\n",
      "Tokenize:  0.005985260009765625 s\n",
      "Remove stopwords and Lemmatize:  0.014990806579589844 s\n",
      "\n",
      "Fix bad wording:  0.0039577484130859375 s\n",
      "Tokenize:  0.00598454475402832 s\n",
      "Remove stopwords and Lemmatize:  0.015956878662109375 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0020203590393066406 s\n",
      "\n",
      "Fix bad wording:  0.0019931793212890625 s\n",
      "Tokenize:  0.001999378204345703 s\n",
      "Remove stopwords and Lemmatize:  0.006951093673706055 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.00997304916381836 s\n",
      "\n",
      "Fix bad wording:  0.008005380630493164 s\n",
      "Tokenize:  0.016927003860473633 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.06284451484680176 s\n",
      "\n",
      "Fix bad wording:  0.005972385406494141 s\n",
      "Tokenize:  0.010000228881835938 s\n",
      "Remove stopwords and Lemmatize:  0.02590203285217285 s\n",
      "\n",
      "Fix bad wording:  0.004987478256225586 s\n",
      "Tokenize:  0.009961605072021484 s\n",
      "Remove stopwords and Lemmatize:  0.02490234375 s\n",
      "\n",
      "Fix bad wording:  0.008983135223388672 s\n",
      "Tokenize:  0.014946222305297852 s\n",
      "Remove stopwords and Lemmatize:  0.04307365417480469 s\n",
      "\n",
      "Fix bad wording:  0.022968769073486328 s\n",
      "Tokenize:  0.044850826263427734 s\n",
      "Remove stopwords and Lemmatize:  0.12521839141845703 s\n",
      "\n",
      "Fix bad wording:  0.005954265594482422 s\n",
      "Tokenize:  0.008975744247436523 s\n",
      "Remove stopwords and Lemmatize:  0.025945663452148438 s\n",
      "\n",
      "Fix bad wording:  0.000982522964477539 s\n",
      "Tokenize:  0.0029990673065185547 s\n",
      "Remove stopwords and Lemmatize:  0.008999347686767578 s\n",
      "\n",
      "Fix bad wording:  0.003985404968261719 s\n",
      "Tokenize:  0.009947538375854492 s\n",
      "Remove stopwords and Lemmatize:  0.029919862747192383 s\n",
      "\n",
      "Fix bad wording:  0.0019989013671875 s\n",
      "Tokenize:  0.0030171871185302734 s\n",
      "Remove stopwords and Lemmatize:  0.005953311920166016 s\n",
      "\n",
      "Fix bad wording:  0.003989696502685547 s\n",
      "Tokenize:  0.0070095062255859375 s\n",
      "Remove stopwords and Lemmatize:  0.014933109283447266 s\n",
      "\n",
      "Fix bad wording:  0.004002094268798828 s\n",
      "Tokenize:  0.007992982864379883 s\n",
      "Remove stopwords and Lemmatize:  0.016956806182861328 s\n",
      "\n",
      "Fix bad wording:  0.007949113845825195 s\n",
      "Tokenize:  0.014959335327148438 s\n",
      "Remove stopwords and Lemmatize:  0.03892850875854492 s\n",
      "\n",
      "Fix bad wording:  0.002986431121826172 s\n",
      "Tokenize:  0.005985260009765625 s\n",
      "Remove stopwords and Lemmatize:  0.012964010238647461 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.000980377197265625 s\n",
      "Remove stopwords and Lemmatize:  0.003010988235473633 s\n",
      "\n",
      "Fix bad wording:  0.002963542938232422 s\n",
      "Tokenize:  0.004016876220703125 s\n",
      "Remove stopwords and Lemmatize:  0.016925811767578125 s\n",
      "\n",
      "Fix bad wording:  0.010973453521728516 s\n",
      "Tokenize:  0.017985105514526367 s\n",
      "Remove stopwords and Lemmatize:  0.03091144561767578 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009942054748535156 s\n",
      "Remove stopwords and Lemmatize:  0.001993894577026367 s\n",
      "\n",
      "Fix bad wording:  0.019949913024902344 s\n",
      "Tokenize:  0.03986787796020508 s\n",
      "Remove stopwords and Lemmatize:  0.10274410247802734 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.006981372833251953 s\n",
      "\n",
      "Fix bad wording:  0.0059583187103271484 s\n",
      "Tokenize:  0.010001420974731445 s\n",
      "Remove stopwords and Lemmatize:  0.030922651290893555 s\n",
      "\n",
      "Fix bad wording:  0.0069806575775146484 s\n",
      "Tokenize:  0.009972095489501953 s\n",
      "Remove stopwords and Lemmatize:  0.028917551040649414 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.00894927978515625 s\n",
      "Remove stopwords and Lemmatize:  0.013965129852294922 s\n",
      "\n",
      "Fix bad wording:  0.004996299743652344 s\n",
      "Tokenize:  0.013926267623901367 s\n",
      "Remove stopwords and Lemmatize:  0.02894878387451172 s\n",
      "\n",
      "Fix bad wording:  0.006986379623413086 s\n",
      "Tokenize:  0.010939359664916992 s\n",
      "Remove stopwords and Lemmatize:  0.02695441246032715 s\n",
      "\n",
      "Fix bad wording:  0.0029964447021484375 s\n",
      "Tokenize:  0.0049839019775390625 s\n",
      "Remove stopwords and Lemmatize:  0.011965751647949219 s\n",
      "\n",
      "Fix bad wording:  0.0010001659393310547 s\n",
      "Tokenize:  0.0019752979278564453 s\n",
      "Remove stopwords and Lemmatize:  0.005005836486816406 s\n",
      "\n",
      "Fix bad wording:  0.011968851089477539 s\n",
      "Tokenize:  0.01894855499267578 s\n",
      "Remove stopwords and Lemmatize:  0.05981111526489258 s\n",
      "\n",
      "Fix bad wording:  0.006997585296630859 s\n",
      "Tokenize:  0.012986898422241211 s\n",
      "Remove stopwords and Lemmatize:  0.034765005111694336 s\n",
      "\n",
      "Fix bad wording:  0.003974199295043945 s\n",
      "Tokenize:  0.008960485458374023 s\n",
      "Remove stopwords and Lemmatize:  0.04092073440551758 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.0029938220977783203 s\n",
      "Remove stopwords and Lemmatize:  0.007969856262207031 s\n",
      "\n",
      "Fix bad wording:  0.01894998550415039 s\n",
      "Tokenize:  0.03985118865966797 s\n",
      "Remove stopwords and Lemmatize:  0.1017005443572998 s\n",
      "\n",
      "Fix bad wording:  0.00401759147644043 s\n",
      "Tokenize:  0.00894474983215332 s\n",
      "Remove stopwords and Lemmatize:  0.018943309783935547 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.015956878662109375 s\n",
      "\n",
      "Fix bad wording:  0.014992237091064453 s\n",
      "Tokenize:  0.024928808212280273 s\n",
      "Remove stopwords and Lemmatize:  0.06283378601074219 s\n",
      "\n",
      "Fix bad wording:  0.0009946823120117188 s\n",
      "Tokenize:  0.0008983612060546875 s\n",
      "Remove stopwords and Lemmatize:  0.003988981246948242 s\n",
      "\n",
      "Fix bad wording:  0.00900888442993164 s\n",
      "Tokenize:  0.016921043395996094 s\n",
      "Remove stopwords and Lemmatize:  0.04986763000488281 s\n",
      "\n",
      "Fix bad wording:  0.008977413177490234 s\n",
      "Tokenize:  0.01695394515991211 s\n",
      "Remove stopwords and Lemmatize:  0.0478978157043457 s\n",
      "\n",
      "Fix bad wording:  0.0059947967529296875 s\n",
      "Tokenize:  0.01096034049987793 s\n",
      "Remove stopwords and Lemmatize:  0.02593398094177246 s\n",
      "\n",
      "Fix bad wording:  0.009949445724487305 s\n",
      "Tokenize:  0.018938541412353516 s\n",
      "Remove stopwords and Lemmatize:  0.04889488220214844 s\n",
      "\n",
      "Fix bad wording:  0.00897836685180664 s\n",
      "Tokenize:  0.017953157424926758 s\n",
      "Remove stopwords and Lemmatize:  0.07979774475097656 s\n",
      "\n",
      "Fix bad wording:  0.016921043395996094 s\n",
      "Tokenize:  0.03294658660888672 s\n",
      "Remove stopwords and Lemmatize:  0.08374309539794922 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.007978677749633789 s\n",
      "\n",
      "Fix bad wording:  0.0020225048065185547 s\n",
      "Tokenize:  0.003995180130004883 s\n",
      "Remove stopwords and Lemmatize:  0.01395559310913086 s\n",
      "\n",
      "Fix bad wording:  0.0109710693359375 s\n",
      "Tokenize:  0.028896093368530273 s\n",
      "Remove stopwords and Lemmatize:  0.07083940505981445 s\n",
      "\n",
      "Fix bad wording:  0.02490973472595215 s\n",
      "Tokenize:  0.038883209228515625 s\n",
      "Remove stopwords and Lemmatize:  0.09773921966552734 s\n",
      "\n",
      "Fix bad wording:  0.011967182159423828 s\n",
      "Tokenize:  0.02094435691833496 s\n",
      "Remove stopwords and Lemmatize:  0.060346126556396484 s\n",
      "\n",
      "Fix bad wording:  0.003003358840942383 s\n",
      "Tokenize:  0.005972623825073242 s\n",
      "Remove stopwords and Lemmatize:  0.020978450775146484 s\n",
      "\n",
      "Fix bad wording:  0.012931108474731445 s\n",
      "Tokenize:  0.022965431213378906 s\n",
      "Remove stopwords and Lemmatize:  0.054854393005371094 s\n",
      "\n",
      "Fix bad wording:  0.0170443058013916 s\n",
      "Tokenize:  0.03687429428100586 s\n",
      "Remove stopwords and Lemmatize:  0.11175656318664551 s\n",
      "\n",
      "Fix bad wording:  0.0069544315338134766 s\n",
      "Tokenize:  0.01296544075012207 s\n",
      "Remove stopwords and Lemmatize:  0.04189038276672363 s\n",
      "\n",
      "Fix bad wording:  0.0060007572174072266 s\n",
      "Tokenize:  0.015973806381225586 s\n",
      "Remove stopwords and Lemmatize:  0.047841548919677734 s\n",
      "\n",
      "Fix bad wording:  0.0030219554901123047 s\n",
      "Tokenize:  0.008943557739257812 s\n",
      "Remove stopwords and Lemmatize:  0.017980337142944336 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0019931793212890625 s\n",
      "Remove stopwords and Lemmatize:  0.007955789566040039 s\n",
      "\n",
      "Fix bad wording:  0.010966062545776367 s\n",
      "Tokenize:  0.02098226547241211 s\n",
      "Remove stopwords and Lemmatize:  0.050852298736572266 s\n",
      "\n",
      "Fix bad wording:  0.0009987354278564453 s\n",
      "Tokenize:  0.0020017623901367188 s\n",
      "Remove stopwords and Lemmatize:  0.004981279373168945 s\n",
      "\n",
      "Fix bad wording:  0.001993417739868164 s\n",
      "Tokenize:  0.003965616226196289 s\n",
      "Remove stopwords and Lemmatize:  0.008996963500976562 s\n",
      "\n",
      "Fix bad wording:  0.0029952526092529297 s\n",
      "Tokenize:  0.005980491638183594 s\n",
      "Remove stopwords and Lemmatize:  0.01743793487548828 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.00197601318359375 s\n",
      "Remove stopwords and Lemmatize:  0.007012367248535156 s\n",
      "\n",
      "Fix bad wording:  0.0009918212890625 s\n",
      "Tokenize:  0.003993034362792969 s\n",
      "Remove stopwords and Lemmatize:  0.0069811344146728516 s\n",
      "\n",
      "Fix bad wording:  0.005980968475341797 s\n",
      "Tokenize:  0.009974241256713867 s\n",
      "Remove stopwords and Lemmatize:  0.024959564208984375 s\n",
      "\n",
      "Fix bad wording:  0.0019397735595703125 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.009004831314086914 s\n",
      "\n",
      "Fix bad wording:  0.0069828033447265625 s\n",
      "Tokenize:  0.011513471603393555 s\n",
      "Remove stopwords and Lemmatize:  0.025922298431396484 s\n",
      "\n",
      "Fix bad wording:  0.0010235309600830078 s\n",
      "Tokenize:  0.0029664039611816406 s\n",
      "Remove stopwords and Lemmatize:  0.004987478256225586 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.0059814453125 s\n",
      "Remove stopwords and Lemmatize:  0.014971733093261719 s\n",
      "\n",
      "Fix bad wording:  0.004056215286254883 s\n",
      "Tokenize:  0.00691533088684082 s\n",
      "Remove stopwords and Lemmatize:  0.01794719696044922 s\n",
      "\n",
      "Fix bad wording:  0.0019969940185546875 s\n",
      "Tokenize:  0.000997781753540039 s\n",
      "Remove stopwords and Lemmatize:  0.006466388702392578 s\n",
      "\n",
      "Fix bad wording:  0.006979703903198242 s\n",
      "Tokenize:  0.013962030410766602 s\n",
      "Remove stopwords and Lemmatize:  0.03792834281921387 s\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix bad wording:  0.009977340698242188 s\n",
      "Tokenize:  0.01694965362548828 s\n",
      "Remove stopwords and Lemmatize:  0.05318593978881836 s\n",
      "\n",
      "Fix bad wording:  0.022019386291503906 s\n",
      "Tokenize:  0.04088759422302246 s\n",
      "Remove stopwords and Lemmatize:  0.11369705200195312 s\n",
      "\n",
      "Fix bad wording:  0.011966466903686523 s\n",
      "Tokenize:  0.01995372772216797 s\n",
      "Remove stopwords and Lemmatize:  0.05983710289001465 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.00496220588684082 s\n",
      "Remove stopwords and Lemmatize:  0.01495981216430664 s\n",
      "\n",
      "Fix bad wording:  0.0030031204223632812 s\n",
      "Tokenize:  0.0059947967529296875 s\n",
      "Remove stopwords and Lemmatize:  0.012956619262695312 s\n",
      "\n",
      "Fix bad wording:  0.0050051212310791016 s\n",
      "Tokenize:  0.008953571319580078 s\n",
      "Remove stopwords and Lemmatize:  0.029919147491455078 s\n",
      "\n",
      "Fix bad wording:  0.0049877166748046875 s\n",
      "Tokenize:  0.005982637405395508 s\n",
      "Remove stopwords and Lemmatize:  0.014991998672485352 s\n",
      "\n",
      "Fix bad wording:  0.0009658336639404297 s\n",
      "Tokenize:  0.0019941329956054688 s\n",
      "Remove stopwords and Lemmatize:  0.003989696502685547 s\n",
      "\n",
      "Fix bad wording:  0.001998424530029297 s\n",
      "Tokenize:  0.002989530563354492 s\n",
      "Remove stopwords and Lemmatize:  0.005997657775878906 s\n",
      "\n",
      "Fix bad wording:  0.0069713592529296875 s\n",
      "Tokenize:  0.011992216110229492 s\n",
      "Remove stopwords and Lemmatize:  0.02892160415649414 s\n",
      "\n",
      "Fix bad wording:  0.001995563507080078 s\n",
      "Tokenize:  0.0029904842376708984 s\n",
      "Remove stopwords and Lemmatize:  0.01495981216430664 s\n",
      "\n",
      "Fix bad wording:  0.00999140739440918 s\n",
      "Tokenize:  0.01594066619873047 s\n",
      "Remove stopwords and Lemmatize:  0.041886329650878906 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.003990888595581055 s\n",
      "Remove stopwords and Lemmatize:  0.005011796951293945 s\n",
      "\n",
      "Fix bad wording:  0.002996206283569336 s\n",
      "Tokenize:  0.004957914352416992 s\n",
      "Remove stopwords and Lemmatize:  0.011902093887329102 s\n",
      "\n",
      "Fix bad wording:  0.001966238021850586 s\n",
      "Tokenize:  0.002997159957885742 s\n",
      "Remove stopwords and Lemmatize:  0.004958629608154297 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.0 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.0169830322265625 s\n",
      "\n",
      "Fix bad wording:  0.0010247230529785156 s\n",
      "Tokenize:  0.001001596450805664 s\n",
      "Remove stopwords and Lemmatize:  0.0069768428802490234 s\n",
      "\n",
      "Fix bad wording:  0.0019927024841308594 s\n",
      "Tokenize:  0.00399017333984375 s\n",
      "Remove stopwords and Lemmatize:  0.013963937759399414 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.0019693374633789062 s\n",
      "Remove stopwords and Lemmatize:  0.006979942321777344 s\n",
      "\n",
      "Fix bad wording:  0.0019943714141845703 s\n",
      "Tokenize:  0.0029921531677246094 s\n",
      "Remove stopwords and Lemmatize:  0.009998083114624023 s\n",
      "\n",
      "Fix bad wording:  0.004984140396118164 s\n",
      "Tokenize:  0.01194143295288086 s\n",
      "Remove stopwords and Lemmatize:  0.033896684646606445 s\n",
      "\n",
      "Fix bad wording:  0.0010251998901367188 s\n",
      "Tokenize:  0.0019922256469726562 s\n",
      "Remove stopwords and Lemmatize:  0.003991365432739258 s\n",
      "\n",
      "Fix bad wording:  0.007967472076416016 s\n",
      "Tokenize:  0.008006572723388672 s\n",
      "Remove stopwords and Lemmatize:  0.03390622138977051 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.00798344612121582 s\n",
      "Remove stopwords and Lemmatize:  0.017964601516723633 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009891986846923828 s\n",
      "Remove stopwords and Lemmatize:  0.0029909610748291016 s\n",
      "\n",
      "Fix bad wording:  0.0010271072387695312 s\n",
      "Tokenize:  0.0039594173431396484 s\n",
      "Remove stopwords and Lemmatize:  0.007990121841430664 s\n",
      "\n",
      "Fix bad wording:  0.006958723068237305 s\n",
      "Tokenize:  0.011963367462158203 s\n",
      "Remove stopwords and Lemmatize:  0.043913841247558594 s\n",
      "\n",
      "Fix bad wording:  0.005953073501586914 s\n",
      "Tokenize:  0.013957500457763672 s\n",
      "Remove stopwords and Lemmatize:  0.03449892997741699 s\n",
      "\n",
      "Fix bad wording:  0.003964424133300781 s\n",
      "Tokenize:  0.007991552352905273 s\n",
      "Remove stopwords and Lemmatize:  0.02092719078063965 s\n",
      "\n",
      "Fix bad wording:  0.006024837493896484 s\n",
      "Tokenize:  0.00696873664855957 s\n",
      "Remove stopwords and Lemmatize:  0.023907899856567383 s\n",
      "\n",
      "Fix bad wording:  0.00698089599609375 s\n",
      "Tokenize:  0.010997533798217773 s\n",
      "Remove stopwords and Lemmatize:  0.03490710258483887 s\n",
      "\n",
      "Fix bad wording:  0.00498652458190918 s\n",
      "Tokenize:  0.008949041366577148 s\n",
      "Remove stopwords and Lemmatize:  0.025969982147216797 s\n",
      "\n",
      "Fix bad wording:  0.008987665176391602 s\n",
      "Tokenize:  0.020931482315063477 s\n",
      "Remove stopwords and Lemmatize:  0.04687643051147461 s\n",
      "\n",
      "Fix bad wording:  0.005969047546386719 s\n",
      "Tokenize:  0.010972023010253906 s\n",
      "Remove stopwords and Lemmatize:  0.030945777893066406 s\n",
      "\n",
      "Fix bad wording:  0.004209041595458984 s\n",
      "Tokenize:  0.00695347785949707 s\n",
      "Remove stopwords and Lemmatize:  0.024960041046142578 s\n",
      "\n",
      "Fix bad wording:  0.009947061538696289 s\n",
      "Tokenize:  0.019977807998657227 s\n",
      "Remove stopwords and Lemmatize:  0.05381631851196289 s\n",
      "\n",
      "Fix bad wording:  0.0039904117584228516 s\n",
      "Tokenize:  0.00795292854309082 s\n",
      "Remove stopwords and Lemmatize:  0.01895284652709961 s\n",
      "\n",
      "Fix bad wording:  0.008949041366577148 s\n",
      "Tokenize:  0.018955707550048828 s\n",
      "Remove stopwords and Lemmatize:  0.05286121368408203 s\n",
      "\n",
      "Fix bad wording:  0.004011631011962891 s\n",
      "Tokenize:  0.00498199462890625 s\n",
      "Remove stopwords and Lemmatize:  0.017934322357177734 s\n",
      "\n",
      "Fix bad wording:  0.010961532592773438 s\n",
      "Tokenize:  0.014990091323852539 s\n",
      "Remove stopwords and Lemmatize:  0.03789544105529785 s\n",
      "\n",
      "Fix bad wording:  0.005989551544189453 s\n",
      "Tokenize:  0.008943319320678711 s\n",
      "Remove stopwords and Lemmatize:  0.024967432022094727 s\n",
      "\n",
      "Fix bad wording:  0.01592254638671875 s\n",
      "Tokenize:  0.02593088150024414 s\n",
      "Remove stopwords and Lemmatize:  0.07876324653625488 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.004014730453491211 s\n",
      "\n",
      "Fix bad wording:  0.0010006427764892578 s\n",
      "Tokenize:  0.0009708404541015625 s\n",
      "Remove stopwords and Lemmatize:  0.0039861202239990234 s\n",
      "\n",
      "Fix bad wording:  0.0030286312103271484 s\n",
      "Tokenize:  0.005987405776977539 s\n",
      "Remove stopwords and Lemmatize:  0.016950368881225586 s\n",
      "\n",
      "Fix bad wording:  0.0019898414611816406 s\n",
      "Tokenize:  0.0039920806884765625 s\n",
      "Remove stopwords and Lemmatize:  0.009973287582397461 s\n",
      "\n",
      "Fix bad wording:  0.004956960678100586 s\n",
      "Tokenize:  0.009002923965454102 s\n",
      "Remove stopwords and Lemmatize:  0.02493453025817871 s\n",
      "\n",
      "Fix bad wording:  0.0059833526611328125 s\n",
      "Tokenize:  0.009973287582397461 s\n",
      "Remove stopwords and Lemmatize:  0.026900291442871094 s\n",
      "\n",
      "Fix bad wording:  0.006979942321777344 s\n",
      "Tokenize:  0.008989334106445312 s\n",
      "Remove stopwords and Lemmatize:  0.029919147491455078 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0019686222076416016 s\n",
      "Remove stopwords and Lemmatize:  0.003017425537109375 s\n",
      "\n",
      "Fix bad wording:  0.003990650177001953 s\n",
      "Tokenize:  0.010943412780761719 s\n",
      "Remove stopwords and Lemmatize:  0.02646613121032715 s\n",
      "\n",
      "Fix bad wording:  0.005964517593383789 s\n",
      "Tokenize:  0.012981653213500977 s\n",
      "Remove stopwords and Lemmatize:  0.03188633918762207 s\n",
      "\n",
      "Fix bad wording:  0.002019166946411133 s\n",
      "Tokenize:  0.0029904842376708984 s\n",
      "Remove stopwords and Lemmatize:  0.011939048767089844 s\n",
      "\n",
      "Fix bad wording:  0.0010223388671875 s\n",
      "Tokenize:  0.0009963512420654297 s\n",
      "Remove stopwords and Lemmatize:  0.0039632320404052734 s\n",
      "\n",
      "Fix bad wording:  0.007013082504272461 s\n",
      "Tokenize:  0.009972095489501953 s\n",
      "Remove stopwords and Lemmatize:  0.029954195022583008 s\n",
      "\n",
      "Fix bad wording:  0.001992940902709961 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.014939069747924805 s\n",
      "\n",
      "Fix bad wording:  0.007010221481323242 s\n",
      "Tokenize:  0.010971546173095703 s\n",
      "Remove stopwords and Lemmatize:  0.03488349914550781 s\n",
      "\n",
      "Fix bad wording:  0.002018451690673828 s\n",
      "Tokenize:  0.003989696502685547 s\n",
      "Remove stopwords and Lemmatize:  0.013960123062133789 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.0029723644256591797 s\n",
      "Remove stopwords and Lemmatize:  0.007971525192260742 s\n",
      "\n",
      "Fix bad wording:  0.00598454475402832 s\n",
      "Tokenize:  0.008974552154541016 s\n",
      "Remove stopwords and Lemmatize:  0.025959014892578125 s\n",
      "\n",
      "Fix bad wording:  0.004959583282470703 s\n",
      "Tokenize:  0.007015705108642578 s\n",
      "Remove stopwords and Lemmatize:  0.019939661026000977 s\n",
      "\n",
      "Fix bad wording:  0.0019960403442382812 s\n",
      "Tokenize:  0.005014181137084961 s\n",
      "Remove stopwords and Lemmatize:  0.01593017578125 s\n",
      "\n",
      "Fix bad wording:  0.0029926300048828125 s\n",
      "Tokenize:  0.003010988235473633 s\n",
      "Remove stopwords and Lemmatize:  0.009972572326660156 s\n",
      "\n",
      "Fix bad wording:  0.004999637603759766 s\n",
      "Tokenize:  0.006953716278076172 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.0179750919342041 s\n",
      "\n",
      "Fix bad wording:  0.002963542938232422 s\n",
      "Tokenize:  0.005984067916870117 s\n",
      "Remove stopwords and Lemmatize:  0.014904260635375977 s\n",
      "\n",
      "Fix bad wording:  0.0029659271240234375 s\n",
      "Tokenize:  0.0070078372955322266 s\n",
      "Remove stopwords and Lemmatize:  0.02300262451171875 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.003989458084106445 s\n",
      "\n",
      "Fix bad wording:  0.009947061538696289 s\n",
      "Tokenize:  0.015985488891601562 s\n",
      "Remove stopwords and Lemmatize:  0.044851064682006836 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0009980201721191406 s\n",
      "Remove stopwords and Lemmatize:  0.003987550735473633 s\n",
      "\n",
      "Fix bad wording:  0.010974407196044922 s\n",
      "Tokenize:  0.01695108413696289 s\n",
      "Remove stopwords and Lemmatize:  0.0428164005279541 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.003000497817993164 s\n",
      "Remove stopwords and Lemmatize:  0.006946086883544922 s\n",
      "\n",
      "Fix bad wording:  0.003989696502685547 s\n",
      "Tokenize:  0.006980419158935547 s\n",
      "Remove stopwords and Lemmatize:  0.018961429595947266 s\n",
      "\n",
      "Fix bad wording:  0.0010225772857666016 s\n",
      "Tokenize:  0.0009970664978027344 s\n",
      "Remove stopwords and Lemmatize:  0.004986763000488281 s\n",
      "\n",
      "Fix bad wording:  0.0019686222076416016 s\n",
      "Tokenize:  0.004024982452392578 s\n",
      "Remove stopwords and Lemmatize:  0.010961294174194336 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.006983518600463867 s\n",
      "\n",
      "Fix bad wording:  0.019950151443481445 s\n",
      "Tokenize:  0.03387618064880371 s\n",
      "Remove stopwords and Lemmatize:  0.09577155113220215 s\n",
      "\n",
      "Fix bad wording:  0.011995077133178711 s\n",
      "Tokenize:  0.020917654037475586 s\n",
      "Remove stopwords and Lemmatize:  0.055852413177490234 s\n",
      "\n",
      "Fix bad wording:  0.004987955093383789 s\n",
      "Tokenize:  0.006978273391723633 s\n",
      "Remove stopwords and Lemmatize:  0.018922090530395508 s\n",
      "\n",
      "Fix bad wording:  0.012990474700927734 s\n",
      "Tokenize:  0.023909807205200195 s\n",
      "Remove stopwords and Lemmatize:  0.06284904479980469 s\n",
      "\n",
      "Fix bad wording:  0.0029892921447753906 s\n",
      "Tokenize:  0.007978200912475586 s\n",
      "Remove stopwords and Lemmatize:  0.014960050582885742 s\n",
      "\n",
      "Fix bad wording:  0.00501251220703125 s\n",
      "Tokenize:  0.00895071029663086 s\n",
      "Remove stopwords and Lemmatize:  0.027924060821533203 s\n",
      "\n",
      "Fix bad wording:  0.004004716873168945 s\n",
      "Tokenize:  0.008960723876953125 s\n",
      "Remove stopwords and Lemmatize:  0.02097606658935547 s\n",
      "\n",
      "Fix bad wording:  0.004971981048583984 s\n",
      "Tokenize:  0.006955862045288086 s\n",
      "Remove stopwords and Lemmatize:  0.022934913635253906 s\n",
      "\n",
      "Fix bad wording:  0.00900888442993164 s\n",
      "Tokenize:  0.013959407806396484 s\n",
      "Remove stopwords and Lemmatize:  0.03986167907714844 s\n",
      "\n",
      "Fix bad wording:  0.00801396369934082 s\n",
      "Tokenize:  0.014959573745727539 s\n",
      "Remove stopwords and Lemmatize:  0.04291367530822754 s\n",
      "\n",
      "Fix bad wording:  0.004994869232177734 s\n",
      "Tokenize:  0.007969141006469727 s\n",
      "Remove stopwords and Lemmatize:  0.020945310592651367 s\n",
      "\n",
      "Fix bad wording:  0.003962039947509766 s\n",
      "Tokenize:  0.0070002079010009766 s\n",
      "Remove stopwords and Lemmatize:  0.015967130661010742 s\n",
      "\n",
      "Fix bad wording:  0.007989883422851562 s\n",
      "Tokenize:  0.014980316162109375 s\n",
      "Remove stopwords and Lemmatize:  0.03890204429626465 s\n",
      "\n",
      "Fix bad wording:  0.0009620189666748047 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.0020341873168945312 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.001985311508178711 s\n",
      "\n",
      "Fix bad wording:  0.0049839019775390625 s\n",
      "Tokenize:  0.008008003234863281 s\n",
      "Remove stopwords and Lemmatize:  0.023961544036865234 s\n",
      "\n",
      "Fix bad wording:  0.010945320129394531 s\n",
      "Tokenize:  0.01895308494567871 s\n",
      "Remove stopwords and Lemmatize:  0.04587268829345703 s\n",
      "\n",
      "Fix bad wording:  0.00401616096496582 s\n",
      "Tokenize:  0.006955146789550781 s\n",
      "Remove stopwords and Lemmatize:  0.02493119239807129 s\n",
      "\n",
      "Fix bad wording:  0.008013486862182617 s\n",
      "Tokenize:  0.012997150421142578 s\n",
      "Remove stopwords and Lemmatize:  0.04089522361755371 s\n",
      "\n",
      "Fix bad wording:  0.000990152359008789 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.000995635986328125 s\n",
      "\n",
      "Fix bad wording:  0.0009706020355224609 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.000995635986328125 s\n",
      "\n",
      "Fix bad wording:  0.00099945068359375 s\n",
      "Tokenize:  0.0009975433349609375 s\n",
      "Remove stopwords and Lemmatize:  0.003962993621826172 s\n",
      "\n",
      "Fix bad wording:  0.00501561164855957 s\n",
      "Tokenize:  0.007978439331054688 s\n",
      "Remove stopwords and Lemmatize:  0.021916627883911133 s\n",
      "\n",
      "Fix bad wording:  0.001990795135498047 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.00899362564086914 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009908676147460938 s\n",
      "Remove stopwords and Lemmatize:  0.002010822296142578 s\n",
      "\n",
      "Fix bad wording:  0.0009982585906982422 s\n",
      "Tokenize:  0.0019953250885009766 s\n",
      "Remove stopwords and Lemmatize:  0.004986763000488281 s\n",
      "\n",
      "Fix bad wording:  0.005983829498291016 s\n",
      "Tokenize:  0.011950492858886719 s\n",
      "Remove stopwords and Lemmatize:  0.03396725654602051 s\n",
      "\n",
      "Fix bad wording:  0.002973794937133789 s\n",
      "Tokenize:  0.004977226257324219 s\n",
      "Remove stopwords and Lemmatize:  0.010997533798217773 s\n",
      "\n",
      "Fix bad wording:  0.009975910186767578 s\n",
      "Tokenize:  0.016953706741333008 s\n",
      "Remove stopwords and Lemmatize:  0.049865007400512695 s\n",
      "\n",
      "Fix bad wording:  0.0009992122650146484 s\n",
      "Tokenize:  0.0015048980712890625 s\n",
      "Remove stopwords and Lemmatize:  0.004987478256225586 s\n",
      "\n",
      "Fix bad wording:  0.00800943374633789 s\n",
      "Tokenize:  0.01495504379272461 s\n",
      "Remove stopwords and Lemmatize:  0.03989672660827637 s\n",
      "\n",
      "Fix bad wording:  0.0019941329956054688 s\n",
      "Tokenize:  0.006978511810302734 s\n",
      "Remove stopwords and Lemmatize:  0.01194143295288086 s\n",
      "\n",
      "Fix bad wording:  0.009982824325561523 s\n",
      "Tokenize:  0.01697254180908203 s\n",
      "Remove stopwords and Lemmatize:  0.04694771766662598 s\n",
      "\n",
      "Fix bad wording:  0.014966249465942383 s\n",
      "Tokenize:  0.024899959564208984 s\n",
      "Remove stopwords and Lemmatize:  0.07782196998596191 s\n",
      "\n",
      "Fix bad wording:  0.001993417739868164 s\n",
      "Tokenize:  0.0019953250885009766 s\n",
      "Remove stopwords and Lemmatize:  0.00997018814086914 s\n",
      "\n",
      "Fix bad wording:  0.003991603851318359 s\n",
      "Tokenize:  0.006980419158935547 s\n",
      "Remove stopwords and Lemmatize:  0.01795053482055664 s\n",
      "\n",
      "Fix bad wording:  0.0030176639556884766 s\n",
      "Tokenize:  0.003964900970458984 s\n",
      "Remove stopwords and Lemmatize:  0.014933347702026367 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0030014514923095703 s\n",
      "\n",
      "Fix bad wording:  0.0029778480529785156 s\n",
      "Tokenize:  0.0059986114501953125 s\n",
      "Remove stopwords and Lemmatize:  0.018921852111816406 s\n",
      "\n",
      "Fix bad wording:  0.002022266387939453 s\n",
      "Tokenize:  0.003987789154052734 s\n",
      "Remove stopwords and Lemmatize:  0.013936042785644531 s\n",
      "\n",
      "Fix bad wording:  0.002023935317993164 s\n",
      "Tokenize:  0.002963542938232422 s\n",
      "Remove stopwords and Lemmatize:  0.009003162384033203 s\n",
      "\n",
      "Fix bad wording:  0.0009984970092773438 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0019953250885009766 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009937286376953125 s\n",
      "Remove stopwords and Lemmatize:  0.0 s\n",
      "\n",
      "Fix bad wording:  0.007009029388427734 s\n",
      "Tokenize:  0.012938737869262695 s\n",
      "Remove stopwords and Lemmatize:  0.03390097618103027 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0009968280792236328 s\n",
      "\n",
      "Fix bad wording:  0.012965202331542969 s\n",
      "Tokenize:  0.024906635284423828 s\n",
      "Remove stopwords and Lemmatize:  0.06587052345275879 s\n",
      "\n",
      "Fix bad wording:  0.008975744247436523 s\n",
      "Tokenize:  0.016907453536987305 s\n",
      "Remove stopwords and Lemmatize:  0.05685019493103027 s\n",
      "\n",
      "Fix bad wording:  0.002996206283569336 s\n",
      "Tokenize:  0.003999471664428711 s\n",
      "Remove stopwords and Lemmatize:  0.00897216796875 s\n",
      "\n",
      "Fix bad wording:  0.020939350128173828 s\n",
      "Tokenize:  0.032898902893066406 s\n",
      "Remove stopwords and Lemmatize:  0.09875297546386719 s\n",
      "\n",
      "Fix bad wording:  0.006945610046386719 s\n",
      "Tokenize:  0.00901341438293457 s\n",
      "Remove stopwords and Lemmatize:  0.029912471771240234 s\n",
      "\n",
      "Fix bad wording:  0.008945703506469727 s\n",
      "Tokenize:  0.010999917984008789 s\n",
      "Remove stopwords and Lemmatize:  0.036872148513793945 s\n",
      "\n",
      "Fix bad wording:  0.002022266387939453 s\n",
      "Tokenize:  0.003990650177001953 s\n",
      "Remove stopwords and Lemmatize:  0.008947134017944336 s\n",
      "\n",
      "Fix bad wording:  0.005023479461669922 s\n",
      "Tokenize:  0.0059833526611328125 s\n",
      "Remove stopwords and Lemmatize:  0.01880335807800293 s\n",
      "\n",
      "Fix bad wording:  0.0049817562103271484 s\n",
      "Tokenize:  0.008949756622314453 s\n",
      "Remove stopwords and Lemmatize:  0.024960994720458984 s\n",
      "\n",
      "Fix bad wording:  0.001962423324584961 s\n",
      "Tokenize:  0.002994537353515625 s\n",
      "Remove stopwords and Lemmatize:  0.011936664581298828 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.0069811344146728516 s\n",
      "Remove stopwords and Lemmatize:  0.023803234100341797 s\n",
      "\n",
      "Fix bad wording:  0.008977890014648438 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize:  0.01692509651184082 s\n",
      "Remove stopwords and Lemmatize:  0.05285835266113281 s\n",
      "\n",
      "Fix bad wording:  0.0009982585906982422 s\n",
      "Tokenize:  0.0019941329956054688 s\n",
      "Remove stopwords and Lemmatize:  0.0029931068420410156 s\n",
      "\n",
      "Fix bad wording:  0.0019936561584472656 s\n",
      "Tokenize:  0.002992391586303711 s\n",
      "Remove stopwords and Lemmatize:  0.009998559951782227 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.001997232437133789 s\n",
      "\n",
      "Fix bad wording:  0.005980491638183594 s\n",
      "Tokenize:  0.01196908950805664 s\n",
      "Remove stopwords and Lemmatize:  0.033935546875 s\n",
      "\n",
      "Fix bad wording:  0.003971576690673828 s\n",
      "Tokenize:  0.006010532379150391 s\n",
      "Remove stopwords and Lemmatize:  0.014929533004760742 s\n",
      "\n",
      "Fix bad wording:  0.0019943714141845703 s\n",
      "Tokenize:  0.002992391586303711 s\n",
      "Remove stopwords and Lemmatize:  0.007978439331054688 s\n",
      "\n",
      "Fix bad wording:  0.0010142326354980469 s\n",
      "Tokenize:  0.0029745101928710938 s\n",
      "Remove stopwords and Lemmatize:  0.005014657974243164 s\n",
      "\n",
      "Fix bad wording:  0.0019941329956054688 s\n",
      "Tokenize:  0.003989219665527344 s\n",
      "Remove stopwords and Lemmatize:  0.00897669792175293 s\n",
      "\n",
      "Fix bad wording:  0.005983829498291016 s\n",
      "Tokenize:  0.011966705322265625 s\n",
      "Remove stopwords and Lemmatize:  0.030920743942260742 s\n",
      "\n",
      "Fix bad wording:  0.0009944438934326172 s\n",
      "Tokenize:  0.0009992122650146484 s\n",
      "Remove stopwords and Lemmatize:  0.0009970664978027344 s\n",
      "\n",
      "Fix bad wording:  0.00797581672668457 s\n",
      "Tokenize:  0.012967824935913086 s\n",
      "Remove stopwords and Lemmatize:  0.03952646255493164 s\n",
      "\n",
      "Fix bad wording:  0.000993967056274414 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.003992795944213867 s\n",
      "\n",
      "Fix bad wording:  0.00498652458190918 s\n",
      "Tokenize:  0.009973526000976562 s\n",
      "Remove stopwords and Lemmatize:  0.02592921257019043 s\n",
      "\n",
      "Fix bad wording:  0.0059812068939208984 s\n",
      "Tokenize:  0.010945558547973633 s\n",
      "Remove stopwords and Lemmatize:  0.03244972229003906 s\n",
      "\n",
      "Fix bad wording:  0.004985809326171875 s\n",
      "Tokenize:  0.008005857467651367 s\n",
      "Remove stopwords and Lemmatize:  0.023900747299194336 s\n",
      "\n",
      "Fix bad wording:  0.003002166748046875 s\n",
      "Tokenize:  0.00497889518737793 s\n",
      "Remove stopwords and Lemmatize:  0.012995481491088867 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.001965045928955078 s\n",
      "\n",
      "Fix bad wording:  0.01099848747253418 s\n",
      "Tokenize:  0.01792311668395996 s\n",
      "Remove stopwords and Lemmatize:  0.049894094467163086 s\n",
      "\n",
      "Fix bad wording:  0.007952451705932617 s\n",
      "Tokenize:  0.011997222900390625 s\n",
      "Remove stopwords and Lemmatize:  0.0418853759765625 s\n",
      "\n",
      "Fix bad wording:  0.001995563507080078 s\n",
      "Tokenize:  0.0029859542846679688 s\n",
      "Remove stopwords and Lemmatize:  0.011945724487304688 s\n",
      "\n",
      "Fix bad wording:  0.0010259151458740234 s\n",
      "Tokenize:  0.002994537353515625 s\n",
      "Remove stopwords and Lemmatize:  0.011967182159423828 s\n",
      "\n",
      "Fix bad wording:  0.00398707389831543 s\n",
      "Tokenize:  0.006989002227783203 s\n",
      "Remove stopwords and Lemmatize:  0.02396082878112793 s\n",
      "\n",
      "Fix bad wording:  0.010977029800415039 s\n",
      "Tokenize:  0.018977880477905273 s\n",
      "Remove stopwords and Lemmatize:  0.056841135025024414 s\n",
      "\n",
      "Fix bad wording:  0.00496363639831543 s\n",
      "Tokenize:  0.008970975875854492 s\n",
      "Remove stopwords and Lemmatize:  0.02994561195373535 s\n",
      "\n",
      "Fix bad wording:  0.0029664039611816406 s\n",
      "Tokenize:  0.006012439727783203 s\n",
      "Remove stopwords and Lemmatize:  0.02390766143798828 s\n",
      "\n",
      "Fix bad wording:  0.002993345260620117 s\n",
      "Tokenize:  0.004004001617431641 s\n",
      "Remove stopwords and Lemmatize:  0.010955333709716797 s\n",
      "\n",
      "Fix bad wording:  0.006008625030517578 s\n",
      "Tokenize:  0.009964466094970703 s\n",
      "Remove stopwords and Lemmatize:  0.025914907455444336 s\n",
      "\n",
      "Fix bad wording:  0.0009965896606445312 s\n",
      "Tokenize:  0.0020236968994140625 s\n",
      "Remove stopwords and Lemmatize:  0.0049855709075927734 s\n",
      "\n",
      "Fix bad wording:  0.00498652458190918 s\n",
      "Tokenize:  0.009946107864379883 s\n",
      "Remove stopwords and Lemmatize:  0.025930404663085938 s\n",
      "\n",
      "Fix bad wording:  0.0019936561584472656 s\n",
      "Tokenize:  0.003018617630004883 s\n",
      "Remove stopwords and Lemmatize:  0.006986856460571289 s\n",
      "\n",
      "Fix bad wording:  0.0009679794311523438 s\n",
      "Tokenize:  0.002019643783569336 s\n",
      "Remove stopwords and Lemmatize:  0.006981611251831055 s\n",
      "\n",
      "Fix bad wording:  0.026929616928100586 s\n",
      "Tokenize:  0.04983663558959961 s\n",
      "Remove stopwords and Lemmatize:  0.14261865615844727 s\n",
      "\n",
      "Fix bad wording:  0.001993894577026367 s\n",
      "Tokenize:  0.0039904117584228516 s\n",
      "Remove stopwords and Lemmatize:  0.01299142837524414 s\n",
      "\n",
      "Fix bad wording:  0.009988069534301758 s\n",
      "Tokenize:  0.019905567169189453 s\n",
      "Remove stopwords and Lemmatize:  0.05356121063232422 s\n",
      "\n",
      "Fix bad wording:  0.0019974708557128906 s\n",
      "Tokenize:  0.002992868423461914 s\n",
      "Remove stopwords and Lemmatize:  0.008975982666015625 s\n",
      "\n",
      "Fix bad wording:  0.004957675933837891 s\n",
      "Tokenize:  0.017950773239135742 s\n",
      "Remove stopwords and Lemmatize:  0.024933338165283203 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.00598454475402832 s\n",
      "Remove stopwords and Lemmatize:  0.02067399024963379 s\n",
      "\n",
      "Fix bad wording:  0.004980802536010742 s\n",
      "Tokenize:  0.007978200912475586 s\n",
      "Remove stopwords and Lemmatize:  0.024934053421020508 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.001003265380859375 s\n",
      "Remove stopwords and Lemmatize:  0.0019931793212890625 s\n",
      "\n",
      "Fix bad wording:  0.0049915313720703125 s\n",
      "Tokenize:  0.006978273391723633 s\n",
      "Remove stopwords and Lemmatize:  0.017925262451171875 s\n",
      "\n",
      "Fix bad wording:  0.008004903793334961 s\n",
      "Tokenize:  0.012967586517333984 s\n",
      "Remove stopwords and Lemmatize:  0.03988361358642578 s\n",
      "\n",
      "Fix bad wording:  0.00200653076171875 s\n",
      "Tokenize:  0.0030252933502197266 s\n",
      "Remove stopwords and Lemmatize:  0.00896906852722168 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0010251998901367188 s\n",
      "Remove stopwords and Lemmatize:  0.002992868423461914 s\n",
      "\n",
      "Fix bad wording:  0.008975982666015625 s\n",
      "Tokenize:  0.015964984893798828 s\n",
      "Remove stopwords and Lemmatize:  0.04387187957763672 s\n",
      "\n",
      "Fix bad wording:  0.016958951950073242 s\n",
      "Tokenize:  0.03056025505065918 s\n",
      "Remove stopwords and Lemmatize:  0.09175419807434082 s\n",
      "\n",
      "Fix bad wording:  0.014575481414794922 s\n",
      "Tokenize:  0.02594780921936035 s\n",
      "Remove stopwords and Lemmatize:  0.0753166675567627 s\n",
      "\n",
      "Fix bad wording:  0.001996278762817383 s\n",
      "Tokenize:  0.003022432327270508 s\n",
      "Remove stopwords and Lemmatize:  0.007947444915771484 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.004994392395019531 s\n",
      "Remove stopwords and Lemmatize:  0.012955904006958008 s\n",
      "\n",
      "Fix bad wording:  0.007984161376953125 s\n",
      "Tokenize:  0.013962984085083008 s\n",
      "Remove stopwords and Lemmatize:  0.03544306755065918 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009949207305908203 s\n",
      "Remove stopwords and Lemmatize:  0.001999378204345703 s\n",
      "\n",
      "Fix bad wording:  0.008972406387329102 s\n",
      "Tokenize:  0.016927480697631836 s\n",
      "Remove stopwords and Lemmatize:  0.06482577323913574 s\n",
      "\n",
      "Fix bad wording:  0.0045185089111328125 s\n",
      "Tokenize:  0.00496220588684082 s\n",
      "Remove stopwords and Lemmatize:  0.015955686569213867 s\n",
      "\n",
      "Fix bad wording:  0.009005546569824219 s\n",
      "Tokenize:  0.016951560974121094 s\n",
      "Remove stopwords and Lemmatize:  0.04587888717651367 s\n",
      "\n",
      "Fix bad wording:  0.005955934524536133 s\n",
      "Tokenize:  0.008005380630493164 s\n",
      "Remove stopwords and Lemmatize:  0.023909568786621094 s\n",
      "\n",
      "Fix bad wording:  0.0019943714141845703 s\n",
      "Tokenize:  0.003989219665527344 s\n",
      "Remove stopwords and Lemmatize:  0.012992382049560547 s\n",
      "\n",
      "Fix bad wording:  0.015961170196533203 s\n",
      "Tokenize:  0.028871774673461914 s\n",
      "Remove stopwords and Lemmatize:  0.0967416763305664 s\n",
      "\n",
      "Fix bad wording:  0.007978677749633789 s\n",
      "Tokenize:  0.009999990463256836 s\n",
      "Remove stopwords and Lemmatize:  0.04185914993286133 s\n",
      "\n",
      "Fix bad wording:  0.017981529235839844 s\n",
      "Tokenize:  0.025899887084960938 s\n",
      "Remove stopwords and Lemmatize:  0.07782554626464844 s\n",
      "\n",
      "Fix bad wording:  0.015950441360473633 s\n",
      "Tokenize:  0.028894901275634766 s\n",
      "Remove stopwords and Lemmatize:  0.08480215072631836 s\n",
      "\n",
      "Fix bad wording:  0.00797581672668457 s\n",
      "Tokenize:  0.014934539794921875 s\n",
      "Remove stopwords and Lemmatize:  0.0419156551361084 s\n",
      "\n",
      "Fix bad wording:  0.0009944438934326172 s\n",
      "Tokenize:  0.0009992122650146484 s\n",
      "Remove stopwords and Lemmatize:  0.003986835479736328 s\n",
      "\n",
      "Fix bad wording:  0.0029592514038085938 s\n",
      "Tokenize:  0.004014730453491211 s\n",
      "Remove stopwords and Lemmatize:  0.010972023010253906 s\n",
      "\n",
      "Fix bad wording:  0.007532358169555664 s\n",
      "Tokenize:  0.007958412170410156 s\n",
      "Remove stopwords and Lemmatize:  0.021915197372436523 s\n",
      "\n",
      "Fix bad wording:  0.0069811344146728516 s\n",
      "Tokenize:  0.010999441146850586 s\n",
      "Remove stopwords and Lemmatize:  0.02593064308166504 s\n",
      "\n",
      "Fix bad wording:  0.005955696105957031 s\n",
      "Tokenize:  0.009003162384033203 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.026929616928100586 s\n",
      "\n",
      "Fix bad wording:  0.0019943714141845703 s\n",
      "Tokenize:  0.003971099853515625 s\n",
      "Remove stopwords and Lemmatize:  0.010976314544677734 s\n",
      "\n",
      "Fix bad wording:  0.0010101795196533203 s\n",
      "Tokenize:  0.0029897689819335938 s\n",
      "Remove stopwords and Lemmatize:  0.005957841873168945 s\n",
      "\n",
      "Fix bad wording:  0.0070078372955322266 s\n",
      "Tokenize:  0.012964248657226562 s\n",
      "Remove stopwords and Lemmatize:  0.037872314453125 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0010256767272949219 s\n",
      "Remove stopwords and Lemmatize:  0.0009694099426269531 s\n",
      "\n",
      "Fix bad wording:  0.0009989738464355469 s\n",
      "Tokenize:  0.0019941329956054688 s\n",
      "Remove stopwords and Lemmatize:  0.007978439331054688 s\n",
      "\n",
      "Fix bad wording:  0.008996248245239258 s\n",
      "Tokenize:  0.010980844497680664 s\n",
      "Remove stopwords and Lemmatize:  0.029917001724243164 s\n",
      "\n",
      "Fix bad wording:  0.0029909610748291016 s\n",
      "Tokenize:  0.003989696502685547 s\n",
      "Remove stopwords and Lemmatize:  0.01294398307800293 s\n",
      "\n",
      "Fix bad wording:  0.0010259151458740234 s\n",
      "Tokenize:  0.0019969940185546875 s\n",
      "Remove stopwords and Lemmatize:  0.003952741622924805 s\n",
      "\n",
      "Fix bad wording:  0.010030984878540039 s\n",
      "Tokenize:  0.017962932586669922 s\n",
      "Remove stopwords and Lemmatize:  0.043878793716430664 s\n",
      "\n",
      "Fix bad wording:  0.0019829273223876953 s\n",
      "Tokenize:  0.004996538162231445 s\n",
      "Remove stopwords and Lemmatize:  0.01395559310913086 s\n",
      "\n",
      "Fix bad wording:  0.0049839019775390625 s\n",
      "Tokenize:  0.008980512619018555 s\n",
      "Remove stopwords and Lemmatize:  0.02501058578491211 s\n",
      "\n",
      "Fix bad wording:  0.0030214786529541016 s\n",
      "Tokenize:  0.0059816837310791016 s\n",
      "Remove stopwords and Lemmatize:  0.016928434371948242 s\n",
      "\n",
      "Fix bad wording:  0.009998559951782227 s\n",
      "Tokenize:  0.01992940902709961 s\n",
      "Remove stopwords and Lemmatize:  0.05237102508544922 s\n",
      "\n",
      "Fix bad wording:  0.004987955093383789 s\n",
      "Tokenize:  0.00994563102722168 s\n",
      "Remove stopwords and Lemmatize:  0.02892160415649414 s\n",
      "\n",
      "Fix bad wording:  0.004023075103759766 s\n",
      "Tokenize:  0.004985332489013672 s\n",
      "Remove stopwords and Lemmatize:  0.01395726203918457 s\n",
      "\n",
      "Fix bad wording:  0.0009982585906982422 s\n",
      "Tokenize:  0.0022039413452148438 s\n",
      "Remove stopwords and Lemmatize:  0.0049877166748046875 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.001992940902709961 s\n",
      "Remove stopwords and Lemmatize:  0.003999948501586914 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009913444519042969 s\n",
      "Remove stopwords and Lemmatize:  0.0019931793212890625 s\n",
      "\n",
      "Fix bad wording:  0.005990266799926758 s\n",
      "Tokenize:  0.009969711303710938 s\n",
      "Remove stopwords and Lemmatize:  0.02892160415649414 s\n",
      "\n",
      "Fix bad wording:  0.01097249984741211 s\n",
      "Tokenize:  0.018950700759887695 s\n",
      "Remove stopwords and Lemmatize:  0.0540165901184082 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.001994609832763672 s\n",
      "Remove stopwords and Lemmatize:  0.003989458084106445 s\n",
      "\n",
      "Fix bad wording:  0.004985809326171875 s\n",
      "Tokenize:  0.012962818145751953 s\n",
      "Remove stopwords and Lemmatize:  0.02890467643737793 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0010004043579101562 s\n",
      "Remove stopwords and Lemmatize:  0.0019922256469726562 s\n",
      "\n",
      "Fix bad wording:  0.003987312316894531 s\n",
      "Tokenize:  0.007952213287353516 s\n",
      "Remove stopwords and Lemmatize:  0.020977258682250977 s\n",
      "\n",
      "Fix bad wording:  0.0059871673583984375 s\n",
      "Tokenize:  0.00996851921081543 s\n",
      "Remove stopwords and Lemmatize:  0.0259246826171875 s\n",
      "\n",
      "Fix bad wording:  0.0019979476928710938 s\n",
      "Tokenize:  0.00401616096496582 s\n",
      "Remove stopwords and Lemmatize:  0.010970354080200195 s\n",
      "\n",
      "Fix bad wording:  0.006011009216308594 s\n",
      "Tokenize:  0.010970830917358398 s\n",
      "Remove stopwords and Lemmatize:  0.028919696807861328 s\n",
      "\n",
      "Fix bad wording:  0.0029997825622558594 s\n",
      "Tokenize:  0.003982067108154297 s\n",
      "Remove stopwords and Lemmatize:  0.009979009628295898 s\n",
      "\n",
      "Fix bad wording:  0.006949424743652344 s\n",
      "Tokenize:  0.013962268829345703 s\n",
      "Remove stopwords and Lemmatize:  0.03094196319580078 s\n",
      "\n",
      "Fix bad wording:  0.005990266799926758 s\n",
      "Tokenize:  0.011937141418457031 s\n",
      "Remove stopwords and Lemmatize:  0.029950380325317383 s\n",
      "\n",
      "Fix bad wording:  0.006952524185180664 s\n",
      "Tokenize:  0.011968135833740234 s\n",
      "Remove stopwords and Lemmatize:  0.02895498275756836 s\n",
      "\n",
      "Fix bad wording:  0.005980014801025391 s\n",
      "Tokenize:  0.007976531982421875 s\n",
      "Remove stopwords and Lemmatize:  0.01994633674621582 s\n",
      "\n",
      "Fix bad wording:  0.002971649169921875 s\n",
      "Tokenize:  0.004011631011962891 s\n",
      "Remove stopwords and Lemmatize:  0.009972333908081055 s\n",
      "\n",
      "Fix bad wording:  0.002000093460083008 s\n",
      "Tokenize:  0.004952192306518555 s\n",
      "Remove stopwords and Lemmatize:  0.013989448547363281 s\n",
      "\n",
      "Fix bad wording:  0.007980585098266602 s\n",
      "Tokenize:  0.012969493865966797 s\n",
      "Remove stopwords and Lemmatize:  0.03488969802856445 s\n",
      "\n",
      "Fix bad wording:  0.0010106563568115234 s\n",
      "Tokenize:  0.0029904842376708984 s\n",
      "Remove stopwords and Lemmatize:  0.007950067520141602 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.001028299331665039 s\n",
      "Remove stopwords and Lemmatize:  0.0019922256469726562 s\n",
      "\n",
      "Fix bad wording:  0.0019931793212890625 s\n",
      "Tokenize:  0.004986763000488281 s\n",
      "Remove stopwords and Lemmatize:  0.011968374252319336 s\n",
      "\n",
      "Fix bad wording:  0.0009996891021728516 s\n",
      "Tokenize:  0.0019936561584472656 s\n",
      "Remove stopwords and Lemmatize:  0.004973649978637695 s\n",
      "\n",
      "Fix bad wording:  0.001979351043701172 s\n",
      "Tokenize:  0.001993417739868164 s\n",
      "Remove stopwords and Lemmatize:  0.008976936340332031 s\n",
      "\n",
      "Fix bad wording:  0.001996755599975586 s\n",
      "Tokenize:  0.002989053726196289 s\n",
      "Remove stopwords and Lemmatize:  0.008948802947998047 s\n",
      "\n",
      "Fix bad wording:  0.0070078372955322266 s\n",
      "Tokenize:  0.008959293365478516 s\n",
      "Remove stopwords and Lemmatize:  0.02495121955871582 s\n",
      "\n",
      "Fix bad wording:  0.0009949207305908203 s\n",
      "Tokenize:  0.001995563507080078 s\n",
      "Remove stopwords and Lemmatize:  0.003991603851318359 s\n",
      "\n",
      "Fix bad wording:  0.006979227066040039 s\n",
      "Tokenize:  0.010973215103149414 s\n",
      "Remove stopwords and Lemmatize:  0.031884193420410156 s\n",
      "\n",
      "Fix bad wording:  0.005983829498291016 s\n",
      "Tokenize:  0.010000467300415039 s\n",
      "Remove stopwords and Lemmatize:  0.026902198791503906 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.007010221481323242 s\n",
      "Remove stopwords and Lemmatize:  0.019916772842407227 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0020036697387695312 s\n",
      "Remove stopwords and Lemmatize:  0.0059850215911865234 s\n",
      "\n",
      "Fix bad wording:  0.01691889762878418 s\n",
      "Tokenize:  0.02988743782043457 s\n",
      "Remove stopwords and Lemmatize:  0.08035063743591309 s\n",
      "\n",
      "Fix bad wording:  0.00498652458190918 s\n",
      "Tokenize:  0.011939287185668945 s\n",
      "Remove stopwords and Lemmatize:  0.025961637496948242 s\n",
      "\n",
      "Fix bad wording:  0.0059812068939208984 s\n",
      "Tokenize:  0.010962247848510742 s\n",
      "Remove stopwords and Lemmatize:  0.028913497924804688 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.003988504409790039 s\n",
      "Remove stopwords and Lemmatize:  0.010973215103149414 s\n",
      "\n",
      "Fix bad wording:  0.0059812068939208984 s\n",
      "Tokenize:  0.01593184471130371 s\n",
      "Remove stopwords and Lemmatize:  0.037944793701171875 s\n",
      "\n",
      "Fix bad wording:  0.012975454330444336 s\n",
      "Tokenize:  0.021939754486083984 s\n",
      "Remove stopwords and Lemmatize:  0.05983996391296387 s\n",
      "\n",
      "Fix bad wording:  0.0020232200622558594 s\n",
      "Tokenize:  0.006952762603759766 s\n",
      "Remove stopwords and Lemmatize:  0.013991355895996094 s\n",
      "\n",
      "Fix bad wording:  0.009977102279663086 s\n",
      "Tokenize:  0.014954566955566406 s\n",
      "Remove stopwords and Lemmatize:  0.044881582260131836 s\n",
      "\n",
      "Fix bad wording:  0.0019276142120361328 s\n",
      "Tokenize:  0.004014253616333008 s\n",
      "Remove stopwords and Lemmatize:  0.011932373046875 s\n",
      "\n",
      "Fix bad wording:  0.0010292530059814453 s\n",
      "Tokenize:  0.001993417739868164 s\n",
      "Remove stopwords and Lemmatize:  0.003989696502685547 s\n",
      "\n",
      "Fix bad wording:  0.000995635986328125 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.002998828887939453 s\n",
      "\n",
      "Fix bad wording:  0.008940696716308594 s\n",
      "Tokenize:  0.014957904815673828 s\n",
      "Remove stopwords and Lemmatize:  0.04959440231323242 s\n",
      "\n",
      "Fix bad wording:  0.005984067916870117 s\n",
      "Tokenize:  0.011999130249023438 s\n",
      "Remove stopwords and Lemmatize:  0.029917478561401367 s\n",
      "\n",
      "Fix bad wording:  0.0029931068420410156 s\n",
      "Tokenize:  0.0049898624420166016 s\n",
      "Remove stopwords and Lemmatize:  0.013963937759399414 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0029592514038085938 s\n",
      "Remove stopwords and Lemmatize:  0.006996631622314453 s\n",
      "\n",
      "Fix bad wording:  0.0010082721710205078 s\n",
      "Tokenize:  0.0019676685333251953 s\n",
      "Remove stopwords and Lemmatize:  0.0050258636474609375 s\n",
      "\n",
      "Fix bad wording:  0.00796651840209961 s\n",
      "Tokenize:  0.015521526336669922 s\n",
      "Remove stopwords and Lemmatize:  0.046930789947509766 s\n",
      "\n",
      "Fix bad wording:  0.002995729446411133 s\n",
      "Tokenize:  0.005955696105957031 s\n",
      "Remove stopwords and Lemmatize:  0.016985654830932617 s\n",
      "\n",
      "Fix bad wording:  0.0009946823120117188 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0019927024841308594 s\n",
      "\n",
      "Fix bad wording:  0.0019686222076416016 s\n",
      "Tokenize:  0.0029916763305664062 s\n",
      "Remove stopwords and Lemmatize:  0.010970592498779297 s\n",
      "\n",
      "Fix bad wording:  0.02097034454345703 s\n",
      "Tokenize:  0.035903215408325195 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.10970878601074219 s\n",
      "\n",
      "Fix bad wording:  0.00598907470703125 s\n",
      "Tokenize:  0.012959480285644531 s\n",
      "Remove stopwords and Lemmatize:  0.03587627410888672 s\n",
      "\n",
      "Fix bad wording:  0.003977060317993164 s\n",
      "Tokenize:  0.007977962493896484 s\n",
      "Remove stopwords and Lemmatize:  0.023935556411743164 s\n",
      "\n",
      "Fix bad wording:  0.010004520416259766 s\n",
      "Tokenize:  0.01794886589050293 s\n",
      "Remove stopwords and Lemmatize:  0.055850982666015625 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0029904842376708984 s\n",
      "Remove stopwords and Lemmatize:  0.0049896240234375 s\n",
      "\n",
      "Fix bad wording:  0.0009951591491699219 s\n",
      "Tokenize:  0.0009949207305908203 s\n",
      "Remove stopwords and Lemmatize:  0.004960536956787109 s\n",
      "\n",
      "Fix bad wording:  0.005984306335449219 s\n",
      "Tokenize:  0.012964963912963867 s\n",
      "Remove stopwords and Lemmatize:  0.030954360961914062 s\n",
      "\n",
      "Fix bad wording:  0.002994060516357422 s\n",
      "Tokenize:  0.005975246429443359 s\n",
      "Remove stopwords and Lemmatize:  0.01596665382385254 s\n",
      "\n",
      "Fix bad wording:  0.0029861927032470703 s\n",
      "Tokenize:  0.003985404968261719 s\n",
      "Remove stopwords and Lemmatize:  0.014961004257202148 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009708404541015625 s\n",
      "Remove stopwords and Lemmatize:  0.0009970664978027344 s\n",
      "\n",
      "Fix bad wording:  0.007010459899902344 s\n",
      "Tokenize:  0.009962797164916992 s\n",
      "Remove stopwords and Lemmatize:  0.026998519897460938 s\n",
      "\n",
      "Fix bad wording:  0.004995107650756836 s\n",
      "Tokenize:  0.007972955703735352 s\n",
      "Remove stopwords and Lemmatize:  0.02293539047241211 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.00199127197265625 s\n",
      "Remove stopwords and Lemmatize:  0.0029952526092529297 s\n",
      "\n",
      "Fix bad wording:  0.0009992122650146484 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.003988981246948242 s\n",
      "\n",
      "Fix bad wording:  0.008979558944702148 s\n",
      "Tokenize:  0.011992692947387695 s\n",
      "Remove stopwords and Lemmatize:  0.035875558853149414 s\n",
      "\n",
      "Fix bad wording:  0.007010936737060547 s\n",
      "Tokenize:  0.013959169387817383 s\n",
      "Remove stopwords and Lemmatize:  0.040873050689697266 s\n",
      "\n",
      "Fix bad wording:  0.006004810333251953 s\n",
      "Tokenize:  0.010958194732666016 s\n",
      "Remove stopwords and Lemmatize:  0.03092789649963379 s\n",
      "\n",
      "Fix bad wording:  0.0019843578338623047 s\n",
      "Tokenize:  0.004001617431640625 s\n",
      "Remove stopwords and Lemmatize:  0.008973360061645508 s\n",
      "\n",
      "Fix bad wording:  0.00997471809387207 s\n",
      "Tokenize:  0.016927242279052734 s\n",
      "Remove stopwords and Lemmatize:  0.045942068099975586 s\n",
      "\n",
      "Fix bad wording:  0.0009949207305908203 s\n",
      "Tokenize:  0.0019948482513427734 s\n",
      "Remove stopwords and Lemmatize:  0.0029883384704589844 s\n",
      "\n",
      "Fix bad wording:  0.002989053726196289 s\n",
      "Tokenize:  0.003993511199951172 s\n",
      "Remove stopwords and Lemmatize:  0.01396942138671875 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0019953250885009766 s\n",
      "Remove stopwords and Lemmatize:  0.003989219665527344 s\n",
      "\n",
      "Fix bad wording:  0.003985166549682617 s\n",
      "Tokenize:  0.0079498291015625 s\n",
      "Remove stopwords and Lemmatize:  0.022000789642333984 s\n",
      "\n",
      "Fix bad wording:  0.01193857192993164 s\n",
      "Tokenize:  0.01996469497680664 s\n",
      "Remove stopwords and Lemmatize:  0.058853864669799805 s\n",
      "\n",
      "Fix bad wording:  0.00897526741027832 s\n",
      "Tokenize:  0.01795220375061035 s\n",
      "Remove stopwords and Lemmatize:  0.04886627197265625 s\n",
      "\n",
      "Fix bad wording:  0.002995729446411133 s\n",
      "Tokenize:  0.00498652458190918 s\n",
      "Remove stopwords and Lemmatize:  0.019919157028198242 s\n",
      "\n",
      "Fix bad wording:  0.008946657180786133 s\n",
      "Tokenize:  0.011960268020629883 s\n",
      "Remove stopwords and Lemmatize:  0.03194308280944824 s\n",
      "\n",
      "Fix bad wording:  0.007949113845825195 s\n",
      "Tokenize:  0.01595926284790039 s\n",
      "Remove stopwords and Lemmatize:  0.045934200286865234 s\n",
      "\n",
      "Fix bad wording:  0.01096963882446289 s\n",
      "Tokenize:  0.0239408016204834 s\n",
      "Remove stopwords and Lemmatize:  0.06633114814758301 s\n",
      "\n",
      "Fix bad wording:  0.0029938220977783203 s\n",
      "Tokenize:  0.005981922149658203 s\n",
      "Remove stopwords and Lemmatize:  0.015929460525512695 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.003991127014160156 s\n",
      "Remove stopwords and Lemmatize:  0.013955831527709961 s\n",
      "\n",
      "Fix bad wording:  0.001996278762817383 s\n",
      "Tokenize:  0.003992319107055664 s\n",
      "Remove stopwords and Lemmatize:  0.010970592498779297 s\n",
      "\n",
      "Fix bad wording:  0.0029850006103515625 s\n",
      "Tokenize:  0.004495382308959961 s\n",
      "Remove stopwords and Lemmatize:  0.01298379898071289 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.003987550735473633 s\n",
      "Remove stopwords and Lemmatize:  0.010970354080200195 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "# schwartz.json or pruned_schwartz.json\n",
    "filepath = 'pruned_schwartz.json'\n",
    "\n",
    "data = read_data(filepath)\n",
    "# corpus = extract_corpus(data)\n",
    "# corpusPP = preprocess_corpus(corpus)\n",
    "\n",
    "corpusPP = list(data.text.apply(tp.clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdetok = MosesDetokenizer()\n",
    "\n",
    "brown_files_sent = []\n",
    "for fid in brown.fileids():\n",
    "    brown_files_sent.append([mdetok.detokenize(' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\").split(), return_str=True)  for sent in brown.sents(fid)])\n",
    "    \n",
    "brown_natural = [' '.join(bfs) for bfs in brown_files_sent]\n",
    "brown_naturalPP = preprocess_corpus(brown_natural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nmf_list, W_list, tfidf, tfidf_vectorizer = train_corpus(corpusPP, data, [], n_topics=6, betaloss = 'kullback-leibler', bckg_brown = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mrefer state regime political gift\n",
      "\u001b[1mTopic #1: \u001b[0moccur often critical give political\n",
      "\u001b[1mTopic #2: \u001b[0missue one canada goal synthesize\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msuicide shame positive time state\n",
      "\u001b[1mTopic #1: \u001b[0mshyness translation relation uncover immediate\n",
      "\u001b[1mTopic #2: \u001b[0mtime remove tip opposite important\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mrelationship large valid luxury product\n",
      "\u001b[1mTopic #1: \u001b[0moccur procedure modern oxford motivational\n",
      "\u001b[1mTopic #2: \u001b[0msocial well great medical production\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msystem unchosen effectively supervise remain\n",
      "\u001b[1mTopic #1: \u001b[0malso relative air pain unsourced\n",
      "\u001b[1mTopic #2: \u001b[0malso locke lead interest violent\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mlike sarawak list undertake incorporate\n",
      "\u001b[1mTopic #1: \u001b[0mrevolutionary issue back interest best\n",
      "\u001b[1mTopic #2: \u001b[0mwell romanticism redistribute strengthbased undermine\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msociety official period window intelligence\n",
      "\u001b[1mTopic #1: \u001b[0mnew regulate sometimes also use\n",
      "\u001b[1mTopic #2: \u001b[0mwhatever refer word together stay\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mverification way understand thirdly nuremberg\n",
      "\u001b[1mTopic #1: \u001b[0muse sag human together case\n",
      "\u001b[1mTopic #2: \u001b[0mwithin whether verb morality upon\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mtalent qualify verbal use factor\n",
      "\u001b[1mTopic #1: \u001b[0mupanishad latin work person postvedic\n",
      "\u001b[1mTopic #2: \u001b[0mwisdom outline aristotle victorian recognize\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mhunt ordinary passenger food ring\n",
      "\u001b[1mTopic #1: \u001b[0mrisk thus south texas think\n",
      "\u001b[1mTopic #2: \u001b[0mresident islamic far life meanwhile\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mservice period thus intruder firefighter\n",
      "\u001b[1mTopic #1: \u001b[0minstitution industrial shape morality similarity\n",
      "\u001b[1mTopic #2: \u001b[0mplace europe partial proportion steal\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "for i in range(10):\n",
    "    print_top_words(nmf_list, i, tfidf_vectorizer, n_top_words=5, n_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0moften face use first way\n",
      "\u001b[1mTopic #1: \u001b[0mdefine concern negative law world\n",
      "\u001b[1mTopic #2: \u001b[0mcritical often force help state\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msadness shock selfconfidence sometimes others\n",
      "\u001b[1mTopic #1: \u001b[0msuggest still say people thus\n",
      "\u001b[1mTopic #2: \u001b[0mmust point say psychological social\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mtheory merely mark occur sincere\n",
      "\u001b[1mTopic #1: \u001b[0msocial relation life student level\n",
      "\u001b[1mTopic #2: \u001b[0mlead use procedure message production\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0malong mean finally task draw\n",
      "\u001b[1mTopic #1: \u001b[0magency one liability italian service\n",
      "\u001b[1mTopic #2: \u001b[0malexandre form put separately tyranny\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mattractive technology increasingly use proper\n",
      "\u001b[1mTopic #1: \u001b[0mlate quite specific toward katanga\n",
      "\u001b[1mTopic #2: \u001b[0mquestion public invoke situation develop\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mpower provide might also party\n",
      "\u001b[1mTopic #1: \u001b[0mrachel servant humiliate utility place\n",
      "\u001b[1mTopic #2: \u001b[0mthus tradition self mazda use\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mshow suggest method trace suppletive\n",
      "\u001b[1mTopic #1: \u001b[0mrefer show upon status hold\n",
      "\u001b[1mTopic #2: \u001b[0mvary unsourced state original peacock\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mtheme lose henry fire pasupata\n",
      "\u001b[1mTopic #1: \u001b[0marabic secret objectively theology past\n",
      "\u001b[1mTopic #2: \u001b[0mkant obedience sophrosyne signify sensual\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mreach problem magic mass exclude\n",
      "\u001b[1mTopic #1: \u001b[0minstrument kilometer international paris incidentfree\n",
      "\u001b[1mTopic #2: \u001b[0mgridiron term ptolemaic sahara similar\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mone interest emotion small opportunity\n",
      "\u001b[1mTopic #1: \u001b[0mexpert regulation team outbreak morality\n",
      "\u001b[1mTopic #2: \u001b[0mhealthcare individual thus imply seven\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in NMF model:\")\n",
    "for i in range(10):\n",
    "    print_top_words(nmf_list, i, tfidf_vectorizer, n_top_words=5, n_topics=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Sum up sub topics\n",
    "W_train_norm_list = []\n",
    "for W in W_list:\n",
    "    W_train_cumul = cumulate_W(W, n_topics=3)\n",
    "    W_train_norm = normalize_W(W_train_cumul)\n",
    "    W_train_norm_list.append(W_train_norm)\n",
    "W_train_norm = np.asarray(W_train_norm_list).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaf3ac2bc634376bd2ec860fa82974b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=466, description='doc', max=933), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.print_train_results>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(print_train_results, doc_topic=fixed(W_train_norm), doc = (0, len(W_train_norm)-1, 1), corpus=fixed(corpus), data=fixed(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Critical thinking \\n Sculpture of Socrates \\n ...</td>\n",
       "      <td>66.123691</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Environmental justice \\n This article has mult...</td>\n",
       "      <td>98.831153</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Natural resource \\n \"Primary resource\" redirec...</td>\n",
       "      <td>99.580788</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ceasefire \\n \"Truce\" redirects here For other ...</td>\n",
       "      <td>99.969178</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>International community \\n The \\n internationa...</td>\n",
       "      <td>99.999451</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism  hedonism  \\\n",
       "0  Critical thinking \\n Sculpture of Socrates \\n ...     66.123691       0.0   \n",
       "1  Environmental justice \\n This article has mult...     98.831153       0.0   \n",
       "2  Natural resource \\n \"Primary resource\" redirec...     99.580788       0.0   \n",
       "3  Ceasefire \\n \"Truce\" redirects here For other ...     99.969178       0.0   \n",
       "4  International community \\n The \\n internationa...     99.999451       0.0   \n",
       "\n",
       "   achievement  power  self-direction  benevolence  conformity  tradition  \\\n",
       "0          0.0    0.0             0.0          0.0         0.0        0.0   \n",
       "1          0.0    0.0             0.0          0.0         0.0        0.0   \n",
       "2          0.0    0.0             0.0          0.0         0.0        0.0   \n",
       "3          0.0    0.0             0.0          0.0         0.0        0.0   \n",
       "4          0.0    0.0             0.0          0.0         0.0        0.0   \n",
       "\n",
       "   stimulation  security  \n",
       "0          0.0       0.0  \n",
       "1          0.0       0.0  \n",
       "2          0.0       0.0  \n",
       "3          0.0       0.0  \n",
       "4          0.0       0.0  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_excel(W_train_norm, corpus, filepath = 'output.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = export_to_csv(W_train_norm, corpus, filepath = 'output.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [nmf_list, tfidf_vectorizer], open( \"nmf2_pretrained_pruned_lem2.p\", \"wb\" ) )\n",
    "#pickle.dump( [nmf_list, tfidf_vectorizer], open( \"nmf2_pretrained_pruned_brown.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Different Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate your documents, simply append them to _docs list_ as a whole string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two example documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus = []\n",
    "f = open(\"pope.txt\", \"r\") #Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "pope = f.read()\n",
    "test_corpus.append(pope)\n",
    "f.close()\n",
    "\n",
    "f = open(\"dod.txt\", \"r\")  # US Department of Defense, https://www.defense.gov/About/\n",
    "dod = f.read()\n",
    "test_corpus.append(dod)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n"
     ]
    }
   ],
   "source": [
    "test_corpusPP = preprocess_corpus(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.00s.\n",
      "Fitting NMF for universalism\n",
      "Fitting NMF for hedonism\n",
      "Fitting NMF for achievement\n",
      "Fitting NMF for power\n",
      "Fitting NMF for self-direction\n",
      "Fitting NMF for benevolence\n",
      "Fitting NMF for conformity\n",
      "Fitting NMF for tradition\n",
      "Fitting NMF for stimulation\n",
      "Fitting NMF for security\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "t0 = time()\n",
    "tfidf_test = tfidf_vectorizer.transform(test_corpusPP)\n",
    "n_features = tfidf_test.shape[1]\n",
    "print(\"done in %0.2fs.\" % (time() - t0))\n",
    "\n",
    "W_test_list = []\n",
    "for i, nmf in enumerate(nmf_list):\n",
    "    print(\"Fitting NMF for \" + str(categories[i]))\n",
    "    W_test = evaluate_docs(test_corpusPP, nmf, tfidf_test, betaloss = 'kullback-leibler')\n",
    "    W_test_list.append(W_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum up sub topics\n",
    "W_test_norm_list = []\n",
    "for W in W_test_list:\n",
    "    W_test_cumul = cumulate_W(W, n_topics=3)\n",
    "    W_test_norm = normalize_W(W_test_cumul)\n",
    "    W_test_norm_list.append(W_test_norm)\n",
    "W_test_norm = np.asarray(W_test_norm_list).T[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aa7894b36a945a1b69d33478f1eb8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='doc', max=1), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.print_test_results>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(print_test_results, doc_topic=fixed(W_test_norm), doc = (0, len(W_test_norm)-1, 1), corpus=fixed(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good evening  or, good morning, I am not su...</td>\n",
       "      <td>26.917947</td>\n",
       "      <td>53.195741</td>\n",
       "      <td>21.905193</td>\n",
       "      <td>33.749939</td>\n",
       "      <td>14.305954</td>\n",
       "      <td>65.909071</td>\n",
       "      <td>76.491778</td>\n",
       "      <td>38.390092</td>\n",
       "      <td>32.039518</td>\n",
       "      <td>36.997533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "      <td>75.736422</td>\n",
       "      <td>5.911529</td>\n",
       "      <td>40.480161</td>\n",
       "      <td>76.072999</td>\n",
       "      <td>61.269648</td>\n",
       "      <td>0.049261</td>\n",
       "      <td>1.888345</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>58.705763</td>\n",
       "      <td>79.900540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism   hedonism  \\\n",
       "0  Good evening  or, good morning, I am not su...     26.917947  53.195741   \n",
       "1  \\nOn behalf of the Secretary of Defense and De...     75.736422   5.911529   \n",
       "\n",
       "   achievement      power  self-direction  benevolence  conformity  tradition  \\\n",
       "0    21.905193  33.749939       14.305954    65.909071   76.491778  38.390092   \n",
       "1    40.480161  76.072999       61.269648     0.049261    1.888345   0.001672   \n",
       "\n",
       "   stimulation   security  \n",
       "0    32.039518  36.997533  \n",
       "1    58.705763  79.900540  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_excel(W_test_norm, test_corpus, filepath = 'output.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>universalism</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>achievement</th>\n",
       "      <th>power</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>security</th>\n",
       "      <th>general</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good evening  or, good morning, I am not su...</td>\n",
       "      <td>3.210163</td>\n",
       "      <td>7.430994</td>\n",
       "      <td>4.855774</td>\n",
       "      <td>6.453324</td>\n",
       "      <td>0.137821</td>\n",
       "      <td>20.459419</td>\n",
       "      <td>27.277332</td>\n",
       "      <td>9.677749</td>\n",
       "      <td>3.746420</td>\n",
       "      <td>5.519147</td>\n",
       "      <td>11.231858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "      <td>19.748271</td>\n",
       "      <td>0.946942</td>\n",
       "      <td>8.978493</td>\n",
       "      <td>16.955744</td>\n",
       "      <td>13.825183</td>\n",
       "      <td>0.004017</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000013</td>\n",
       "      <td>12.855169</td>\n",
       "      <td>26.148252</td>\n",
       "      <td>0.537895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Text  universalism  hedonism  \\\n",
       "0  Good evening  or, good morning, I am not su...      3.210163  7.430994   \n",
       "1  \\nOn behalf of the Secretary of Defense and De...     19.748271  0.946942   \n",
       "\n",
       "   achievement      power  self-direction  benevolence  conformity  tradition  \\\n",
       "0     4.855774   6.453324        0.137821    20.459419   27.277332   9.677749   \n",
       "1     8.978493  16.955744       13.825183     0.004017    0.000021   0.000013   \n",
       "\n",
       "   stimulation   security    general  \n",
       "0     3.746420   5.519147  11.231858  \n",
       "1    12.855169  26.148252   0.537895  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_csv(W_test_norm, test_corpus, filepath = 'output.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>universalism - word</th>\n",
       "      <th>universalism - score</th>\n",
       "      <th>benevolence - word</th>\n",
       "      <th>benevolence - score</th>\n",
       "      <th>conformity - word</th>\n",
       "      <th>conformity - score</th>\n",
       "      <th>tradition - word</th>\n",
       "      <th>tradition - score</th>\n",
       "      <th>security - word</th>\n",
       "      <th>security - score</th>\n",
       "      <th>power - word</th>\n",
       "      <th>power - score</th>\n",
       "      <th>achievement - word</th>\n",
       "      <th>achievement - score</th>\n",
       "      <th>hedonism - word</th>\n",
       "      <th>hedonism - score</th>\n",
       "      <th>stimulation - word</th>\n",
       "      <th>stimulation - score</th>\n",
       "      <th>self-direction - word</th>\n",
       "      <th>self-direction - score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>environment</td>\n",
       "      <td>0.0701</td>\n",
       "      <td>moral</td>\n",
       "      <td>0.0810</td>\n",
       "      <td>god</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>virtu</td>\n",
       "      <td>0.1417</td>\n",
       "      <td>reciproc</td>\n",
       "      <td>0.1349</td>\n",
       "      <td>power</td>\n",
       "      <td>0.1135</td>\n",
       "      <td>capit</td>\n",
       "      <td>0.1522</td>\n",
       "      <td>pleasur</td>\n",
       "      <td>0.0727</td>\n",
       "      <td>travel</td>\n",
       "      <td>0.1690</td>\n",
       "      <td>creativ</td>\n",
       "      <td>0.0880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>peac</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>good</td>\n",
       "      <td>0.0739</td>\n",
       "      <td>command</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>temper</td>\n",
       "      <td>0.0985</td>\n",
       "      <td>secur</td>\n",
       "      <td>0.1242</td>\n",
       "      <td>author</td>\n",
       "      <td>0.1116</td>\n",
       "      <td>statu</td>\n",
       "      <td>0.0813</td>\n",
       "      <td>happi</td>\n",
       "      <td>0.0631</td>\n",
       "      <td>sport</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>independ</td>\n",
       "      <td>0.0648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ecolog</td>\n",
       "      <td>0.0542</td>\n",
       "      <td>ethic</td>\n",
       "      <td>0.0622</td>\n",
       "      <td>cultur</td>\n",
       "      <td>0.0511</td>\n",
       "      <td>humil</td>\n",
       "      <td>0.0921</td>\n",
       "      <td>social</td>\n",
       "      <td>0.0613</td>\n",
       "      <td>social</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>social</td>\n",
       "      <td>0.0796</td>\n",
       "      <td>emot</td>\n",
       "      <td>0.0525</td>\n",
       "      <td>adventur</td>\n",
       "      <td>0.1471</td>\n",
       "      <td>intellig</td>\n",
       "      <td>0.0532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>social</td>\n",
       "      <td>0.0448</td>\n",
       "      <td>truth</td>\n",
       "      <td>0.0485</td>\n",
       "      <td>parent</td>\n",
       "      <td>0.0477</td>\n",
       "      <td>tradit</td>\n",
       "      <td>0.0705</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.0578</td>\n",
       "      <td>domin</td>\n",
       "      <td>0.0468</td>\n",
       "      <td>need</td>\n",
       "      <td>0.0458</td>\n",
       "      <td>pain</td>\n",
       "      <td>0.0407</td>\n",
       "      <td>tourism</td>\n",
       "      <td>0.1462</td>\n",
       "      <td>ye</td>\n",
       "      <td>0.0518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>right</td>\n",
       "      <td>0.0425</td>\n",
       "      <td>god</td>\n",
       "      <td>0.0410</td>\n",
       "      <td>disciplin</td>\n",
       "      <td>0.0466</td>\n",
       "      <td>moral</td>\n",
       "      <td>0.0562</td>\n",
       "      <td>clean</td>\n",
       "      <td>0.0543</td>\n",
       "      <td>revolut</td>\n",
       "      <td>0.0414</td>\n",
       "      <td>human</td>\n",
       "      <td>0.0426</td>\n",
       "      <td>hedon</td>\n",
       "      <td>0.0402</td>\n",
       "      <td>explor</td>\n",
       "      <td>0.1033</td>\n",
       "      <td>invent</td>\n",
       "      <td>0.0468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>human</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>evil</td>\n",
       "      <td>0.0406</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0463</td>\n",
       "      <td>sophrosyn</td>\n",
       "      <td>0.0547</td>\n",
       "      <td>contamin</td>\n",
       "      <td>0.0535</td>\n",
       "      <td>control</td>\n",
       "      <td>0.0405</td>\n",
       "      <td>individu</td>\n",
       "      <td>0.0424</td>\n",
       "      <td>person</td>\n",
       "      <td>0.0385</td>\n",
       "      <td>stimul</td>\n",
       "      <td>0.1031</td>\n",
       "      <td>innov</td>\n",
       "      <td>0.0386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>intern</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>forgiv</td>\n",
       "      <td>0.0393</td>\n",
       "      <td>polit</td>\n",
       "      <td>0.0450</td>\n",
       "      <td>christian</td>\n",
       "      <td>0.0527</td>\n",
       "      <td>risk</td>\n",
       "      <td>0.0449</td>\n",
       "      <td>polit</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>manag</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>psycholog</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>genr</td>\n",
       "      <td>0.0664</td>\n",
       "      <td>territori</td>\n",
       "      <td>0.0376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>war</td>\n",
       "      <td>0.0314</td>\n",
       "      <td>one</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>behavior</td>\n",
       "      <td>0.0433</td>\n",
       "      <td>myth</td>\n",
       "      <td>0.0469</td>\n",
       "      <td>pollut</td>\n",
       "      <td>0.0354</td>\n",
       "      <td>state</td>\n",
       "      <td>0.0365</td>\n",
       "      <td>intellectu</td>\n",
       "      <td>0.0382</td>\n",
       "      <td>one</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>fiction</td>\n",
       "      <td>0.0620</td>\n",
       "      <td>govern</td>\n",
       "      <td>0.0372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>natur</td>\n",
       "      <td>0.0307</td>\n",
       "      <td>theori</td>\n",
       "      <td>0.0351</td>\n",
       "      <td>norm</td>\n",
       "      <td>0.0374</td>\n",
       "      <td>charact</td>\n",
       "      <td>0.0457</td>\n",
       "      <td>hygien</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>collaps</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>person</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>feel</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>game</td>\n",
       "      <td>0.0467</td>\n",
       "      <td>process</td>\n",
       "      <td>0.0368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>resourc</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>valu</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>use</td>\n",
       "      <td>0.0369</td>\n",
       "      <td>one</td>\n",
       "      <td>0.0419</td>\n",
       "      <td>wast</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>time</td>\n",
       "      <td>0.0363</td>\n",
       "      <td>societi</td>\n",
       "      <td>0.0379</td>\n",
       "      <td>love</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>stori</td>\n",
       "      <td>0.0464</td>\n",
       "      <td>idea</td>\n",
       "      <td>0.0356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>justic</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>law</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>may</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>sin</td>\n",
       "      <td>0.0418</td>\n",
       "      <td>safeti</td>\n",
       "      <td>0.0342</td>\n",
       "      <td>milgram</td>\n",
       "      <td>0.0359</td>\n",
       "      <td>econom</td>\n",
       "      <td>0.0378</td>\n",
       "      <td>depress</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>stimulu</td>\n",
       "      <td>0.0372</td>\n",
       "      <td>theori</td>\n",
       "      <td>0.0350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>polit</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>justic</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>one</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>ethic</td>\n",
       "      <td>0.0413</td>\n",
       "      <td>peac</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>wealth</td>\n",
       "      <td>0.0352</td>\n",
       "      <td>class</td>\n",
       "      <td>0.0377</td>\n",
       "      <td>anxieti</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>receptor</td>\n",
       "      <td>0.0329</td>\n",
       "      <td>play</td>\n",
       "      <td>0.0336</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>state</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>peopl</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>person</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>folklor</td>\n",
       "      <td>0.0390</td>\n",
       "      <td>product</td>\n",
       "      <td>0.0327</td>\n",
       "      <td>individu</td>\n",
       "      <td>0.0349</td>\n",
       "      <td>incom</td>\n",
       "      <td>0.0357</td>\n",
       "      <td>may</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>use</td>\n",
       "      <td>0.0324</td>\n",
       "      <td>emot</td>\n",
       "      <td>0.0333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>world</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>plato</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>social</td>\n",
       "      <td>0.0323</td>\n",
       "      <td>seven</td>\n",
       "      <td>0.0353</td>\n",
       "      <td>cleanroom</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>petti</td>\n",
       "      <td>0.0338</td>\n",
       "      <td>power</td>\n",
       "      <td>0.0339</td>\n",
       "      <td>desir</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>fantasi</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>unit</td>\n",
       "      <td>0.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>equal</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>person</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>formal</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>ascet</td>\n",
       "      <td>0.0350</td>\n",
       "      <td>state</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>veto</td>\n",
       "      <td>0.0333</td>\n",
       "      <td>achiev</td>\n",
       "      <td>0.0337</td>\n",
       "      <td>experi</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>may</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>sternberg</td>\n",
       "      <td>0.0330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>law</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>love</td>\n",
       "      <td>0.0245</td>\n",
       "      <td>etiquett</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>greek</td>\n",
       "      <td>0.0346</td>\n",
       "      <td>cleanli</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>boundari</td>\n",
       "      <td>0.0330</td>\n",
       "      <td>peopl</td>\n",
       "      <td>0.0334</td>\n",
       "      <td>self</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>drama</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>improvis</td>\n",
       "      <td>0.0317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>nation</td>\n",
       "      <td>0.0264</td>\n",
       "      <td>self</td>\n",
       "      <td>0.0242</td>\n",
       "      <td>conform</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>madonna</td>\n",
       "      <td>0.0344</td>\n",
       "      <td>homeland</td>\n",
       "      <td>0.0320</td>\n",
       "      <td>use</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>work</td>\n",
       "      <td>0.0332</td>\n",
       "      <td>affect</td>\n",
       "      <td>0.0265</td>\n",
       "      <td>voyag</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>declar</td>\n",
       "      <td>0.0311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>sustain</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>welfar</td>\n",
       "      <td>0.0239</td>\n",
       "      <td>tablet</td>\n",
       "      <td>0.0292</td>\n",
       "      <td>aido</td>\n",
       "      <td>0.0340</td>\n",
       "      <td>behaviour</td>\n",
       "      <td>0.0304</td>\n",
       "      <td>societi</td>\n",
       "      <td>0.0295</td>\n",
       "      <td>develop</td>\n",
       "      <td>0.0328</td>\n",
       "      <td>life</td>\n",
       "      <td>0.0257</td>\n",
       "      <td>televis</td>\n",
       "      <td>0.0280</td>\n",
       "      <td>freedom</td>\n",
       "      <td>0.0309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>movement</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>repent</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>idol</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>ascetic</td>\n",
       "      <td>0.0321</td>\n",
       "      <td>cooper</td>\n",
       "      <td>0.0302</td>\n",
       "      <td>media</td>\n",
       "      <td>0.0294</td>\n",
       "      <td>rank</td>\n",
       "      <td>0.0326</td>\n",
       "      <td>social</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>sensori</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>brainstorm</td>\n",
       "      <td>0.0302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>global</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>help</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>pieta</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>aparigraha</td>\n",
       "      <td>0.0313</td>\n",
       "      <td>cleaner</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>manag</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>affili</td>\n",
       "      <td>0.0318</td>\n",
       "      <td>also</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>tourist</td>\n",
       "      <td>0.0270</td>\n",
       "      <td>gener</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>speci</td>\n",
       "      <td>0.0256</td>\n",
       "      <td>lie</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>adulteri</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>pharise</td>\n",
       "      <td>0.0287</td>\n",
       "      <td>nation</td>\n",
       "      <td>0.0300</td>\n",
       "      <td>person</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>mcclelland</td>\n",
       "      <td>0.0296</td>\n",
       "      <td>peopl</td>\n",
       "      <td>0.0230</td>\n",
       "      <td>countri</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>curios</td>\n",
       "      <td>0.0285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>organ</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>sincer</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>child</td>\n",
       "      <td>0.0281</td>\n",
       "      <td>parabl</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0299</td>\n",
       "      <td>project</td>\n",
       "      <td>0.0266</td>\n",
       "      <td>organiz</td>\n",
       "      <td>0.0289</td>\n",
       "      <td>individu</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>discov</td>\n",
       "      <td>0.0268</td>\n",
       "      <td>task</td>\n",
       "      <td>0.0279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>use</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>individu</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>shall</td>\n",
       "      <td>0.0277</td>\n",
       "      <td>deadli</td>\n",
       "      <td>0.0279</td>\n",
       "      <td>human</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>remov</td>\n",
       "      <td>0.0258</td>\n",
       "      <td>stratif</td>\n",
       "      <td>0.0286</td>\n",
       "      <td>regret</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>includ</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>bulli</td>\n",
       "      <td>0.0277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>green</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>compass</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>maxim</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>god</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>use</td>\n",
       "      <td>0.0267</td>\n",
       "      <td>may</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>high</td>\n",
       "      <td>0.0285</td>\n",
       "      <td>joy</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>music</td>\n",
       "      <td>0.0263</td>\n",
       "      <td>secess</td>\n",
       "      <td>0.0277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>develop</td>\n",
       "      <td>0.0234</td>\n",
       "      <td>natur</td>\n",
       "      <td>0.0220</td>\n",
       "      <td>respect</td>\n",
       "      <td>0.0269</td>\n",
       "      <td>self</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>individu</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>experi</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>influenc</td>\n",
       "      <td>0.0283</td>\n",
       "      <td>shame</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>european</td>\n",
       "      <td>0.0255</td>\n",
       "      <td>state</td>\n",
       "      <td>0.0273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>environ</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>trust</td>\n",
       "      <td>0.0217</td>\n",
       "      <td>worship</td>\n",
       "      <td>0.0262</td>\n",
       "      <td>publican</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>may</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>truth</td>\n",
       "      <td>0.0248</td>\n",
       "      <td>labour</td>\n",
       "      <td>0.0282</td>\n",
       "      <td>disord</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>expedit</td>\n",
       "      <td>0.0254</td>\n",
       "      <td>perform</td>\n",
       "      <td>0.0268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>univers</td>\n",
       "      <td>0.0231</td>\n",
       "      <td>social</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>rigour</td>\n",
       "      <td>0.0260</td>\n",
       "      <td>church</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>stratif</td>\n",
       "      <td>0.0225</td>\n",
       "      <td>cratolog</td>\n",
       "      <td>0.0246</td>\n",
       "      <td>group</td>\n",
       "      <td>0.0276</td>\n",
       "      <td>behavior</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>tour</td>\n",
       "      <td>0.0241</td>\n",
       "      <td>new</td>\n",
       "      <td>0.0264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>econom</td>\n",
       "      <td>0.0226</td>\n",
       "      <td>pardon</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>deindividu</td>\n",
       "      <td>0.0253</td>\n",
       "      <td>goddess</td>\n",
       "      <td>0.0249</td>\n",
       "      <td>one</td>\n",
       "      <td>0.0223</td>\n",
       "      <td>govern</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>product</td>\n",
       "      <td>0.0274</td>\n",
       "      <td>embarrass</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>also</td>\n",
       "      <td>0.0235</td>\n",
       "      <td>use</td>\n",
       "      <td>0.0255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>conserv</td>\n",
       "      <td>0.0219</td>\n",
       "      <td>may</td>\n",
       "      <td>0.0209</td>\n",
       "      <td>children</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>origin</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>societi</td>\n",
       "      <td>0.0222</td>\n",
       "      <td>peopl</td>\n",
       "      <td>0.0237</td>\n",
       "      <td>one</td>\n",
       "      <td>0.0272</td>\n",
       "      <td>guilt</td>\n",
       "      <td>0.0216</td>\n",
       "      <td>subgenr</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>cognit</td>\n",
       "      <td>0.0254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>unit</td>\n",
       "      <td>0.0214</td>\n",
       "      <td>honesti</td>\n",
       "      <td>0.0208</td>\n",
       "      <td>authoritarian</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>humbl</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>system</td>\n",
       "      <td>0.0218</td>\n",
       "      <td>commun</td>\n",
       "      <td>0.0233</td>\n",
       "      <td>employe</td>\n",
       "      <td>0.0271</td>\n",
       "      <td>envi</td>\n",
       "      <td>0.0215</td>\n",
       "      <td>destin</td>\n",
       "      <td>0.0228</td>\n",
       "      <td>self</td>\n",
       "      <td>0.0251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33109</th>\n",
       "      <td>makeshift</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nippl</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omoto</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orgast</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ombudsman</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>or</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onit</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neocommun</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ovid</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operaia</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33110</th>\n",
       "      <td>makeyouwel</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nippon</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnivor</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orgasmo</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ombrir</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opu</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ongana</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neocort</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overshadow</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opaqu</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33111</th>\n",
       "      <td>makhnov</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nipponzan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olympiad</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orderli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olier</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operaio</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omn</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neofasc</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overthrow</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onesto</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33112</th>\n",
       "      <td>makinen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nijholt</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnism</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orgasm</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omax</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>optimum</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onfray</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negri</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oversho</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ootacamund</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33113</th>\n",
       "      <td>makoto</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nijhoff</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olympian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ordin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oligarch</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operandi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnia</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negofemin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oversight</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oneworld</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33114</th>\n",
       "      <td>mal</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nijenhui</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>om</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ordinarili</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oligarchi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operation</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnibu</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neapolitan</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oversimplif</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onfray</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33115</th>\n",
       "      <td>mala</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigeria</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omaha</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ordoliber</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oligocen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operationalis</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnipot</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neechi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oversimplifi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ongana</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33116</th>\n",
       "      <td>malaa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>niel</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omak</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ordovician</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olik</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operationalist</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnipotenct</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nearest</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overst</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onion</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33117</th>\n",
       "      <td>malabar</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nielsen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omalley</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ordzi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operatornam</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnisci</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neatli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overstat</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onit</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33118</th>\n",
       "      <td>malachi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>niemand</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oman</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ore</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oliph</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operculari</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnism</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nebuchadnezzar</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overstay</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onkar</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33119</th>\n",
       "      <td>majorityof</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>niemcewicz</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omar</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oregon</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olivel</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operett</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnivor</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nebul</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overstep</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onlook</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33120</th>\n",
       "      <td>majli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>niet</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omax</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orellana</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olivero</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>operon</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnivori</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>necag</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overstretch</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onnect</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33121</th>\n",
       "      <td>maiden</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nieto</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ombrir</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oren</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olivi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ophelia</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omoto</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>necesario</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overt</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ono</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33122</th>\n",
       "      <td>majest</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nieuwenhui</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ombud</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orend</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olivia</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ophi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>on</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>necklac</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overtak</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onondaga</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33123</th>\n",
       "      <td>maidserv</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigamananda</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ombudsman</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orest</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oliviero</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onam</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nectar</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overtaken</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onset</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33124</th>\n",
       "      <td>maier</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigel</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omega</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orf</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ollman</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opiat</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oncogenesi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neech</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overthrown</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onsit</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33125</th>\n",
       "      <td>mailonlin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>niger</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omelett</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orff</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ollut</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opioid</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oncolog</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neecha</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overzeal</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onslaught</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33126</th>\n",
       "      <td>maim</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigerian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>orfrom</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olmifon</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opium</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oncologist</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>needham</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overtim</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ontic</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33127</th>\n",
       "      <td>maimoned</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>niip</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omic</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organel</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olof</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oponopono</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oncorhynchu</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negev</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overtli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ontogenet</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33128</th>\n",
       "      <td>mainfram</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigger</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organigraph</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olomouc</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opp</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onda</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>needi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overton</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ontogeni</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33129</th>\n",
       "      <td>mainspr</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigh</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omiss</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organisationalist</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olusegun</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oppen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ondansetron</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>needless</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overtook</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ontolog</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33130</th>\n",
       "      <td>mainstay</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nightclub</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omitara</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organischen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olympia</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oppervlaktewat</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ondual</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>needlessli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overtop</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ontrari</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33131</th>\n",
       "      <td>maint</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nightingal</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omn</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organism</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olympiad</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oppon</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onegin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neeed</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overturn</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onward</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33132</th>\n",
       "      <td>mainyu</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nightli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnia</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organismen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>olympian</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opportunist</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oneida</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neef</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overus</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oo</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33133</th>\n",
       "      <td>maio</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nightmar</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnibu</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organiz</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>om</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>opportunitythen</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oneil</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>neff</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overvalu</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oocyt</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33134</th>\n",
       "      <td>maitani</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nighttim</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnipot</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organochlorin</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omak</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oprah</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oneiroi</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negara</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overweight</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>ooda</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33135</th>\n",
       "      <td>maitri</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigra</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnipotenct</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organopesticid</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omalley</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oprc</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onesthey</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negash</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overwhelm</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oophagi</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33136</th>\n",
       "      <td>maiz</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nigrostriat</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnipres</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organophosph</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oman</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>optimis</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>onesto</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negativ</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overwhelmingli</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oord</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33137</th>\n",
       "      <td>majah</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>nih</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omnisci</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>organum</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>omar</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>optimist</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oneworld</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>negel</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>overwork</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>oost</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33138</th>\n",
       "      <td>legibl</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>legibl</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>aa</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33139 rows  20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      universalism - word  universalism - score benevolence - word  \\\n",
       "0             environment                0.0701              moral   \n",
       "1                    peac                0.0590               good   \n",
       "2                  ecolog                0.0542              ethic   \n",
       "3                  social                0.0448              truth   \n",
       "4                   right                0.0425                god   \n",
       "5                   human                0.0338               evil   \n",
       "6                  intern                0.0332             forgiv   \n",
       "7                     war                0.0314                one   \n",
       "8                   natur                0.0307             theori   \n",
       "9                 resourc                0.0292               valu   \n",
       "10                 justic                0.0282                law   \n",
       "11                  polit                0.0274             justic   \n",
       "12                  state                0.0269              peopl   \n",
       "13                  world                0.0268              plato   \n",
       "14                  equal                0.0267             person   \n",
       "15                    law                0.0266               love   \n",
       "16                 nation                0.0264               self   \n",
       "17                sustain                0.0262             welfar   \n",
       "18               movement                0.0258             repent   \n",
       "19                 global                0.0258               help   \n",
       "20                  speci                0.0256                lie   \n",
       "21                  organ                0.0255             sincer   \n",
       "22                    use                0.0254           individu   \n",
       "23                  green                0.0250            compass   \n",
       "24                develop                0.0234              natur   \n",
       "25                environ                0.0231              trust   \n",
       "26                univers                0.0231             social   \n",
       "27                 econom                0.0226             pardon   \n",
       "28                conserv                0.0219                may   \n",
       "29                   unit                0.0214            honesti   \n",
       "...                   ...                   ...                ...   \n",
       "33109           makeshift                0.0000              nippl   \n",
       "33110          makeyouwel                0.0000             nippon   \n",
       "33111             makhnov                0.0000          nipponzan   \n",
       "33112             makinen                0.0000            nijholt   \n",
       "33113              makoto                0.0000            nijhoff   \n",
       "33114                 mal                0.0000           nijenhui   \n",
       "33115                mala                0.0000            nigeria   \n",
       "33116               malaa                0.0000               niel   \n",
       "33117             malabar                0.0000            nielsen   \n",
       "33118             malachi                0.0000            niemand   \n",
       "33119          majorityof                0.0000         niemcewicz   \n",
       "33120               majli                0.0000               niet   \n",
       "33121              maiden                0.0000              nieto   \n",
       "33122              majest                0.0000         nieuwenhui   \n",
       "33123            maidserv                0.0000        nigamananda   \n",
       "33124               maier                0.0000              nigel   \n",
       "33125           mailonlin                0.0000              niger   \n",
       "33126                maim                0.0000           nigerian   \n",
       "33127            maimoned                0.0000               niip   \n",
       "33128            mainfram                0.0000             nigger   \n",
       "33129             mainspr                0.0000               nigh   \n",
       "33130            mainstay                0.0000          nightclub   \n",
       "33131               maint                0.0000         nightingal   \n",
       "33132              mainyu                0.0000            nightli   \n",
       "33133                maio                0.0000           nightmar   \n",
       "33134             maitani                0.0000           nighttim   \n",
       "33135              maitri                0.0000              nigra   \n",
       "33136                maiz                0.0000        nigrostriat   \n",
       "33137               majah                0.0000                nih   \n",
       "33138              legibl                0.0000                 aa   \n",
       "\n",
       "       benevolence - score conformity - word  conformity - score  \\\n",
       "0                   0.0810               god              0.0843   \n",
       "1                   0.0739           command              0.0582   \n",
       "2                   0.0622            cultur              0.0511   \n",
       "3                   0.0485            parent              0.0477   \n",
       "4                   0.0410         disciplin              0.0466   \n",
       "5                   0.0406             group              0.0463   \n",
       "6                   0.0393             polit              0.0450   \n",
       "7                   0.0354          behavior              0.0433   \n",
       "8                   0.0351              norm              0.0374   \n",
       "9                   0.0342               use              0.0369   \n",
       "10                  0.0285               may              0.0332   \n",
       "11                  0.0272               one              0.0330   \n",
       "12                  0.0258            person              0.0326   \n",
       "13                  0.0249            social              0.0323   \n",
       "14                  0.0245            formal              0.0306   \n",
       "15                  0.0245          etiquett              0.0306   \n",
       "16                  0.0242           conform              0.0296   \n",
       "17                  0.0239            tablet              0.0292   \n",
       "18                  0.0237              idol              0.0285   \n",
       "19                  0.0233             pieta              0.0283   \n",
       "20                  0.0232          adulteri              0.0281   \n",
       "21                  0.0228             child              0.0281   \n",
       "22                  0.0222             shall              0.0277   \n",
       "23                  0.0222             maxim              0.0274   \n",
       "24                  0.0220           respect              0.0269   \n",
       "25                  0.0217           worship              0.0262   \n",
       "26                  0.0216            rigour              0.0260   \n",
       "27                  0.0214        deindividu              0.0253   \n",
       "28                  0.0209          children              0.0251   \n",
       "29                  0.0208     authoritarian              0.0250   \n",
       "...                    ...               ...                 ...   \n",
       "33109               0.0000             omoto              0.0000   \n",
       "33110               0.0000           omnivor              0.0000   \n",
       "33111               0.0000          olympiad              0.0000   \n",
       "33112               0.0000            omnism              0.0000   \n",
       "33113               0.0000          olympian              0.0000   \n",
       "33114               0.0000                om              0.0000   \n",
       "33115               0.0000             omaha              0.0000   \n",
       "33116               0.0000              omak              0.0000   \n",
       "33117               0.0000           omalley              0.0000   \n",
       "33118               0.0000              oman              0.0000   \n",
       "33119               0.0000              omar              0.0000   \n",
       "33120               0.0000              omax              0.0000   \n",
       "33121               0.0000            ombrir              0.0000   \n",
       "33122               0.0000             ombud              0.0000   \n",
       "33123               0.0000         ombudsman              0.0000   \n",
       "33124               0.0000             omega              0.0000   \n",
       "33125               0.0000           omelett              0.0000   \n",
       "33126               0.0000               omi              0.0000   \n",
       "33127               0.0000              omic              0.0000   \n",
       "33128               0.0000              omin              0.0000   \n",
       "33129               0.0000             omiss              0.0000   \n",
       "33130               0.0000           omitara              0.0000   \n",
       "33131               0.0000               omn              0.0000   \n",
       "33132               0.0000             omnia              0.0000   \n",
       "33133               0.0000            omnibu              0.0000   \n",
       "33134               0.0000           omnipot              0.0000   \n",
       "33135               0.0000       omnipotenct              0.0000   \n",
       "33136               0.0000          omnipres              0.0000   \n",
       "33137               0.0000           omnisci              0.0000   \n",
       "33138               0.0000                aa              0.0000   \n",
       "\n",
       "        tradition - word  tradition - score security - word  security - score  \\\n",
       "0                  virtu             0.1417        reciproc            0.1349   \n",
       "1                 temper             0.0985           secur            0.1242   \n",
       "2                  humil             0.0921          social            0.0613   \n",
       "3                 tradit             0.0705            norm            0.0578   \n",
       "4                  moral             0.0562           clean            0.0543   \n",
       "5              sophrosyn             0.0547        contamin            0.0535   \n",
       "6              christian             0.0527            risk            0.0449   \n",
       "7                   myth             0.0469          pollut            0.0354   \n",
       "8                charact             0.0457          hygien            0.0352   \n",
       "9                    one             0.0419            wast            0.0346   \n",
       "10                   sin             0.0418          safeti            0.0342   \n",
       "11                 ethic             0.0413            peac            0.0332   \n",
       "12               folklor             0.0390         product            0.0327   \n",
       "13                 seven             0.0353       cleanroom            0.0326   \n",
       "14                 ascet             0.0350           state            0.0320   \n",
       "15                 greek             0.0346         cleanli            0.0320   \n",
       "16               madonna             0.0344        homeland            0.0320   \n",
       "17                  aido             0.0340       behaviour            0.0304   \n",
       "18               ascetic             0.0321          cooper            0.0302   \n",
       "19            aparigraha             0.0313         cleaner            0.0301   \n",
       "20               pharise             0.0287          nation            0.0300   \n",
       "21                parabl             0.0282           group            0.0299   \n",
       "22                deadli             0.0279           human            0.0272   \n",
       "23                   god             0.0274             use            0.0267   \n",
       "24                  self             0.0271        individu            0.0254   \n",
       "25              publican             0.0260             may            0.0249   \n",
       "26                church             0.0249         stratif            0.0225   \n",
       "27               goddess             0.0249             one            0.0223   \n",
       "28                origin             0.0228         societi            0.0222   \n",
       "29                 humbl             0.0227          system            0.0218   \n",
       "...                  ...                ...             ...               ...   \n",
       "33109             orgast             0.0000       ombudsman            0.0000   \n",
       "33110            orgasmo             0.0000          ombrir            0.0000   \n",
       "33111            orderli             0.0000           olier            0.0000   \n",
       "33112             orgasm             0.0000            omax            0.0000   \n",
       "33113              ordin             0.0000        oligarch            0.0000   \n",
       "33114         ordinarili             0.0000       oligarchi            0.0000   \n",
       "33115          ordoliber             0.0000        oligocen            0.0000   \n",
       "33116         ordovician             0.0000            olik            0.0000   \n",
       "33117              ordzi             0.0000            olin            0.0000   \n",
       "33118                ore             0.0000           oliph            0.0000   \n",
       "33119             oregon             0.0000          olivel            0.0000   \n",
       "33120           orellana             0.0000         olivero            0.0000   \n",
       "33121               oren             0.0000           olivi            0.0000   \n",
       "33122              orend             0.0000          olivia            0.0000   \n",
       "33123              orest             0.0000        oliviero            0.0000   \n",
       "33124                orf             0.0000          ollman            0.0000   \n",
       "33125               orff             0.0000           ollut            0.0000   \n",
       "33126             orfrom             0.0000         olmifon            0.0000   \n",
       "33127            organel             0.0000            olof            0.0000   \n",
       "33128        organigraph             0.0000         olomouc            0.0000   \n",
       "33129  organisationalist             0.0000        olusegun            0.0000   \n",
       "33130        organischen             0.0000         olympia            0.0000   \n",
       "33131           organism             0.0000        olympiad            0.0000   \n",
       "33132         organismen             0.0000        olympian            0.0000   \n",
       "33133            organiz             0.0000              om            0.0000   \n",
       "33134      organochlorin             0.0000            omak            0.0000   \n",
       "33135     organopesticid             0.0000         omalley            0.0000   \n",
       "33136       organophosph             0.0000            oman            0.0000   \n",
       "33137            organum             0.0000            omar            0.0000   \n",
       "33138                 aa             0.0000              aa            0.0000   \n",
       "\n",
       "          power - word  power - score achievement - word  achievement - score  \\\n",
       "0                power         0.1135              capit               0.1522   \n",
       "1               author         0.1116              statu               0.0813   \n",
       "2               social         0.0481             social               0.0796   \n",
       "3                domin         0.0468               need               0.0458   \n",
       "4              revolut         0.0414              human               0.0426   \n",
       "5              control         0.0405           individu               0.0424   \n",
       "6                polit         0.0379              manag               0.0413   \n",
       "7                state         0.0365         intellectu               0.0382   \n",
       "8              collaps         0.0363             person               0.0381   \n",
       "9                 time         0.0363            societi               0.0379   \n",
       "10             milgram         0.0359             econom               0.0378   \n",
       "11              wealth         0.0352              class               0.0377   \n",
       "12            individu         0.0349              incom               0.0357   \n",
       "13               petti         0.0338              power               0.0339   \n",
       "14                veto         0.0333             achiev               0.0337   \n",
       "15            boundari         0.0330              peopl               0.0334   \n",
       "16                 use         0.0316               work               0.0332   \n",
       "17             societi         0.0295            develop               0.0328   \n",
       "18               media         0.0294               rank               0.0326   \n",
       "19               manag         0.0282             affili               0.0318   \n",
       "20              person         0.0269         mcclelland               0.0296   \n",
       "21             project         0.0266            organiz               0.0289   \n",
       "22               remov         0.0258            stratif               0.0286   \n",
       "23                 may         0.0254               high               0.0285   \n",
       "24              experi         0.0251           influenc               0.0283   \n",
       "25               truth         0.0248             labour               0.0282   \n",
       "26            cratolog         0.0246              group               0.0276   \n",
       "27              govern         0.0244            product               0.0274   \n",
       "28               peopl         0.0237                one               0.0272   \n",
       "29              commun         0.0233            employe               0.0271   \n",
       "...                ...            ...                ...                  ...   \n",
       "33109               or         0.0000               onit               0.0000   \n",
       "33110              opu         0.0000             ongana               0.0000   \n",
       "33111          operaio         0.0000                omn               0.0000   \n",
       "33112          optimum         0.0000             onfray               0.0000   \n",
       "33113         operandi         0.0000              omnia               0.0000   \n",
       "33114        operation         0.0000             omnibu               0.0000   \n",
       "33115    operationalis         0.0000            omnipot               0.0000   \n",
       "33116   operationalist         0.0000        omnipotenct               0.0000   \n",
       "33117      operatornam         0.0000            omnisci               0.0000   \n",
       "33118       operculari         0.0000             omnism               0.0000   \n",
       "33119          operett         0.0000            omnivor               0.0000   \n",
       "33120           operon         0.0000           omnivori               0.0000   \n",
       "33121          ophelia         0.0000              omoto               0.0000   \n",
       "33122             ophi         0.0000                 on               0.0000   \n",
       "33123              opi         0.0000               onam               0.0000   \n",
       "33124            opiat         0.0000         oncogenesi               0.0000   \n",
       "33125           opioid         0.0000            oncolog               0.0000   \n",
       "33126            opium         0.0000         oncologist               0.0000   \n",
       "33127        oponopono         0.0000        oncorhynchu               0.0000   \n",
       "33128              opp         0.0000               onda               0.0000   \n",
       "33129            oppen         0.0000        ondansetron               0.0000   \n",
       "33130   oppervlaktewat         0.0000             ondual               0.0000   \n",
       "33131            oppon         0.0000             onegin               0.0000   \n",
       "33132      opportunist         0.0000             oneida               0.0000   \n",
       "33133  opportunitythen         0.0000              oneil               0.0000   \n",
       "33134            oprah         0.0000            oneiroi               0.0000   \n",
       "33135             oprc         0.0000           onesthey               0.0000   \n",
       "33136          optimis         0.0000             onesto               0.0000   \n",
       "33137         optimist         0.0000           oneworld               0.0000   \n",
       "33138               aa         0.0000                 aa               0.0000   \n",
       "\n",
       "      hedonism - word  hedonism - score stimulation - word  \\\n",
       "0             pleasur            0.0727             travel   \n",
       "1               happi            0.0631              sport   \n",
       "2                emot            0.0525           adventur   \n",
       "3                pain            0.0407            tourism   \n",
       "4               hedon            0.0402             explor   \n",
       "5              person            0.0385             stimul   \n",
       "6           psycholog            0.0350               genr   \n",
       "7                 one            0.0342            fiction   \n",
       "8                feel            0.0321               game   \n",
       "9                love            0.0303              stori   \n",
       "10            depress            0.0294            stimulu   \n",
       "11            anxieti            0.0279           receptor   \n",
       "12                may            0.0276                use   \n",
       "13              desir            0.0276            fantasi   \n",
       "14             experi            0.0268                may   \n",
       "15               self            0.0266              drama   \n",
       "16             affect            0.0265              voyag   \n",
       "17               life            0.0257            televis   \n",
       "18             social            0.0256            sensori   \n",
       "19               also            0.0233            tourist   \n",
       "20              peopl            0.0230            countri   \n",
       "21           individu            0.0227             discov   \n",
       "22             regret            0.0225             includ   \n",
       "23                joy            0.0220              music   \n",
       "24              shame            0.0219           european   \n",
       "25             disord            0.0218            expedit   \n",
       "26           behavior            0.0218               tour   \n",
       "27          embarrass            0.0216               also   \n",
       "28              guilt            0.0216            subgenr   \n",
       "29               envi            0.0215             destin   \n",
       "...               ...               ...                ...   \n",
       "33109       neocommun            0.0000               ovid   \n",
       "33110         neocort            0.0000         overshadow   \n",
       "33111         neofasc            0.0000          overthrow   \n",
       "33112           negri            0.0000            oversho   \n",
       "33113       negofemin            0.0000          oversight   \n",
       "33114      neapolitan            0.0000        oversimplif   \n",
       "33115          neechi            0.0000       oversimplifi   \n",
       "33116         nearest            0.0000             overst   \n",
       "33117          neatli            0.0000           overstat   \n",
       "33118  nebuchadnezzar            0.0000           overstay   \n",
       "33119           nebul            0.0000           overstep   \n",
       "33120           necag            0.0000        overstretch   \n",
       "33121       necesario            0.0000              overt   \n",
       "33122         necklac            0.0000            overtak   \n",
       "33123          nectar            0.0000          overtaken   \n",
       "33124           neech            0.0000         overthrown   \n",
       "33125          neecha            0.0000           overzeal   \n",
       "33126         needham            0.0000            overtim   \n",
       "33127           negev            0.0000            overtli   \n",
       "33128           needi            0.0000            overton   \n",
       "33129        needless            0.0000           overtook   \n",
       "33130      needlessli            0.0000            overtop   \n",
       "33131           neeed            0.0000           overturn   \n",
       "33132            neef            0.0000             overus   \n",
       "33133            neff            0.0000           overvalu   \n",
       "33134          negara            0.0000         overweight   \n",
       "33135          negash            0.0000          overwhelm   \n",
       "33136         negativ            0.0000     overwhelmingli   \n",
       "33137           negel            0.0000           overwork   \n",
       "33138          legibl            0.0000                 aa   \n",
       "\n",
       "       stimulation - score self-direction - word  self-direction - score  \n",
       "0                   0.1690               creativ                  0.0880  \n",
       "1                   0.1499              independ                  0.0648  \n",
       "2                   0.1471              intellig                  0.0532  \n",
       "3                   0.1462                    ye                  0.0518  \n",
       "4                   0.1033                invent                  0.0468  \n",
       "5                   0.1031                 innov                  0.0386  \n",
       "6                   0.0664             territori                  0.0376  \n",
       "7                   0.0620                govern                  0.0372  \n",
       "8                   0.0467               process                  0.0368  \n",
       "9                   0.0464                  idea                  0.0356  \n",
       "10                  0.0372                theori                  0.0350  \n",
       "11                  0.0329                  play                  0.0336  \n",
       "12                  0.0324                  emot                  0.0333  \n",
       "13                  0.0306                  unit                  0.0330  \n",
       "14                  0.0299             sternberg                  0.0330  \n",
       "15                  0.0296              improvis                  0.0317  \n",
       "16                  0.0292                declar                  0.0311  \n",
       "17                  0.0280               freedom                  0.0309  \n",
       "18                  0.0271            brainstorm                  0.0302  \n",
       "19                  0.0270                 gener                  0.0285  \n",
       "20                  0.0268                curios                  0.0285  \n",
       "21                  0.0268                  task                  0.0279  \n",
       "22                  0.0263                 bulli                  0.0277  \n",
       "23                  0.0263                secess                  0.0277  \n",
       "24                  0.0255                 state                  0.0273  \n",
       "25                  0.0254               perform                  0.0268  \n",
       "26                  0.0241                   new                  0.0264  \n",
       "27                  0.0235                   use                  0.0255  \n",
       "28                  0.0228                cognit                  0.0254  \n",
       "29                  0.0228                  self                  0.0251  \n",
       "...                    ...                   ...                     ...  \n",
       "33109               0.0000               operaia                  0.0000  \n",
       "33110               0.0000                 opaqu                  0.0000  \n",
       "33111               0.0000                onesto                  0.0000  \n",
       "33112               0.0000            ootacamund                  0.0000  \n",
       "33113               0.0000              oneworld                  0.0000  \n",
       "33114               0.0000                onfray                  0.0000  \n",
       "33115               0.0000                ongana                  0.0000  \n",
       "33116               0.0000                 onion                  0.0000  \n",
       "33117               0.0000                  onit                  0.0000  \n",
       "33118               0.0000                 onkar                  0.0000  \n",
       "33119               0.0000                onlook                  0.0000  \n",
       "33120               0.0000                onnect                  0.0000  \n",
       "33121               0.0000                   ono                  0.0000  \n",
       "33122               0.0000              onondaga                  0.0000  \n",
       "33123               0.0000                 onset                  0.0000  \n",
       "33124               0.0000                 onsit                  0.0000  \n",
       "33125               0.0000             onslaught                  0.0000  \n",
       "33126               0.0000                 ontic                  0.0000  \n",
       "33127               0.0000             ontogenet                  0.0000  \n",
       "33128               0.0000              ontogeni                  0.0000  \n",
       "33129               0.0000               ontolog                  0.0000  \n",
       "33130               0.0000               ontrari                  0.0000  \n",
       "33131               0.0000                onward                  0.0000  \n",
       "33132               0.0000                    oo                  0.0000  \n",
       "33133               0.0000                 oocyt                  0.0000  \n",
       "33134               0.0000                  ooda                  0.0000  \n",
       "33135               0.0000               oophagi                  0.0000  \n",
       "33136               0.0000                  oord                  0.0000  \n",
       "33137               0.0000                  oost                  0.0000  \n",
       "33138               0.0000                    aa                  0.0000  \n",
       "\n",
       "[33139 rows x 20 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "cumul = 0\n",
    "word_list = []\n",
    "\n",
    "theme_counts = data.groupby(['theme.id','theme']).count().iloc[:,1]\n",
    "pd_theme_counts = pd.DataFrame(theme_counts)\n",
    "\n",
    "for i, row in pd.DataFrame(theme_counts).iterrows():\n",
    "    tmp_list = []\n",
    "    tfidf_avg = np.average(tfidf[cumul:cumul+int(row['document.id'])].toarray(), axis=0)\n",
    "    cumul += int(row['document.id'])\n",
    "    \n",
    "    for idx in list(reversed(tfidf_avg.argsort())):\n",
    "        tmp_list.append((feature_names[idx], np.round(tfidf_avg[idx], 4)))\n",
    "        \n",
    "    word_list.append(tmp_list)\n",
    "\n",
    "schwartz_word_score = []\n",
    "for sch in schwartz:\n",
    "    schwartz_word_score.append(word_list[categories.index(sch)])\n",
    "\n",
    "df_list = []\n",
    "for i, a in enumerate(schwartz_word_score):\n",
    "    df_list.append(pd.DataFrame(a, columns=[schwartz[i]+\" - word\", schwartz[i]+\" - score\"]))\n",
    "score_df = pd.concat(df_list, axis=1)\n",
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_df.to_excel(\"wiki_tfidf_average.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
