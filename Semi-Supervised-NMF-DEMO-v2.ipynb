{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All Method\n",
    "\n",
    "Train NMF for each topic separately.\n",
    "\n",
    "Use all Wiki articles as Background Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *\n",
    "\n",
    "import pickle\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "from IPython.display import display\n",
    "\n",
    "import requests\n",
    "import json\n",
    "\n",
    "import re\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc, doc_names):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    \n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - \" + doc_names[doc])\n",
    "    plt.savefig(\"Schwartz_Chart_\" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, theme, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print(color.CYAN + color.BOLD + categories[theme] + color.END)\n",
    "    for topic_idx, topic in enumerate(model[theme].components_):\n",
    "        if topic_idx / n_topics == 1:\n",
    "            break\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_train_results(doc_topic, doc, corpus, data):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_train_doc_topics(data, doc_topic, doc, 11) \n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "    \n",
    "def print_test_results(doc, doc_topic, test_corpusPP, pre_nmf_list, pre_tfidf_vectorizer, word_topic_scores, word_topic_sp,\n",
    "                       corpus, doc_names, pre_trained_doc, purity_score, word_count, only_doc_words):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + \": \" + doc_names[doc] + color.END)\n",
    "    #print()\n",
    "    #print(color.BOLD + \"Text: \" + color.END)\n",
    "    #print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    \n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_test_doc_topics(doc_topic, doc, 11)\n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc, doc_names)\n",
    "    print()\n",
    "    \n",
    "    df_scores = schwartz_word_scores(doc, W_test_norm, test_corpusPP, word_topic_scores, word_topic_sp, pre_tfidf_vectorizer, purity_score, word_count, only_doc_words)    \n",
    "    \n",
    "    display(df_scores)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm\n",
    "\n",
    "def prepare_export(W, docs, doc_names, filepath):\n",
    "    schwartz_dist = []\n",
    "    for doc in range(len(docs)):\n",
    "        temp_dist = []\n",
    "        for sch in schwartz:\n",
    "            temp_dist.append(W[doc][categories.index(sch)])\n",
    "        schwartz_dist.append(temp_dist)\n",
    "    schwartz_dist = np.asarray(schwartz_dist)\n",
    "    \n",
    "    df = pd.DataFrame(data=schwartz_dist,index = range(len(schwartz_dist)), columns=schwartz)\n",
    "    df['Text'] = docs\n",
    "    df[\"name\"] = doc_names\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def export_to_excel(W, docs, doc_names, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put xlsx as file extension '''\n",
    "    \n",
    "    df = prepare_export(W, docs, doc_names, filepath)\n",
    "    df.to_excel(filepath)\n",
    "    return df\n",
    "\n",
    "def export_to_csv(W, docs, doc_names, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put csv as file extension '''\n",
    "    \n",
    "    df = prepare_export(W, docs, doc_names, filepath)\n",
    "    df.to_csv(filepath)\n",
    "    return df\n",
    "\n",
    "def export_word_scores_excel(W_test_norm, W_test_list, doc_names, pre_trained_doc, filepath, purity_score=False, word_count=10, only_doc_words=False):\n",
    "    writer = pd.ExcelWriter(filepath, engine = 'xlsxwriter')\n",
    "    \n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    word_topic_scores, word_topic_sp = calculate_word_topic_scores(pre_nmf_list, W_test_list)\n",
    "    \n",
    "    for i, dn in enumerate(doc_names):\n",
    "        df = schwartz_word_scores(i, W_test_norm, test_corpusPP, word_topic_scores, word_topic_sp, pre_tfidf_vectorizer, purity_score, word_count, only_doc_words)\n",
    "        dn = re.sub('[\\\\\\:/*?\\[\\]]', '', dn)\n",
    "        df.to_excel(writer, str(i)+'-'+dn[:25])\n",
    "        \n",
    "    writer.save()\n",
    "    writer.close()\n",
    "    \n",
    "def export_doc_tfidf_scores(tfidf_test, doc_names, pre_trained_doc, filepath):\n",
    "    writer = pd.ExcelWriter(filepath, engine = 'xlsxwriter')\n",
    "    \n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    \n",
    "    for i, dn in enumerate(doc_names):\n",
    "        word_list = []\n",
    "        tfidf_doc = tfidf_test[i].toarray()[0]\n",
    "        feature_names = pre_tfidf_vectorizer.get_feature_names()\n",
    "        for idx in list(reversed(tfidf_doc.argsort())):\n",
    "            if tfidf_doc[idx] < 0.0005:\n",
    "                break\n",
    "            word_list.append((feature_names[idx], np.round(tfidf_doc[idx], 3)))\n",
    "\n",
    "        dn = re.sub('[\\\\\\:/*?\\[\\]]', '', dn)\n",
    "        pd.DataFrame(word_list, columns=[\"Word\", \"tf-idf\"]).to_excel(writer, dn[:30])\n",
    "    \n",
    "    writer.save()\n",
    "    writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLinksHTMLaref(page):\n",
    "    \"\"\"\n",
    "\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"a href=\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "def getLinksHTML(page):\n",
    "    \"\"\"\n",
    "\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"href=\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('\"htt', start_link)\n",
    "    end_quote = page.find('\"', start_quote + 1)\n",
    "    url = page[start_quote + 1: end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "def getLinksXML(page):\n",
    "    \"\"\"\n",
    "\n",
    "    :param page: html of web page (here: Python home page) \n",
    "    :return: urls in that page \n",
    "    \"\"\"\n",
    "    start_link = page.find(\"<link/>\")\n",
    "    if start_link == -1:\n",
    "        return None, 0\n",
    "    start_quote = page.find('http', start_link)\n",
    "    end_quote = page.find('<', start_quote )\n",
    "    url = page[start_quote : end_quote]\n",
    "    return url, end_quote\n",
    "\n",
    "\n",
    "def extractFromURL(surl):\n",
    "    response = requests.get(surl)\n",
    "    # parse html\n",
    "    page = str(BeautifulSoup(response.content,\"lxml\"))\n",
    "    is_XML = surl.endswith('xml')\n",
    "    url_list = []\n",
    "    while True:\n",
    "        if is_XML:\n",
    "            url, n = getLinksXML(page)\n",
    "        else:\n",
    "            url, n = getLinksHTML(page)\n",
    "        \n",
    "        page = page[n:]\n",
    "        if url:\n",
    "            if set(url_list).intersection(set(url)) == set() or len(set(url_list).intersection(set(url))) != len(url):\n",
    "                url_list.append(url)\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    page = str(BeautifulSoup(response.content,\"lxml\"))\n",
    "    stlink= surl.find(\"//\")\n",
    "    stlink= surl.find(\"/\",stlink+2 )\n",
    "    base = surl[0:stlink]\n",
    "    while True:\n",
    "        if is_XML:\n",
    "            break\n",
    "        else:\n",
    "            url, n = getLinksHTMLaref(page)\n",
    "        page = page[n:]\n",
    "        if url:\n",
    "            url = base+url\n",
    "            if set(url_list).intersection(set(url)) == set() or len(set(url_list).intersection(set(url))) != len(url):\n",
    "                url_list.append(url)\n",
    "        else:\n",
    "            break\n",
    "            \n",
    "    return url_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_corpus(corpus):\n",
    "    \n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, \n",
    "                extra_process = ['stem'])['TF'])) if doc != '' else '' for doc in corpus]\n",
    "    return PPcorpus\n",
    "    \n",
    "def evaluate_docs(docs, nmf, tfidf_test, betaloss = 'kullback-leibler'):\n",
    "    X_test = tfidf_test\n",
    "    H_test = nmf.components_\n",
    "    \n",
    "    # Fit the NMF model\n",
    "    t0 = time()\n",
    "\n",
    "    W_test = nmf.transform(X_test)\n",
    "    \n",
    "    return W_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_topics(pretrained_filepath):\n",
    "    nmf_list, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    print(\"\\nTopics in NMF model:\")\n",
    "    for i in range(10):\n",
    "        print_top_words(nmf_list, i, tfidf_vectorizer, n_top_words=5, n_topics=3)\n",
    "\n",
    "def add_corpus_txt(filepath, test_corpus):\n",
    "    try:\n",
    "        f = open(filepath, \"r\")\n",
    "        txt = f.read()\n",
    "        test_corpus.append(txt)\n",
    "        f.close()\n",
    "    except:\n",
    "        test_corpus.append(\"\")\n",
    "        print(\"File not found - \" + filepath)\n",
    "\n",
    "\n",
    "def add_corpus_url(url, api_key, test_corpus):\n",
    "    insightIP = 'http://178.62.229.16'\n",
    "    insightPort = '8484'\n",
    "    insightVersion = 'v1.0'\n",
    "\n",
    "    insightSetting = insightIP + ':' + insightPort + '/api/' + insightVersion \n",
    "    request = '/text_analytics/url_scraper?' + 'url=' + url + '&' + 'api_key=' + api_key\n",
    "\n",
    "    # send a request\n",
    "    res = requests.get(insightSetting + request)\n",
    "    if \"Unauthorized Connection\" in res.json():\n",
    "        test_corpus.append(\"\")\n",
    "        print(res.json()[\"Unauthorized Connection\"] + \" - \" + url)\n",
    "    elif \"Error\" in res.json():\n",
    "        test_corpus.append(\"\")\n",
    "        print(res.json()[\"Error\"] + \" - \" + url)\n",
    "    elif \"text\" in res.json():\n",
    "        test_corpus.append(res.json()['text'])\n",
    "        if res.json()['text'] == \"\":\n",
    "            print(\"Empty text - \" + url)\n",
    "    else:\n",
    "        test_corpus.append(\"\")\n",
    "        print(\"Empty text - \" + url)\n",
    "    \n",
    "def evaluate_test_corpus(pretrained_filepath, test_corpus):\n",
    "    nmf_list, tfidf_vectorizer = pickle.load( open( pretrained_filepath, \"rb\" ) )\n",
    "    test_corpusPP = preprocess_corpus(test_corpus)\n",
    "    print()\n",
    "    print('-'*30)\n",
    "    print()\n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    t0 = time()\n",
    "    tfidf_test = tfidf_vectorizer.transform(test_corpusPP)\n",
    "    #tfidf = tfidf_vectorizer.transform(corpusX)\n",
    "    print(tfidf_test.shape[1])\n",
    "    n_features = tfidf_test.shape[1]\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "\n",
    "    W_test_list = []\n",
    "    for i, nmf in enumerate(nmf_list):\n",
    "        print(\"Fitting NMF for \" + str(categories[i]))\n",
    "        W_test = evaluate_docs(test_corpusPP, nmf, tfidf_test, betaloss = 'kullback-leibler')\n",
    "        W_test_list.append(W_test)\n",
    "        \n",
    "    # Sum up sub topics\n",
    "    W_test_norm_list = []\n",
    "    for W in W_test_list:\n",
    "        W_test_cumul = cumulate_W(W, n_topics=3)\n",
    "        W_test_norm = normalize_W(W_test_cumul)\n",
    "        W_test_norm_list.append(W_test_norm)\n",
    "    W_test_norm = np.asarray(W_test_norm_list).T[0]\n",
    "    W_test_norm = np.nan_to_num(W_test_norm)\n",
    "\n",
    "    # cumulated-normalized and raw\n",
    "    return W_test_norm, np.asarray(W_test_list), test_corpusPP, tfidf_test\n",
    "\n",
    "def print_interactive_test_results(W_test_norm, W_test_list, test_corpus, test_corpusPP, doc_names, pre_trained_doc, purity_score, word_count, only_doc_words):\n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    word_topic_scores, word_topic_sp = calculate_word_topic_scores(pre_nmf_list, W_test_list)\n",
    "    \n",
    "    interact(print_test_results,\n",
    "             doc = (0, len(W_test_norm)-1, 1),\n",
    "             doc_topic=fixed(W_test_norm),\n",
    "             test_corpusPP=fixed(test_corpusPP),\n",
    "             pre_nmf_list=fixed(pre_nmf_list),\n",
    "             pre_tfidf_vectorizer=fixed(pre_tfidf_vectorizer),\n",
    "             word_topic_scores=fixed(word_topic_scores),\n",
    "             word_topic_sp=fixed(word_topic_sp),\n",
    "             corpus=fixed(test_corpus),\n",
    "             doc_names=fixed(doc_names),\n",
    "             pre_trained_doc=fixed(pre_trained_doc),\n",
    "             purity_score=fixed(purity_score),\n",
    "             word_count=fixed(word_count),\n",
    "             only_doc_words=fixed(only_doc_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonnegative Matrix Factorization (NMF) method was first proposed by Lee and Seung paper1. The NMF is a method of decomposing a given nonnegative *X* matrix into *W* and *H* factors that contain nonnegative values. The value of the product of the two matrices obtained is approximately equal to the value of the decomposed matrix. In NMF, given a $W \\times K$ nonnegative matrix $X = \\left \\{ x_{\\nu, \\tau} \\right \\}$ where $\\nu = 1:V, i = 1:I \\text{ and } \\tau = 1:T$, we seek nonnegative matrices *W* and *H* such that\n",
    "\n",
    "\\begin{align*}\n",
    "x_{\\nu, \\tau} \\approx \\left [ WH \\right ]_{\\nu, \\tau} = \\sum_{i} w_{\\nu,i}h_{i,\\tau}\n",
    "\\end{align*}\n",
    "\n",
    "In this paper, we will refer to the $V\\times I$ matrix W as the *template matrix*, and $I\\times T$ matrix *H* the *excitation matrix*.\n",
    "\n",
    "\n",
    "$X = WH$\n",
    "\n",
    "$X$: documents X vocabulary. tf-idf is used for vocabulary.\n",
    "\n",
    "$W$: documents X topics. Calculate a seperate W for each Schwartz Value using corresponding H.\n",
    "\n",
    "$H$: topics X vocabulary. Calculate a seperate H for each Schwartz Value in the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating Schwartz Word Scores\n",
    "\n",
    "* We have a fixed (learned) H matrix for each Schwartz Value that holds word-topic distribution.\n",
    "* We have W matrix for each document's Schwartz Values that holds topic-document distribution.\n",
    "* H matrix gave us an idea about the important words for each Schwartz Value (by providing some kind of weights for each word), but actually the weights of those words can be different for each document.\n",
    "* We propose two different methods to calculate those document spesific weighted word scores.\n",
    " * The summary of the approach is as follows: If a word appears in a document frequently (except stopwords) it can be considered as an important word for this document. If this words only occurs in a specific document then it is even more important. This is basically tf-idf which is our essential feature for this model. Moreover, if this word's tf-idf score obtained more from a specific topic rather than background info then we can accept it as an important indicator of this document and topic.\n",
    " * General equation: $X = WH$. Rather than directly using X or H, we figure in W to the calculation.   \n",
    " * Direct Schwartz: Multiply W and H only through the specific Schwartz Value Topics, excluding backgorund.\n",
    " * Purity Schwartz: Find the Schwartz Value purity of each word by taking the proportions of Direct Schwartz Score of this word to Direct Background Score (exclude Schwartz Value, include Backgroun) for each Schwartz Value. Then multiply this purity score with Direct Schwartz score to obtain Purity Schwartz Score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Schwartz Value WH carpimi:**\n",
    "\n",
    "Her Schwartz Value icin hangi kelimelerin daha onemli oldugunu anlamak icin H matrisini inceleyebiliriz. Her H matirisi bir Schwartz Value ve backgorund corpus icin birden cok sub-topic seviyesinde kelime dagilimlarini barindirmkata. Yani 3 sub-topic seviyesinde Universalism ornegi dusunursek, modelimiz tek bir cesit universalism degil de 3 farkli universalism cesidi ogrenmeye calisiyor. Bu da bize her universalism ceisidi icin farkli kelime onemleri sunuyor. Fakat universalism'le alakali en onemli kelimeler ne dendigi zaman sub-topic lerden bahsetmek yerine tek bir cati altinda toplamak genel resmi anlamayi cok daha kolaylastirmakta. \n",
    "\n",
    "Fakat burda sadece H matrisi uzerinden bir toplam yaptigggimiz zaman dokumanlarin hangi Universalism sub-topic iyle alakali oldugu bilgisini atmis olmaktayiz. Bu sebeple her dokumanin neden belirli bir Schwartz Value'ya yoneldigini gosteren kelimeleri highlight etmek icin dokumanlarin sub-topic seviyesinde yoneldikleri Schwartz Value degerleri (W) ile kelimelerin sub-topic seviyesinde gruplandigi Schwartz Value (H) degerlerini carpip topluyoruz. Sonuc olarak bir dokuman icin onu siniflandirmamizda en cok etkileyen kelimeleri Schwartz Value lar arasinda da karsilastirma yapabildigimiz bir skorlama vermis oluyor. \n",
    "\n",
    "**Schwartz Value Purity**\n",
    "\n",
    "Yukarida bahsedilen yontem butun Schwartz Value lar ve kelimeler arasinda goreceli bir karsilastirma yontemi saglamakta Fakat kelimeleri modellemekte kullandigimiz tf-idf ten gelen bir kelimenin bir dokumanda cokca gectigi icin onemi (skorunun) daha fazla gozukmekte. Bir yandan bunun etkisini azaltan ve ayni zamanda Schwartz Value purity konseptini uygulayan bir eklenti yapiyoruz. Kelimelerin her dokuman ve her Schwartz Value icin ne kadar saf oldugunu olcuyoruz. Ve bunu da buldugumuz skorla carpiyoruz. Boylece bu kelime sadece istedigimiz Schwartz Value da geciyorsa skoru gorecelei olarak artmis oluyor. Eger bu kelime cogunlukla istedigimiz Schwartz Value da degil de backgorund corpus ta geciorsa goreceli olarak skoru azalmis oluyor. Bu yontem ile aslinda istedigimiz Schwartz Value ile cok ilgili olmasa da sadece belirli dokumanlarda diger dokumanlara gore daha fazla gectigi icin skoru yuksek olan kelimelerin etkisini azaltmis oluyor. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schwartz Value Word Scores\n",
    "\n",
    "Understanding the behavior of the model is important to make deductions from it. Our model uses words to match the Schwartz Values with documents. The training of the model forms the $H$ matrix, which holds the word-topic distributions for each Schwartz Value. If we have used a classic, simpler NMF model, then, to find the importance order of the words for each Schwartz Value, we can directly take the marginal of $H$ matrix for each topic. But, our model offers much more information with its sub-topics for each Schwartz Values and semi-supervised nature. \n",
    "\n",
    "#### Direct Word Scores\n",
    "\n",
    "Direct word score exploits the sub-topic structure of the model to come up with different word importance scores and orders for each document. $H$ matrix includes different word-distributions for each sub-topic of both a Schwartz Value and Background Corpus. In other words, if there is three sub-topics for \\textit{Power} Schwartz Value in the $H$ matrix, then our model learns three different concept for Power Schwartz Value which provides different word scores for each concept. However, it is more logical to present a  single set of word scores for a Schwartz Value rather than three different word score sets obtained from sub-topics.\n",
    "\n",
    "We can sum up values under sub-topics of H matrix to come up with a single word distribution with the cost of losing valuable sub-topic information. Thus, rather than finding a unified word-topic distribution for all documents, we calculate separate word scores for each document to highlight the important words that lead a document to be soft-classified as a specific Schwartz Value by dot product of documents' sub-topic level Schwartz Value scores ($W$) and words sub-topic level Schwartz Value scores ($H$). As a result, we obtain scores for all words under each Schwartz Value for each document that can be comparable with each other.\n",
    "\n",
    "\\begin{align*}\n",
    "DWS = \\sum_{i = 1}^{I/2} w_{\\nu,i}h_{i,\\tau}\n",
    "\\end{align*}\n",
    "\n",
    "#### Purity Word Scores\n",
    "\n",
    "\\begin{align*}\n",
    "DWS &= \\sum_{i = 1}^{I/2} w_{\\nu,i}h_{i,\\tau}\\\\\n",
    "BWS &= \\sum_{i = I/2}^{T} w_{\\nu,i}h_{i,\\tau}\\\\\n",
    "Purity &= \\frac{DWS}{DWS+BWS} \\\\\n",
    "PWS &= DWS * Purity\n",
    "\\end{align*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scores are multiplied by 100\n",
    "def calculate_word_topic_scores(pre_nmf_list, W_test_list, n_topics=3):\n",
    "    H_list = []\n",
    "    for pnmf in pre_nmf_list:\n",
    "        H_list.append(pnmf.components_)\n",
    "    H_list = np.asarray(H_list)\n",
    "    \n",
    "    # [value, doc, word]\n",
    "    word_topic_scores = []\n",
    "    word_background_scores = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        word_topic_scores.append(np.dot(W_test_list[i][:,:n_topics], H_list[i][:n_topics,:]))\n",
    "        word_background_scores.append(np.dot(W_test_list[i][:,n_topics:], H_list[i][n_topics:,:]))\n",
    "        \n",
    "    word_topic_scores = np.asarray(word_topic_scores)\n",
    "    word_background_scores = np.asarray(word_background_scores)\n",
    "    \n",
    "    word_topic_purity = np.nan_to_num(np.divide(word_topic_scores,word_topic_scores+word_background_scores))\n",
    "    word_topic_sp = word_topic_scores*word_topic_purity\n",
    "    \n",
    "    word_topic_scores *= 100\n",
    "    word_topic_sp *= 100\n",
    "    \n",
    "    return word_topic_scores, word_topic_sp\n",
    "\n",
    "def find_top_word_scores(pre_tfidf_vectorizer, word_topic, word_count, test_corpusPP, only_doc_words):\n",
    "    word_list = []\n",
    "    feature_names = pre_tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    tcpp = test_corpusPP.split()\n",
    "    \n",
    "    for theme in range(10):\n",
    "        tmp_list = []\n",
    "        i = 0 \n",
    "        for idx in list(reversed(word_topic[theme].argsort())):\n",
    "            if i == word_count:\n",
    "                break\n",
    "            if not(only_doc_words and (feature_names[idx] not in tcpp)):\n",
    "                tmp_list.append((feature_names[idx], np.round(word_topic[theme][idx], 3)))\n",
    "            else:\n",
    "                i -= 1\n",
    "            i += 1\n",
    "        word_list.append(tmp_list)\n",
    "    return word_list\n",
    "\n",
    "def schwartz_word_scores(doc, W_test_norm, test_corpusPP, word_topic_scores, word_topic_sp, pre_tfidf_vectorizer, purity_score, word_count, only_doc_words):\n",
    "    if purity_score:\n",
    "        top_scores = find_top_word_scores(pre_tfidf_vectorizer, word_topic_sp[:,doc,:], word_count, test_corpusPP[doc], only_doc_words)\n",
    "    else:\n",
    "        top_scores = find_top_word_scores(pre_tfidf_vectorizer, word_topic_scores[:,doc,:], word_count, test_corpusPP[doc], only_doc_words)\n",
    "    \n",
    "    schwartz_word_score = []\n",
    "    schwartz_W_test = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_word_score.append(top_scores[categories.index(sch)])\n",
    "        schwartz_W_test.append((sch.upper(), np.round(W_test_norm[doc][categories.index(sch)], 3)))\n",
    "        \n",
    "    df_list = []\n",
    "    for i, a in enumerate(schwartz_word_score):\n",
    "        df_list.append(pd.DataFrame([schwartz_W_test[i]]+a, columns=[schwartz[i]+\" - word\", schwartz[i]+\" - score\"]))\n",
    "    score_df = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    return score_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print Pretrained Model's Topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**nmf2_pretrained.p** or **nmf2_pretrained_pruned.p** includes pretrained NMF model generated using **Semi-Supervised-NMF-train-v2.ipynb** notebook. It has the nmf model and tfidf_vectorizer.\n",
    "\n",
    "for the details of purned version see also **\"OMTermz HZ.ipynb\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_words(pre_trained_doc, word_count, anti=0):\n",
    "    pre_nmf_list, pre_tfidf_vectorizer = pickle.load( open( pre_trained_doc, \"rb\" ) )\n",
    "    \n",
    "    word_list = []\n",
    "    feature_names = pre_tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "    for theme in range(10):\n",
    "        word_topic = cumulate_W(pre_nmf_list[theme].components_.T,3).T[anti]\n",
    "        tmp_list = []\n",
    "        for i, idx in enumerate(list(reversed(word_topic.argsort()))):\n",
    "            if i == word_count:\n",
    "                break\n",
    "            tmp_list.append((feature_names[idx], np.round(word_topic[idx], 3)))\n",
    "        word_list.append(tmp_list)\n",
    "    \n",
    "    schwartz_word_score = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_word_score.append(word_list[categories.index(sch)])\n",
    "        \n",
    "    df_list = []\n",
    "    for i, a in enumerate(schwartz_word_score):\n",
    "        df_list.append(pd.DataFrame(a, columns=[schwartz[i]+\" - word\", schwartz[i]+\" - score\"]))\n",
    "    score_df = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    return score_df\n",
    "\n",
    "def export_pretrained_excel(pre_trained_doc, filepath, word_count=-1, anti=0):\n",
    "    df = get_pretrained_words(pre_trained_doc, word_count, anti)\n",
    "    df.to_excel(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mstate human analyt form topic\n",
      "\u001b[1mTopic #1: \u001b[0mintern peopl creation develop grow\n",
      "\u001b[1mTopic #2: \u001b[0mgroup first disarma help econom\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msee psycholog problem sometim western\n",
      "\u001b[1mTopic #1: \u001b[0mtime reaction repres research simpli\n",
      "\u001b[1mTopic #2: \u001b[0mshock studi import less schadenfreud\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msocial role motiv other tribe\n",
      "\u001b[1mTopic #1: \u001b[0mpeopl theori scale merchant support\n",
      "\u001b[1mTopic #2: \u001b[0mrelat primari owen top increasingli\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mmay lower compos specialti peopl\n",
      "\u001b[1mTopic #1: \u001b[0marticl toxic environ belong idea\n",
      "\u001b[1mTopic #2: \u001b[0mleadership tool partner bia law\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0muse gener resourc carrol interperson\n",
      "\u001b[1mTopic #1: \u001b[0mmade take known variou well\n",
      "\u001b[1mTopic #2: \u001b[0mbenedek romantic liberti domin olivero\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mtheori seem natur need thought\n",
      "\u001b[1mTopic #1: \u001b[0mone automat upon realiti uneduc\n",
      "\u001b[1mTopic #2: \u001b[0msometim turkey shuv school noisi\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mother great uncodifi problem still\n",
      "\u001b[1mTopic #1: \u001b[0mthu collectivist voltag rewritten undesir\n",
      "\u001b[1mTopic #2: \u001b[0mshow time note purpos sociolog\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mantiquitatem passion simon site three\n",
      "\u001b[1mTopic #1: \u001b[0msuccess law televis help mahavatar\n",
      "\u001b[1mTopic #2: \u001b[0mparticularli thing sinc preciou gener\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mrisk miyazaki declin feud previous\n",
      "\u001b[1mTopic #1: \u001b[0mreason land fun suffer psycholog\n",
      "\u001b[1mTopic #2: \u001b[0mindic exo mainli ongo part\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mland thousand pollut kid play\n",
      "\u001b[1mTopic #1: \u001b[0mpesticid signific paint meyer qualiti\n",
      "\u001b[1mTopic #2: \u001b[0mthu evid fals spread someth\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pre_trained_doc = \"nmf2_pretrained_pruned.p\"\n",
    "print_training_topics(pre_trained_doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scores are cumulated word-topic values directly obtained from the pretrained word-topic matrix (H). The reason of higher values in universalism or hedonism is probably unbalanced distribution of training documents. (Universalism and Hedonism have much more training documents than others)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>universalism - word</th>\n",
       "      <th>universalism - score</th>\n",
       "      <th>benevolence - word</th>\n",
       "      <th>benevolence - score</th>\n",
       "      <th>conformity - word</th>\n",
       "      <th>conformity - score</th>\n",
       "      <th>tradition - word</th>\n",
       "      <th>tradition - score</th>\n",
       "      <th>security - word</th>\n",
       "      <th>security - score</th>\n",
       "      <th>power - word</th>\n",
       "      <th>power - score</th>\n",
       "      <th>achievement - word</th>\n",
       "      <th>achievement - score</th>\n",
       "      <th>hedonism - word</th>\n",
       "      <th>hedonism - score</th>\n",
       "      <th>stimulation - word</th>\n",
       "      <th>stimulation - score</th>\n",
       "      <th>self-direction - word</th>\n",
       "      <th>self-direction - score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>group</td>\n",
       "      <td>1.968</td>\n",
       "      <td>one</td>\n",
       "      <td>0.693</td>\n",
       "      <td>show</td>\n",
       "      <td>0.859</td>\n",
       "      <td>particularli</td>\n",
       "      <td>0.567</td>\n",
       "      <td>signific</td>\n",
       "      <td>0.617</td>\n",
       "      <td>leadership</td>\n",
       "      <td>0.647</td>\n",
       "      <td>social</td>\n",
       "      <td>0.882</td>\n",
       "      <td>see</td>\n",
       "      <td>1.544</td>\n",
       "      <td>reason</td>\n",
       "      <td>0.576</td>\n",
       "      <td>use</td>\n",
       "      <td>0.899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>state</td>\n",
       "      <td>1.458</td>\n",
       "      <td>sometim</td>\n",
       "      <td>0.683</td>\n",
       "      <td>use</td>\n",
       "      <td>0.846</td>\n",
       "      <td>passion</td>\n",
       "      <td>0.522</td>\n",
       "      <td>thu</td>\n",
       "      <td>0.561</td>\n",
       "      <td>articl</td>\n",
       "      <td>0.636</td>\n",
       "      <td>use</td>\n",
       "      <td>0.830</td>\n",
       "      <td>studi</td>\n",
       "      <td>1.253</td>\n",
       "      <td>risk</td>\n",
       "      <td>0.563</td>\n",
       "      <td>well</td>\n",
       "      <td>0.726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peopl</td>\n",
       "      <td>1.421</td>\n",
       "      <td>natur</td>\n",
       "      <td>0.668</td>\n",
       "      <td>thu</td>\n",
       "      <td>0.713</td>\n",
       "      <td>sinc</td>\n",
       "      <td>0.503</td>\n",
       "      <td>relat</td>\n",
       "      <td>0.536</td>\n",
       "      <td>use</td>\n",
       "      <td>0.628</td>\n",
       "      <td>peopl</td>\n",
       "      <td>0.799</td>\n",
       "      <td>time</td>\n",
       "      <td>1.246</td>\n",
       "      <td>indic</td>\n",
       "      <td>0.550</td>\n",
       "      <td>gener</td>\n",
       "      <td>0.674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>human</td>\n",
       "      <td>1.277</td>\n",
       "      <td>theori</td>\n",
       "      <td>0.663</td>\n",
       "      <td>other</td>\n",
       "      <td>0.702</td>\n",
       "      <td>thing</td>\n",
       "      <td>0.499</td>\n",
       "      <td>two</td>\n",
       "      <td>0.531</td>\n",
       "      <td>may</td>\n",
       "      <td>0.567</td>\n",
       "      <td>theori</td>\n",
       "      <td>0.772</td>\n",
       "      <td>psycholog</td>\n",
       "      <td>1.106</td>\n",
       "      <td>land</td>\n",
       "      <td>0.507</td>\n",
       "      <td>take</td>\n",
       "      <td>0.662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>term</td>\n",
       "      <td>1.235</td>\n",
       "      <td>view</td>\n",
       "      <td>0.654</td>\n",
       "      <td>time</td>\n",
       "      <td>0.658</td>\n",
       "      <td>antiquitatem</td>\n",
       "      <td>0.482</td>\n",
       "      <td>land</td>\n",
       "      <td>0.523</td>\n",
       "      <td>peopl</td>\n",
       "      <td>0.526</td>\n",
       "      <td>role</td>\n",
       "      <td>0.770</td>\n",
       "      <td>shock</td>\n",
       "      <td>1.100</td>\n",
       "      <td>fun</td>\n",
       "      <td>0.437</td>\n",
       "      <td>known</td>\n",
       "      <td>0.615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>intern</td>\n",
       "      <td>1.216</td>\n",
       "      <td>two</td>\n",
       "      <td>0.648</td>\n",
       "      <td>note</td>\n",
       "      <td>0.565</td>\n",
       "      <td>law</td>\n",
       "      <td>0.464</td>\n",
       "      <td>play</td>\n",
       "      <td>0.522</td>\n",
       "      <td>power</td>\n",
       "      <td>0.496</td>\n",
       "      <td>lower</td>\n",
       "      <td>0.751</td>\n",
       "      <td>use</td>\n",
       "      <td>1.028</td>\n",
       "      <td>exo</td>\n",
       "      <td>0.394</td>\n",
       "      <td>made</td>\n",
       "      <td>0.586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>unit</td>\n",
       "      <td>1.205</td>\n",
       "      <td>thought</td>\n",
       "      <td>0.635</td>\n",
       "      <td>sourc</td>\n",
       "      <td>0.560</td>\n",
       "      <td>preciou</td>\n",
       "      <td>0.452</td>\n",
       "      <td>evid</td>\n",
       "      <td>0.517</td>\n",
       "      <td>lower</td>\n",
       "      <td>0.467</td>\n",
       "      <td>term</td>\n",
       "      <td>0.686</td>\n",
       "      <td>self</td>\n",
       "      <td>0.996</td>\n",
       "      <td>miyazaki</td>\n",
       "      <td>0.275</td>\n",
       "      <td>resourc</td>\n",
       "      <td>0.542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>form</td>\n",
       "      <td>1.147</td>\n",
       "      <td>need</td>\n",
       "      <td>0.612</td>\n",
       "      <td>refer</td>\n",
       "      <td>0.517</td>\n",
       "      <td>success</td>\n",
       "      <td>0.438</td>\n",
       "      <td>pollut</td>\n",
       "      <td>0.508</td>\n",
       "      <td>idea</td>\n",
       "      <td>0.462</td>\n",
       "      <td>relat</td>\n",
       "      <td>0.674</td>\n",
       "      <td>repres</td>\n",
       "      <td>0.993</td>\n",
       "      <td>declin</td>\n",
       "      <td>0.251</td>\n",
       "      <td>benedek</td>\n",
       "      <td>0.487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>world</td>\n",
       "      <td>1.146</td>\n",
       "      <td>upon</td>\n",
       "      <td>0.605</td>\n",
       "      <td>great</td>\n",
       "      <td>0.515</td>\n",
       "      <td>help</td>\n",
       "      <td>0.393</td>\n",
       "      <td>someth</td>\n",
       "      <td>0.502</td>\n",
       "      <td>least</td>\n",
       "      <td>0.450</td>\n",
       "      <td>work</td>\n",
       "      <td>0.661</td>\n",
       "      <td>social</td>\n",
       "      <td>0.968</td>\n",
       "      <td>suffer</td>\n",
       "      <td>0.167</td>\n",
       "      <td>romantic</td>\n",
       "      <td>0.468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>refer</td>\n",
       "      <td>1.101</td>\n",
       "      <td>seem</td>\n",
       "      <td>0.605</td>\n",
       "      <td>word</td>\n",
       "      <td>0.463</td>\n",
       "      <td>three</td>\n",
       "      <td>0.392</td>\n",
       "      <td>fals</td>\n",
       "      <td>0.485</td>\n",
       "      <td>compos</td>\n",
       "      <td>0.430</td>\n",
       "      <td>loan</td>\n",
       "      <td>0.644</td>\n",
       "      <td>seri</td>\n",
       "      <td>0.961</td>\n",
       "      <td>psycholog</td>\n",
       "      <td>0.164</td>\n",
       "      <td>liberti</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  universalism - word  universalism - score benevolence - word  \\\n",
       "0               group                 1.968                one   \n",
       "1               state                 1.458            sometim   \n",
       "2               peopl                 1.421              natur   \n",
       "3               human                 1.277             theori   \n",
       "4                term                 1.235               view   \n",
       "5              intern                 1.216                two   \n",
       "6                unit                 1.205            thought   \n",
       "7                form                 1.147               need   \n",
       "8               world                 1.146               upon   \n",
       "9               refer                 1.101               seem   \n",
       "\n",
       "   benevolence - score conformity - word  conformity - score tradition - word  \\\n",
       "0                0.693              show               0.859     particularli   \n",
       "1                0.683               use               0.846          passion   \n",
       "2                0.668               thu               0.713             sinc   \n",
       "3                0.663             other               0.702            thing   \n",
       "4                0.654              time               0.658     antiquitatem   \n",
       "5                0.648              note               0.565              law   \n",
       "6                0.635             sourc               0.560          preciou   \n",
       "7                0.612             refer               0.517          success   \n",
       "8                0.605             great               0.515             help   \n",
       "9                0.605              word               0.463            three   \n",
       "\n",
       "   tradition - score security - word  security - score power - word  \\\n",
       "0              0.567        signific             0.617   leadership   \n",
       "1              0.522             thu             0.561       articl   \n",
       "2              0.503           relat             0.536          use   \n",
       "3              0.499             two             0.531          may   \n",
       "4              0.482            land             0.523        peopl   \n",
       "5              0.464            play             0.522        power   \n",
       "6              0.452            evid             0.517        lower   \n",
       "7              0.438          pollut             0.508         idea   \n",
       "8              0.393          someth             0.502        least   \n",
       "9              0.392            fals             0.485       compos   \n",
       "\n",
       "   power - score achievement - word  achievement - score hedonism - word  \\\n",
       "0          0.647             social                0.882             see   \n",
       "1          0.636                use                0.830           studi   \n",
       "2          0.628              peopl                0.799            time   \n",
       "3          0.567             theori                0.772       psycholog   \n",
       "4          0.526               role                0.770           shock   \n",
       "5          0.496              lower                0.751             use   \n",
       "6          0.467               term                0.686            self   \n",
       "7          0.462              relat                0.674          repres   \n",
       "8          0.450               work                0.661          social   \n",
       "9          0.430               loan                0.644            seri   \n",
       "\n",
       "   hedonism - score stimulation - word  stimulation - score  \\\n",
       "0             1.544             reason                0.576   \n",
       "1             1.253               risk                0.563   \n",
       "2             1.246              indic                0.550   \n",
       "3             1.106               land                0.507   \n",
       "4             1.100                fun                0.437   \n",
       "5             1.028                exo                0.394   \n",
       "6             0.996           miyazaki                0.275   \n",
       "7             0.993             declin                0.251   \n",
       "8             0.968             suffer                0.167   \n",
       "9             0.961          psycholog                0.164   \n",
       "\n",
       "  self-direction - word  self-direction - score  \n",
       "0                   use                   0.899  \n",
       "1                  well                   0.726  \n",
       "2                 gener                   0.674  \n",
       "3                  take                   0.662  \n",
       "4                 known                   0.615  \n",
       "5                  made                   0.586  \n",
       "6               resourc                   0.542  \n",
       "7               benedek                   0.487  \n",
       "8              romantic                   0.468  \n",
       "9               liberti                   0.465  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pretrained_words(pre_trained_doc, word_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exports all word-score pairs in vocabulary (~33000 words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-a899ed327581>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mexport_pretrained_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_trained_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pretrained_words.xlsx'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-8-4225b3bcd069>\u001b[0m in \u001b[0;36mexport_pretrained_excel\u001b[1;34m(pre_trained_doc, filepath, word_count, anti)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mexport_pretrained_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_trained_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manti\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_pretrained_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpre_trained_doc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0manti\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 29\u001b[1;33m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_excel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36mto_excel\u001b[1;34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, encoding, inf_rep, verbose, freeze_panes)\u001b[0m\n\u001b[0;32m   1543\u001b[0m         formatter.write(excel_writer, sheet_name=sheet_name, startrow=startrow,\n\u001b[0;32m   1544\u001b[0m                         \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreeze_panes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreeze_panes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1545\u001b[1;33m                         engine=engine)\n\u001b[0m\u001b[0;32m   1546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1547\u001b[0m     def to_stata(self, fname, convert_dates=None, write_index=True,\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\excel.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine)\u001b[0m\n\u001b[0;32m    647\u001b[0m         writer.write_cells(formatted_cells, sheet_name,\n\u001b[0;32m    648\u001b[0m                            \u001b[0mstartrow\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartrow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstartcol\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstartcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 649\u001b[1;33m                            freeze_panes=freeze_panes)\n\u001b[0m\u001b[0;32m    650\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mneed_save\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    651\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\excel.py\u001b[0m in \u001b[0;36mwrite_cells\u001b[1;34m(self, cells, sheet_name, startrow, startcol, freeze_panes)\u001b[0m\n\u001b[0;32m   1646\u001b[0m                 wks.write(startrow + cell.row,\n\u001b[0;32m   1647\u001b[0m                           \u001b[0mstartcol\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1648\u001b[1;33m                           val, style)\n\u001b[0m\u001b[0;32m   1649\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1650\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_convert_to_style\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstyle_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_format_str\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py\u001b[0m in \u001b[0;36mcell_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m             \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnew_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcell_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\xlsxwriter\\worksheet.py\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, row, col, *args)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[1;31m# Write string types.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 392\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    393\u001b[0m             \u001b[1;31m# Map the data to the appropriate write_*() method.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    394\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "export_pretrained_excel(pre_trained_doc, filepath='pretrained_words.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Different Documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding two example documents to the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pope ted talk, https://www.ted.com/speakers/pope_francis\n",
    "# US Department of Defense, https://www.defense.gov/About/\n",
    "doc_names = [\"pope.txt\", \"dod.txt\", \"https://www.nationalgeographic.com/science/space/solar-system/earth/\", \"https://sadasd\", \"asdasd\"]\n",
    "#doc_names = [\"pope.txt\", \"dod.txt\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_corpus_docs(doc_names, test_corpus, insigth_api_key):\n",
    "    for doc in doc_names:\n",
    "        if re.match(\"^(http|https)://\", doc) is None:\n",
    "            add_corpus_txt(doc, test_corpus)\n",
    "        else:\n",
    "            add_corpus_url(doc, insigth_api_key, test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crawling a website using InSight API and adding its text to test_corpus.\n",
    "\n",
    "Always check the text, added to the corpus via add_corpus_url. Because websites can have unexpected embedded texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content Not Found - https://sadasd\n",
      "File not found - asdasd\n"
     ]
    }
   ],
   "source": [
    "test_corpus = []\n",
    "insigth_api_key = \"\" #needs to be filled\n",
    "add_corpus_docs(doc_names, test_corpus, insigth_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate model for the test_corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 1857\n",
      "Cleaning process: Initial size of tokens = 1857\n",
      "Reduction due to punctuations and stopwords = 1332.\n",
      "Reduction due to all numeral terms = 0\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 1332\n",
      "Percentage = 72%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 835\n",
      "Cleaning process: Initial size of tokens = 835\n",
      "Reduction due to punctuations and stopwords = 551.\n",
      "Reduction due to all numeral terms = 18\n",
      "Reduction due to short terms = 2\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 571\n",
      "Percentage = 68%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 554\n",
      "Cleaning process: Initial size of tokens = 554\n",
      "Reduction due to punctuations and stopwords = 316.\n",
      "Reduction due to all numeral terms = 36\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 352\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "Outputs will be written under D:\\Boun\\OpenMaker\\Insight\\semi-supervised-nmf/\n",
      "Configuring the text cleaner ...\n",
      "No custom stopword list is given, nltk.corpus.stopwords will be used.\n",
      "File access error at ./data/stopwords_openmaker.txt loading is skipped.\n",
      "File access error at ./data/specifics_openmaker.txt, data loading is skipped.\n",
      "A single text is provided.\n",
      "Extracting the terms ...\n",
      "Tokenizing the input text ..\n",
      "Done. Number of terms: 554\n",
      "Cleaning process: Initial size of tokens = 554\n",
      "Reduction due to punctuations and stopwords = 316.\n",
      "Reduction due to all numeral terms = 36\n",
      "Reduction due to short terms = 0\n",
      "Reduction due to rare terms = 0\n",
      "Reduction due to partially numeral terms = 0\n",
      "Reduction due to terms with not allowed symbols = 0\n",
      "The total term count reduction during this cleaning process = 352\n",
      "Percentage = 64%\n",
      "Stemming the terms in the corpus ..\n",
      "Done.\n",
      "COMPLETED.\n",
      "\n",
      "------------------------------\n",
      "\n",
      "Extracting tf-idf features for NMF...\n",
      "33139\n",
      "done in 0.01s.\n",
      "Fitting NMF for universalism\n",
      "Fitting NMF for hedonism\n",
      "Fitting NMF for achievement\n",
      "Fitting NMF for power\n",
      "Fitting NMF for self-direction\n",
      "Fitting NMF for benevolence\n",
      "Fitting NMF for conformity\n",
      "Fitting NMF for tradition\n",
      "Fitting NMF for stimulation\n",
      "Fitting NMF for security\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "W_test_norm, W_test_list, test_corpusPP, tfidf_test = evaluate_test_corpus(pre_trained_doc, test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results for test_corpus\n",
    "\n",
    "(All values are multiplied by 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When \"only_doc_words\" parameter set to True, the table will only show words from the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfc3bcbc556d4a27a9cecc723e077f57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>interactive</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "interactive(children=(IntSlider(value=2, description='doc', max=4), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_interactive_test_results(W_test_norm, W_test_list, test_corpus, test_corpusPP, doc_names, pre_trained_doc, purity_score = False, word_count = 10, only_doc_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>universalism</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>security</th>\n",
       "      <th>power</th>\n",
       "      <th>achievement</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope.txt</td>\n",
       "      <td>19.643708</td>\n",
       "      <td>62.965842</td>\n",
       "      <td>77.557248</td>\n",
       "      <td>38.481883</td>\n",
       "      <td>17.296200</td>\n",
       "      <td>43.970087</td>\n",
       "      <td>44.737977</td>\n",
       "      <td>51.584388</td>\n",
       "      <td>41.782835</td>\n",
       "      <td>49.243369</td>\n",
       "      <td>Good evening  or, good morning, I am not su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dod.txt</td>\n",
       "      <td>88.126194</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>10.455854</td>\n",
       "      <td>28.390529</td>\n",
       "      <td>78.130468</td>\n",
       "      <td>60.376097</td>\n",
       "      <td>32.600423</td>\n",
       "      <td>4.951889</td>\n",
       "      <td>32.506595</td>\n",
       "      <td>32.863906</td>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nationalgeographic.com/science/spa...</td>\n",
       "      <td>85.302025</td>\n",
       "      <td>11.082767</td>\n",
       "      <td>24.768660</td>\n",
       "      <td>5.908274</td>\n",
       "      <td>60.309391</td>\n",
       "      <td>64.362547</td>\n",
       "      <td>4.187538</td>\n",
       "      <td>31.662962</td>\n",
       "      <td>91.641003</td>\n",
       "      <td>52.770443</td>\n",
       "      <td>Earth, our home planet, is the only planet in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://sadasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asdasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  universalism  \\\n",
       "0                                           pope.txt     19.643708   \n",
       "1                                            dod.txt     88.126194   \n",
       "2  https://www.nationalgeographic.com/science/spa...     85.302025   \n",
       "3                                     https://sadasd      0.000000   \n",
       "4                                             asdasd      0.000000   \n",
       "\n",
       "   benevolence  conformity  tradition   security      power  achievement  \\\n",
       "0    62.965842   77.557248  38.481883  17.296200  43.970087    44.737977   \n",
       "1     0.003423   10.455854  28.390529  78.130468  60.376097    32.600423   \n",
       "2    11.082767   24.768660   5.908274  60.309391  64.362547     4.187538   \n",
       "3     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "4     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "\n",
       "    hedonism  stimulation  self-direction  \\\n",
       "0  51.584388    41.782835       49.243369   \n",
       "1   4.951889    32.506595       32.863906   \n",
       "2  31.662962    91.641003       52.770443   \n",
       "3   0.000000     0.000000        0.000000   \n",
       "4   0.000000     0.000000        0.000000   \n",
       "\n",
       "                                                Text  \n",
       "0  Good evening  or, good morning, I am not su...  \n",
       "1  \\nOn behalf of the Secretary of Defense and De...  \n",
       "2  Earth, our home planet, is the only planet in ...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_excel(W_test_norm, test_corpus, doc_names, filepath = 'output.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>universalism</th>\n",
       "      <th>benevolence</th>\n",
       "      <th>conformity</th>\n",
       "      <th>tradition</th>\n",
       "      <th>security</th>\n",
       "      <th>power</th>\n",
       "      <th>achievement</th>\n",
       "      <th>hedonism</th>\n",
       "      <th>stimulation</th>\n",
       "      <th>self-direction</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pope.txt</td>\n",
       "      <td>19.643708</td>\n",
       "      <td>62.965842</td>\n",
       "      <td>77.557248</td>\n",
       "      <td>38.481883</td>\n",
       "      <td>17.296200</td>\n",
       "      <td>43.970087</td>\n",
       "      <td>44.737977</td>\n",
       "      <td>51.584388</td>\n",
       "      <td>41.782835</td>\n",
       "      <td>49.243369</td>\n",
       "      <td>Good evening  or, good morning, I am not su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dod.txt</td>\n",
       "      <td>88.126194</td>\n",
       "      <td>0.003423</td>\n",
       "      <td>10.455854</td>\n",
       "      <td>28.390529</td>\n",
       "      <td>78.130468</td>\n",
       "      <td>60.376097</td>\n",
       "      <td>32.600423</td>\n",
       "      <td>4.951889</td>\n",
       "      <td>32.506595</td>\n",
       "      <td>32.863906</td>\n",
       "      <td>\\nOn behalf of the Secretary of Defense and De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.nationalgeographic.com/science/spa...</td>\n",
       "      <td>85.302025</td>\n",
       "      <td>11.082767</td>\n",
       "      <td>24.768660</td>\n",
       "      <td>5.908274</td>\n",
       "      <td>60.309391</td>\n",
       "      <td>64.362547</td>\n",
       "      <td>4.187538</td>\n",
       "      <td>31.662962</td>\n",
       "      <td>91.641003</td>\n",
       "      <td>52.770443</td>\n",
       "      <td>Earth, our home planet, is the only planet in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://sadasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>asdasd</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  universalism  \\\n",
       "0                                           pope.txt     19.643708   \n",
       "1                                            dod.txt     88.126194   \n",
       "2  https://www.nationalgeographic.com/science/spa...     85.302025   \n",
       "3                                     https://sadasd      0.000000   \n",
       "4                                             asdasd      0.000000   \n",
       "\n",
       "   benevolence  conformity  tradition   security      power  achievement  \\\n",
       "0    62.965842   77.557248  38.481883  17.296200  43.970087    44.737977   \n",
       "1     0.003423   10.455854  28.390529  78.130468  60.376097    32.600423   \n",
       "2    11.082767   24.768660   5.908274  60.309391  64.362547     4.187538   \n",
       "3     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "4     0.000000    0.000000   0.000000   0.000000   0.000000     0.000000   \n",
       "\n",
       "    hedonism  stimulation  self-direction  \\\n",
       "0  51.584388    41.782835       49.243369   \n",
       "1   4.951889    32.506595       32.863906   \n",
       "2  31.662962    91.641003       52.770443   \n",
       "3   0.000000     0.000000        0.000000   \n",
       "4   0.000000     0.000000        0.000000   \n",
       "\n",
       "                                                Text  \n",
       "0  Good evening  or, good morning, I am not su...  \n",
       "1  \\nOn behalf of the Secretary of Defense and De...  \n",
       "2  Earth, our home planet, is the only planet in ...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = export_to_csv(W_test_norm, test_corpus, doc_names, filepath = 'output.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "# When word_count is -1, it exports all the words\n",
    "# When only_doc_words is set to True, it exports only the words used in the documents\n",
    "\n",
    "# if you want proper document names in the output file change 'doc_names' list.\n",
    "export_word_scores_excel(W_test_norm, W_test_list, doc_names, pre_trained_doc, filepath = 'ssnmf_words.xlsx', purity_score=False, word_count=-1, only_doc_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exports tf-idf scores of the words that are used in the documents as a single xlsx file\n",
    "export_doc_tfidf_scores(tfidf_test, doc_names, pre_trained_doc, filepath = 'tfidf_docs.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
