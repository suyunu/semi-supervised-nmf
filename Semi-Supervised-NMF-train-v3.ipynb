{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One vs All Method\n",
    "\n",
    "Train NMF for each topic separately.\n",
    "\n",
    "Use all Wiki articles as Background Corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Burki\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize.moses import MosesDetokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import pi\n",
    "\n",
    "from omterms.interface import *\n",
    "\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "import pickle\n",
    "\n",
    "import libs.text_preprocess as tp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots and Prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=['universalism', 'hedonism', 'achievement', 'power',\n",
    "       'self-direction', 'benevolence', 'conformity', 'tradition', 'stimulation',\n",
    "       'security']\n",
    "\n",
    "def plot_radar_chart(doc_topic_cumul, doc):\n",
    "    # ------- PART 1: Create background\n",
    " \n",
    "    # number of variablecategories\n",
    "    schwartz =['universalism', 'benevolence', 'conformity', 'tradition',\n",
    "       'security', 'power', 'achievement', 'hedonism', 'stimulation',\n",
    "       'self-direction']\n",
    "    \n",
    "    schwartz_dist = []\n",
    "    for sch in schwartz:\n",
    "        schwartz_dist.append(doc_topic_cumul[doc][categories.index(sch)])\n",
    "    \n",
    "    N = len(schwartz)\n",
    "    \n",
    "    # What will be the angle of each axis in the plot? (we divide the plot / number of variable)\n",
    "    angles = [n / float(N) * 2 * pi for n in range(N)]\n",
    "    angles += angles[:1]\n",
    "\n",
    "    plt.figure(figsize=(8,8))\n",
    "    # Initialise the spider plot\n",
    "    ax = plt.subplot(111, polar=True)\n",
    "\n",
    "    # If you want the first axis to be on top:\n",
    "    ax.set_theta_offset(pi / 2)\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # Draw one axe per variable + add labels labels yet\n",
    "    plt.xticks(angles[:-1], schwartz)\n",
    "\n",
    "    # Draw ylabels\n",
    "    ax.set_rlabel_position(0)\n",
    "    plt.yticks([25,50,75], [\"25\",\"50\",\"75\"], color=\"grey\", size=7)\n",
    "    plt.ylim(0,100)\n",
    "\n",
    "\n",
    "    # ------- PART 2: Add plots\n",
    "\n",
    "    # Plot each individual = each line of the data\n",
    "    # I don't do a loop, because plotting more than 3 groups makes the chart unreadable\n",
    "\n",
    "    # Ind1\n",
    "    values = list(schwartz_dist) + list(schwartz_dist[:1])\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid')\n",
    "    ax.fill(angles, values, 'b', alpha=0.1)\n",
    "\n",
    "    # Add legend\n",
    "    #plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "    plt.title(\"Schwartz Chart - Doc \" + str(doc))\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'\n",
    "    \n",
    "    \n",
    "def print_top_words(model, theme, tfidf_vectorizer, n_top_words, n_topics=3):\n",
    "    feature_names = tfidf_vectorizer.get_feature_names()\n",
    "    print(color.CYAN + color.BOLD + categories[theme] + color.END)\n",
    "    for topic_idx, topic in enumerate(model[theme].components_):\n",
    "        if topic_idx / n_topics == 1:\n",
    "            break\n",
    "        message = color.BOLD + \"Topic #%d: \" % topic_idx + color.END\n",
    "        message += \" - \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "    \n",
    "def print_cumulative_train_doc_topics(data, doc_topic, doc, n_best):\n",
    "    test_theme = data.iloc[doc]['theme']\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + color.RED +  \" (\" + test_theme + \")\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "    \n",
    "def print_cumulative_test_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    dt = doc_topic[doc]\n",
    "    for i in dt.argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, dt[i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_doc_topics(doc_topic, doc, n_best):\n",
    "    print(color.BOLD + \"Doc \" + str(doc) + \"\\t: \" + color.END, end='')\n",
    "    for i in doc_topic[doc].argsort()[:-n_best - 1:-1]:\n",
    "        print(\"(\", end='')\n",
    "        try:\n",
    "            print(color.CYAN + color.BOLD + categories[i//3] + color.END, end='')\n",
    "        except:\n",
    "            print(color.CYAN + color.BOLD + \"General\" + color.END, end='')\n",
    "        print(\", %d, %.2lf)  \" %(i, doc_topic[doc][i]), end='')    \n",
    "    print()\n",
    "\n",
    "def print_train_results(doc_topic, doc, corpus, data):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_train_doc_topics(data, doc_topic, doc, 11) \n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "def print_test_results(doc_topic, doc, corpus):\n",
    "    print(color.BOLD + \"Document \" + str(doc) + color.END)\n",
    "    print()\n",
    "    print(color.BOLD + \"Text: \" + color.END)\n",
    "    print(\"...\" + corpus[doc][len(corpus[doc])//3:len(corpus[doc])//3+500] + \"...\")\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    print(color.BOLD + \"Topic Distribution: \" + color.END)\n",
    "    \n",
    "    #print(pd.DataFrame(data=[W_test_norm[doc]], index = [doc], columns=categories+['general']))\n",
    "    print_cumulative_test_doc_topics(doc_topic, doc, 11)\n",
    "    print()\n",
    "    \n",
    "    plot_radar_chart(doc_topic, doc)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cumulate_W(W, n_topics):\n",
    "    W_cumul = []\n",
    "    for d in W:\n",
    "        temp = []\n",
    "        for i in range(W.shape[1]//n_topics):\n",
    "            temp.append(d[i*n_topics:(i+1)*n_topics].sum())\n",
    "        W_cumul.append(temp)\n",
    "\n",
    "    W_cumul = np.asarray(W_cumul)\n",
    "    \n",
    "    return W_cumul\n",
    "\n",
    "def normalize_W(W):\n",
    "    W_cumul_norm = W/(W.sum(axis=1).reshape(W.shape[0], 1))\n",
    "    W_cumul_norm *= 100\n",
    "    \n",
    "    return W_cumul_norm\n",
    "\n",
    "def export_to_excel(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put xlsx as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories)\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_excel(filepath)\n",
    "    return df\n",
    "\n",
    "def export_to_csv(W, docs, filepath):\n",
    "    '''\n",
    "    Take cumulated W as input.\n",
    "    Don't forget to put csv as file extension '''\n",
    "    \n",
    "    df = pd.DataFrame(data=W,index = range(len(W)), columns=categories)\n",
    "    df['Text'] = docs\n",
    "    cols = df.columns.tolist()\n",
    "    cols = cols[-1:] + cols[:-1]\n",
    "    df = df[cols]\n",
    "    df.to_csv(filepath)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just seperates list\n",
    "# https://stackoverflow.com/a/35518205\n",
    "def partition_list2(a, k):\n",
    "    if k <= 1: return [a]\n",
    "    if k >= len(a): return [[x] for x in a]\n",
    "    partition_between = [(i+1)*len(a) // k for i in range(k-1)]\n",
    "    average_height = float(sum(a[:,0]))/k\n",
    "    best_score = None\n",
    "    best_partitions = None\n",
    "    count = 0\n",
    "\n",
    "    while True:\n",
    "        starts = [0] + partition_between\n",
    "        ends = partition_between + [len(a)]\n",
    "        partitions = [a[starts[i]:ends[i]] for i in range(k)]\n",
    "        heights = [np.sum(p[:,0]) for p in partitions]\n",
    "        #heights = list(map(sum, np.array(partitions)[:,0]))\n",
    "\n",
    "        abs_height_diffs = list(map(lambda x: abs(average_height - x), heights))\n",
    "        worst_partition_index = abs_height_diffs.index(max(abs_height_diffs))\n",
    "        worst_height_diff = average_height - heights[worst_partition_index]\n",
    "\n",
    "        if best_score is None or abs(worst_height_diff) < best_score:\n",
    "            best_score = abs(worst_height_diff)\n",
    "            best_partitions = partitions\n",
    "            no_improvements_count = 0\n",
    "        else:\n",
    "            no_improvements_count += 1\n",
    "\n",
    "        if worst_height_diff == 0 or no_improvements_count > 5 or count > 100:\n",
    "            return best_partitions\n",
    "        count += 1\n",
    "\n",
    "        move = -1 if worst_height_diff < 0 else 1\n",
    "        bound_to_move = 0 if worst_partition_index == 0\\\n",
    "                        else k-2 if worst_partition_index == k-1\\\n",
    "                        else worst_partition_index-1 if (worst_height_diff < 0) ^ (heights[worst_partition_index-1] > heights[worst_partition_index+1])\\\n",
    "                        else worst_partition_index\n",
    "        direction = -1 if bound_to_move < worst_partition_index else 1\n",
    "        partition_between[bound_to_move] += move * direction\n",
    "\n",
    "def print_best_partition(a, k):\n",
    "    print('Partitioning {0} into {1} partitions'.format(a, k))\n",
    "    p = partition_list(a, k)\n",
    "    print('The best partitioning is {0}\\n    With heights {1}\\n'.format(p, list(map(sum, p))))\n",
    "    \n",
    "def initialize_H3(X, theme_counts, n_topics, p):\n",
    "    X_sum = list(np.sum(X>0, axis=1))\n",
    "    X_sum = [(x, i) for i, x in enumerate(X_sum)]\n",
    "    #X_sum.sort(reverse=True)\n",
    "    #X_sum = np.array(X_sum)\n",
    "\n",
    "    X_sum_list = []\n",
    "    X_parts_list = []\n",
    "\n",
    "    tc_sum = 0\n",
    "    for tc in theme_counts:\n",
    "        X_sum_list.append(X_sum[tc_sum:tc+tc_sum])\n",
    "        X_sum_list[-1].sort(reverse=True)\n",
    "        X_sum_list[-1] = np.array(X_sum_list[-1][:p*n_topics])\n",
    "        X_parts_list.append(partition_list2(X_sum_list[-1], n_topics))\n",
    "\n",
    "\n",
    "        tc_sum += tc\n",
    "\n",
    "    H_list = []\n",
    "    tc_sum = 0\n",
    "    for i, tc in enumerate(theme_counts):\n",
    "        H = np.zeros((n_topics+1, X.shape[1]))\n",
    "\n",
    "        for j in range(0, n_topics):\n",
    "            H[j] = np.average(X[X_parts_list[i][j%len(X_parts_list[i])][:, 1]], axis=0)\n",
    "            #H[j][np.where(H[j]==0)] += np.average(X[tc_sum:tc+tc_sum], axis=0)[np.where(H[j]==0)]\n",
    "\n",
    "        for j in range(n_topics, n_topics+1):\n",
    "            bckg_parts = []\n",
    "            for k in range(len(theme_counts)):\n",
    "                bckg_parts.extend(X_parts_list[k][(j-n_topics)%len(X_parts_list[k])][:int(p//2), 1])\n",
    "            H[j] = np.average(X[bckg_parts], axis=0)\n",
    "            #H[j][np.where(H[j]==0)] += np.average(X, axis=0)[np.where(H[j]==0)]\n",
    "        tc_sum += tc\n",
    "            \n",
    "\n",
    "        H_list.append(H)\n",
    "    return H_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    data = pd.read_json(filepath)\n",
    "    data = data[data['text']!=\"\"]\n",
    "    data = data.sort_values('theme.id')\n",
    "    \n",
    "    return data\n",
    "    \n",
    "def extract_corpus(data):    \n",
    "    corpus = list(data['text'])\n",
    "    return corpus\n",
    "\n",
    "def preprocess_corpus(corpus):\n",
    "    PPcorpus = [' '.join(list((extract_terms(doc, extra_process = ['stem'])['Stem']+' ')*extract_terms(doc, extra_process = ['stem'])['TF'])) for doc in corpus]\n",
    "    return PPcorpus\n",
    "\n",
    "def train_corpus(corpus, data, brown_corpus, n_topics=3, betaloss = 'kullback-leibler', bckg_brown = False):\n",
    "    N = len(data)\n",
    "    \n",
    "    theme_counts = data.groupby(['theme.id','theme']).count().iloc[:,1]\n",
    "    pd_theme_counts = pd.DataFrame(theme_counts)\n",
    "    n_themes = len(theme_counts)\n",
    "    \n",
    "    n_top_words = 5\n",
    "    n_components = n_topics*(n_themes)\n",
    "    \n",
    "    \n",
    "    print(\"Extracting tf-idf features for NMF...\")\n",
    "    #tfidf_vectorizer= TfidfVectorizer(min_df=1, ngram_range=(1,3), max_features=50000)\n",
    "    tfidf_vectorizer= CountVectorizer(min_df=1, ngram_range=(1,3), max_features=50000)\n",
    "    t0 = time()\n",
    "    \n",
    "    W_list = []\n",
    "    \n",
    "    if bckg_brown:\n",
    "        tfidf = tfidf_vectorizer.fit_transform(corpus+brown_corpus)\n",
    "        tc_sum = 0\n",
    "        for tc in theme_counts:\n",
    "            W = np.zeros((N+len(brown_corpus),n_topics+1))\n",
    "            W[N:, n_topics:] = np.random.random((len(brown_corpus),1))\n",
    "            W[tc_sum:tc_sum+tc, :] = np.random.random((tc,n_topics+1))\n",
    "\n",
    "            tc_sum += tc\n",
    "            W_list.append(W)\n",
    "    else:\n",
    "        tfidf = tfidf_vectorizer.fit_transform(corpus)\n",
    "        tc_sum = 0\n",
    "        for tc in theme_counts:\n",
    "            W = np.zeros((N,n_topics+1))\n",
    "            W[:, n_topics:] = np.random.random((N,1))\n",
    "            W[tc_sum:tc_sum+tc, :n_topics] = np.random.random((tc,n_topics))\n",
    "\n",
    "            tc_sum += tc\n",
    "            W_list.append(W)\n",
    "        \n",
    "    n_features = tfidf.shape[1]\n",
    "    print(n_features)\n",
    "    print(\"done in %0.2fs.\" % (time() - t0))\n",
    "    \n",
    "    X = tfidf \n",
    "    nmf_list = []\n",
    "    H_list = initialize_H3(X.toarray(), theme_counts, n_topics, p=20)\n",
    "\n",
    "    for i, W in enumerate(W_list):\n",
    "        print(\"Fitting NMF for \" + str(theme_counts.index[i][1]))\n",
    "        t0 = time()\n",
    "        H = H_list[i]\n",
    "        #H = np.random.rand(n_topics+1, n_features)\n",
    "\n",
    "        nmf = NMF(n_components= n_topics+1, solver='mu', beta_loss=betaloss,\n",
    "                  alpha=.1, l1_ratio=.5, init = 'custom')\n",
    "\n",
    "        nmf.fit_transform(X=X,W=W,H=H)\n",
    "        print(\"done in %0.2fs.\" % (time() - t0))\n",
    "\n",
    "        nmf_list.append(nmf)\n",
    "    \n",
    "    \n",
    "    return nmf_list, W_list, tfidf, tfidf_vectorizer\n",
    "    \n",
    "def get_pretrained_words(pre_nmf_list, pre_tfidf_vectorizer, word_count, normalized=False, anti=0):\n",
    "    n_topics = pre_nmf_list[0].components_.shape[0]-1\n",
    "    word_list = []\n",
    "    feature_names = pre_tfidf_vectorizer.get_feature_names()\n",
    "    \n",
    "    nmf_comps = []\n",
    "    for pnmf in pre_nmf_list:\n",
    "        aa = pnmf.components_\n",
    "        nmf_comps.append(aa/np.sum(aa,axis=1)[:, np.newaxis])\n",
    "    \n",
    "    for theme in range(10):\n",
    "        #word_topic = cumulate_W(pre_nmf_list[theme].components_.T,n_topics).T[anti]\n",
    "        for nt in range(n_topics):\n",
    "            if normalized:\n",
    "                word_topic = nmf_comps[theme][nt]\n",
    "            else:\n",
    "                word_topic = pre_nmf_list[theme].components_[nt]\n",
    "            tmp_list = []\n",
    "            for i, idx in enumerate(list(reversed(word_topic.argsort()))):\n",
    "                if i == word_count:\n",
    "                    break\n",
    "                tmp_list.append((feature_names[idx], np.round(word_topic[idx], 3)))\n",
    "            word_list.append(tmp_list)\n",
    "    \n",
    "    schwartz_word_score = []\n",
    "    for sch in schwartz:\n",
    "        for nt in range(n_topics):\n",
    "            schwartz_word_score.append(word_list[n_topics*categories.index(sch)+nt])\n",
    "        \n",
    "    df_list = []\n",
    "    for i, a in enumerate(schwartz_word_score):\n",
    "        df_list.append(pd.DataFrame(a, columns=[schwartz[i//n_topics]+ \" (\" + str(i%n_topics)+ \") - word\",\n",
    "                                                schwartz[i//n_topics]+ \" (\" + str(i%n_topics)+ \") - score\"]))\n",
    "    score_df = pd.concat(df_list, axis=1)\n",
    "    \n",
    "    return score_df\n",
    "\n",
    "def export_pretrained_excel(pre_nmf_list, pre_tfidf_vectorizer, filepath, word_count=-1, anti=0):\n",
    "    df = get_pretrained_words(pre_nmf_list, pre_tfidf_vectorizer, word_count, anti)\n",
    "    df.to_excel(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix bad wording:  0.009974241256713867 s\n",
      "Tokenize:  0.009972333908081055 s\n",
      "Remove stopwords and Lemmatize:  2.500488042831421 s\n",
      "\n",
      "Fix bad wording:  0.004986763000488281 s\n",
      "Tokenize:  0.008976459503173828 s\n",
      "Remove stopwords and Lemmatize:  0.029918909072875977 s\n",
      "\n",
      "Fix bad wording:  0.004986763000488281 s\n",
      "Tokenize:  0.006981611251831055 s\n",
      "Remove stopwords and Lemmatize:  0.013963460922241211 s\n",
      "\n",
      "Fix bad wording:  0.005983114242553711 s\n",
      "Tokenize:  0.021940946578979492 s\n",
      "Remove stopwords and Lemmatize:  0.05884361267089844 s\n",
      "\n",
      "Fix bad wording:  0.003989458084106445 s\n",
      "Tokenize:  0.0059850215911865234 s\n",
      "Remove stopwords and Lemmatize:  0.011965751647949219 s\n",
      "\n",
      "Fix bad wording:  0.00997304916381836 s\n",
      "Tokenize:  0.02693009376525879 s\n",
      "Remove stopwords and Lemmatize:  0.07181620597839355 s\n",
      "\n",
      "Fix bad wording:  0.001993417739868164 s\n",
      "Tokenize:  0.003989219665527344 s\n",
      "Remove stopwords and Lemmatize:  0.00997304916381836 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.001993894577026367 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.0029921531677246094 s\n",
      "Remove stopwords and Lemmatize:  0.009973287582397461 s\n",
      "\n",
      "Fix bad wording:  0.0049860477447509766 s\n",
      "Tokenize:  0.011967897415161133 s\n",
      "Remove stopwords and Lemmatize:  0.025931596755981445 s\n",
      "\n",
      "Fix bad wording:  0.0498654842376709 s\n",
      "Tokenize:  0.12267327308654785 s\n",
      "Remove stopwords and Lemmatize:  0.3809821605682373 s\n",
      "\n",
      "Fix bad wording:  0.020943403244018555 s\n",
      "Tokenize:  0.05285763740539551 s\n",
      "Remove stopwords and Lemmatize:  0.18152093887329102 s\n",
      "\n",
      "Fix bad wording:  0.004981517791748047 s\n",
      "Tokenize:  0.007979393005371094 s\n",
      "Remove stopwords and Lemmatize:  0.015971660614013672 s\n",
      "\n",
      "Fix bad wording:  0.009972333908081055 s\n",
      "Tokenize:  0.013961553573608398 s\n",
      "Remove stopwords and Lemmatize:  0.037905216217041016 s\n",
      "\n",
      "Fix bad wording:  0.0029883384704589844 s\n",
      "Tokenize:  0.0029897689819335938 s\n",
      "Remove stopwords and Lemmatize:  0.014961481094360352 s\n",
      "\n",
      "Fix bad wording:  0.0019936561584472656 s\n",
      "Tokenize:  0.0009982585906982422 s\n",
      "Remove stopwords and Lemmatize:  0.004985332489013672 s\n",
      "\n",
      "Fix bad wording:  0.01795220375061035 s\n",
      "Tokenize:  0.04886960983276367 s\n",
      "Remove stopwords and Lemmatize:  0.14561033248901367 s\n",
      "\n",
      "Fix bad wording:  0.003989696502685547 s\n",
      "Tokenize:  0.006985902786254883 s\n",
      "Remove stopwords and Lemmatize:  0.021936893463134766 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0009970664978027344 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.0049893856048583984 s\n",
      "\n",
      "Fix bad wording:  0.018949270248413086 s\n",
      "Tokenize:  0.026935100555419922 s\n",
      "Remove stopwords and Lemmatize:  0.09274411201477051 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.006981849670410156 s\n",
      "Remove stopwords and Lemmatize:  0.010970830917358398 s\n",
      "\n",
      "Fix bad wording:  0.004986763000488281 s\n",
      "Tokenize:  0.0069811344146728516 s\n",
      "Remove stopwords and Lemmatize:  0.01994609832763672 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.007978439331054688 s\n",
      "Remove stopwords and Lemmatize:  0.016956567764282227 s\n",
      "\n",
      "Fix bad wording:  0.001996278762817383 s\n",
      "Tokenize:  0.0029838085174560547 s\n",
      "Remove stopwords and Lemmatize:  0.006979465484619141 s\n",
      "\n",
      "Fix bad wording:  0.02094411849975586 s\n",
      "Tokenize:  0.02892279624938965 s\n",
      "Remove stopwords and Lemmatize:  0.0718071460723877 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.007978677749633789 s\n",
      "\n",
      "Fix bad wording:  0.00797891616821289 s\n",
      "Tokenize:  0.013962745666503906 s\n",
      "Remove stopwords and Lemmatize:  0.031914472579956055 s\n",
      "\n",
      "Fix bad wording:  0.023935794830322266 s\n",
      "Tokenize:  0.03191494941711426 s\n",
      "Remove stopwords and Lemmatize:  0.08178138732910156 s\n",
      "\n",
      "Fix bad wording:  0.004987001419067383 s\n",
      "Tokenize:  0.008974552154541016 s\n",
      "Remove stopwords and Lemmatize:  0.034905195236206055 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.006983280181884766 s\n",
      "Remove stopwords and Lemmatize:  0.011967897415161133 s\n",
      "\n",
      "Fix bad wording:  0.003990888595581055 s\n",
      "Tokenize:  0.00697779655456543 s\n",
      "Remove stopwords and Lemmatize:  0.019947052001953125 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009970664978027344 s\n",
      "Remove stopwords and Lemmatize:  0.0029914379119873047 s\n",
      "\n",
      "Fix bad wording:  0.002992391586303711 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.012965202331542969 s\n",
      "\n",
      "Fix bad wording:  0.002992391586303711 s\n",
      "Tokenize:  0.003987789154052734 s\n",
      "Remove stopwords and Lemmatize:  0.007979631423950195 s\n",
      "\n",
      "Fix bad wording:  0.004986763000488281 s\n",
      "Tokenize:  0.005984067916870117 s\n",
      "Remove stopwords and Lemmatize:  0.013962030410766602 s\n",
      "\n",
      "Fix bad wording:  0.006982564926147461 s\n",
      "Tokenize:  0.010970115661621094 s\n",
      "Remove stopwords and Lemmatize:  0.04188704490661621 s\n",
      "\n",
      "Fix bad wording:  0.008975505828857422 s\n",
      "Tokenize:  0.0109710693359375 s\n",
      "Remove stopwords and Lemmatize:  0.03291177749633789 s\n",
      "\n",
      "Fix bad wording:  0.012965679168701172 s\n",
      "Tokenize:  0.02892279624938965 s\n",
      "Remove stopwords and Lemmatize:  0.05385613441467285 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.019947052001953125 s\n",
      "\n",
      "Fix bad wording:  0.044879913330078125 s\n",
      "Tokenize:  0.05684804916381836 s\n",
      "Remove stopwords and Lemmatize:  0.18749737739562988 s\n",
      "\n",
      "Fix bad wording:  0.009973287582397461 s\n",
      "Tokenize:  0.017952442169189453 s\n",
      "Remove stopwords and Lemmatize:  0.09828710556030273 s\n",
      "\n",
      "Fix bad wording:  0.011968135833740234 s\n",
      "Tokenize:  0.024933815002441406 s\n",
      "Remove stopwords and Lemmatize:  0.0718080997467041 s\n",
      "\n",
      "Fix bad wording:  0.02393937110900879 s\n",
      "Tokenize:  0.05285453796386719 s\n",
      "Remove stopwords and Lemmatize:  0.1186835765838623 s\n",
      "\n",
      "Fix bad wording:  0.027926206588745117 s\n",
      "Tokenize:  0.03490471839904785 s\n",
      "Remove stopwords and Lemmatize:  0.1047205924987793 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.005984306335449219 s\n",
      "Remove stopwords and Lemmatize:  0.013961553573608398 s\n",
      "\n",
      "Fix bad wording:  0.005984067916870117 s\n",
      "Tokenize:  0.00997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.019947528839111328 s\n",
      "\n",
      "Fix bad wording:  0.02892136573791504 s\n",
      "Tokenize:  0.05585145950317383 s\n",
      "Remove stopwords and Lemmatize:  0.1904916763305664 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.004993915557861328 s\n",
      "Remove stopwords and Lemmatize:  0.0059773921966552734 s\n",
      "\n",
      "Fix bad wording:  0.011981964111328125 s\n",
      "Tokenize:  0.02093672752380371 s\n",
      "Remove stopwords and Lemmatize:  0.042885780334472656 s\n",
      "\n",
      "Fix bad wording:  0.0009965896606445312 s\n",
      "Tokenize:  0.002992868423461914 s\n",
      "Remove stopwords and Lemmatize:  0.006981849670410156 s\n",
      "\n",
      "Fix bad wording:  0.0019953250885009766 s\n",
      "Tokenize:  0.003988027572631836 s\n",
      "Remove stopwords and Lemmatize:  0.009972810745239258 s\n",
      "\n",
      "Fix bad wording:  0.005989551544189453 s\n",
      "Tokenize:  0.010965347290039062 s\n",
      "Remove stopwords and Lemmatize:  0.030904769897460938 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.002992391586303711 s\n",
      "Remove stopwords and Lemmatize:  0.012968778610229492 s\n",
      "\n",
      "Fix bad wording:  0.00697779655456543 s\n",
      "Tokenize:  0.011968374252319336 s\n",
      "Remove stopwords and Lemmatize:  0.030916929244995117 s\n",
      "\n",
      "Fix bad wording:  0.00697636604309082 s\n",
      "Tokenize:  0.012965917587280273 s\n",
      "Remove stopwords and Lemmatize:  0.038405656814575195 s\n",
      "\n",
      "Fix bad wording:  0.0019941329956054688 s\n",
      "Tokenize:  0.004593372344970703 s\n",
      "Remove stopwords and Lemmatize:  0.011364459991455078 s\n",
      "\n",
      "Fix bad wording:  0.010970830917358398 s\n",
      "Tokenize:  0.01994776725769043 s\n",
      "Remove stopwords and Lemmatize:  0.07080936431884766 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0019996166229248047 s\n",
      "Remove stopwords and Lemmatize:  0.0046234130859375 s\n",
      "\n",
      "Fix bad wording:  0.0203094482421875 s\n",
      "Tokenize:  0.03191423416137695 s\n",
      "Remove stopwords and Lemmatize:  0.06881594657897949 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.006981849670410156 s\n",
      "\n",
      "Fix bad wording:  0.006981611251831055 s\n",
      "Tokenize:  0.010970830917358398 s\n",
      "Remove stopwords and Lemmatize:  0.02992105484008789 s\n",
      "\n",
      "Fix bad wording:  0.009972333908081055 s\n",
      "Tokenize:  0.01995086669921875 s\n",
      "Remove stopwords and Lemmatize:  0.06280875205993652 s\n",
      "\n",
      "Fix bad wording:  0.0069942474365234375 s\n",
      "Tokenize:  0.010962247848510742 s\n",
      "Remove stopwords and Lemmatize:  0.027910947799682617 s\n",
      "\n",
      "Fix bad wording:  0.0049860477447509766 s\n",
      "Tokenize:  0.009973287582397461 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.027925968170166016 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.0019943714141845703 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.00997471809387207 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.005983829498291016 s\n",
      "Remove stopwords and Lemmatize:  0.01795220375061035 s\n",
      "\n",
      "Fix bad wording:  0.0139617919921875 s\n",
      "Tokenize:  0.023941516876220703 s\n",
      "Remove stopwords and Lemmatize:  0.07978677749633789 s\n",
      "\n",
      "Fix bad wording:  0.0069811344146728516 s\n",
      "Tokenize:  0.011968374252319336 s\n",
      "Remove stopwords and Lemmatize:  0.03789830207824707 s\n",
      "\n",
      "Fix bad wording:  0.008976936340332031 s\n",
      "Tokenize:  0.014958858489990234 s\n",
      "Remove stopwords and Lemmatize:  0.03789806365966797 s\n",
      "\n",
      "Fix bad wording:  0.010972261428833008 s\n",
      "Tokenize:  0.00997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.040891408920288086 s\n",
      "\n",
      "Fix bad wording:  0.03989291191101074 s\n",
      "Tokenize:  0.06482720375061035 s\n",
      "Remove stopwords and Lemmatize:  0.13962745666503906 s\n",
      "\n",
      "Fix bad wording:  0.0069811344146728516 s\n",
      "Tokenize:  0.011968851089477539 s\n",
      "Remove stopwords and Lemmatize:  0.038895368576049805 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.0049860477447509766 s\n",
      "Remove stopwords and Lemmatize:  0.009972572326660156 s\n",
      "\n",
      "Fix bad wording:  0.003988504409790039 s\n",
      "Tokenize:  0.012965679168701172 s\n",
      "Remove stopwords and Lemmatize:  0.04787182807922363 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.0019941329956054688 s\n",
      "Remove stopwords and Lemmatize:  0.005983829498291016 s\n",
      "\n",
      "Fix bad wording:  0.0029914379119873047 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.024933338165283203 s\n",
      "\n",
      "Fix bad wording:  0.005983829498291016 s\n",
      "Tokenize:  0.00897526741027832 s\n",
      "Remove stopwords and Lemmatize:  0.019948244094848633 s\n",
      "\n",
      "Fix bad wording:  0.012965202331542969 s\n",
      "Tokenize:  0.02094411849975586 s\n",
      "Remove stopwords and Lemmatize:  0.0359044075012207 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.005984067916870117 s\n",
      "Remove stopwords and Lemmatize:  0.011965274810791016 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009958744049072266 s\n",
      "Remove stopwords and Lemmatize:  0.0019936561584472656 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.0029900074005126953 s\n",
      "Remove stopwords and Lemmatize:  0.00897669792175293 s\n",
      "\n",
      "Fix bad wording:  0.007980108261108398 s\n",
      "Tokenize:  0.01096963882446289 s\n",
      "Remove stopwords and Lemmatize:  0.03889799118041992 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.002990245819091797 s\n",
      "\n",
      "Fix bad wording:  0.036904096603393555 s\n",
      "Tokenize:  0.03789663314819336 s\n",
      "Remove stopwords and Lemmatize:  0.12466692924499512 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.003989458084106445 s\n",
      "Remove stopwords and Lemmatize:  0.012965202331542969 s\n",
      "\n",
      "Fix bad wording:  0.010970830917358398 s\n",
      "Tokenize:  0.018949270248413086 s\n",
      "Remove stopwords and Lemmatize:  0.050864219665527344 s\n",
      "\n",
      "Fix bad wording:  0.010970592498779297 s\n",
      "Tokenize:  0.017952680587768555 s\n",
      "Remove stopwords and Lemmatize:  0.039893388748168945 s\n",
      "\n",
      "Fix bad wording:  0.00299072265625 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.016955137252807617 s\n",
      "\n",
      "Fix bad wording:  0.004985809326171875 s\n",
      "Tokenize:  0.009975194931030273 s\n",
      "Remove stopwords and Lemmatize:  0.03191423416137695 s\n",
      "\n",
      "Fix bad wording:  0.009972810745239258 s\n",
      "Tokenize:  0.00997471809387207 s\n",
      "Remove stopwords and Lemmatize:  0.02792525291442871 s\n",
      "\n",
      "Fix bad wording:  0.0029926300048828125 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.012966156005859375 s\n",
      "\n",
      "Fix bad wording:  0.0010001659393310547 s\n",
      "Tokenize:  0.0019936561584472656 s\n",
      "Remove stopwords and Lemmatize:  0.005983114242553711 s\n",
      "\n",
      "Fix bad wording:  0.009972810745239258 s\n",
      "Tokenize:  0.02792525291442871 s\n",
      "Remove stopwords and Lemmatize:  0.07280659675598145 s\n",
      "\n",
      "Fix bad wording:  0.006980419158935547 s\n",
      "Tokenize:  0.010970830917358398 s\n",
      "Remove stopwords and Lemmatize:  0.04388427734375 s\n",
      "\n",
      "Fix bad wording:  0.006980419158935547 s\n",
      "Tokenize:  0.014960765838623047 s\n",
      "Remove stopwords and Lemmatize:  0.05086231231689453 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.005983591079711914 s\n",
      "\n",
      "Fix bad wording:  0.023937702178955078 s\n",
      "Tokenize:  0.049865007400512695 s\n",
      "Remove stopwords and Lemmatize:  0.15559124946594238 s\n",
      "\n",
      "Fix bad wording:  0.006982088088989258 s\n",
      "Tokenize:  0.009978055953979492 s\n",
      "Remove stopwords and Lemmatize:  0.025925636291503906 s\n",
      "\n",
      "Fix bad wording:  0.004986763000488281 s\n",
      "Tokenize:  0.00797891616821289 s\n",
      "Remove stopwords and Lemmatize:  0.016954898834228516 s\n",
      "\n",
      "Fix bad wording:  0.02293872833251953 s\n",
      "Tokenize:  0.04188823699951172 s\n",
      "Remove stopwords and Lemmatize:  0.11369585990905762 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.00498652458190918 s\n",
      "\n",
      "Fix bad wording:  0.008975744247436523 s\n",
      "Tokenize:  0.014961957931518555 s\n",
      "Remove stopwords and Lemmatize:  0.05784440040588379 s\n",
      "\n",
      "Fix bad wording:  0.010969400405883789 s\n",
      "Tokenize:  0.02293872833251953 s\n",
      "Remove stopwords and Lemmatize:  0.07380318641662598 s\n",
      "\n",
      "Fix bad wording:  0.007978200912475586 s\n",
      "Tokenize:  0.009974241256713867 s\n",
      "Remove stopwords and Lemmatize:  0.04488015174865723 s\n",
      "\n",
      "Fix bad wording:  0.009973526000976562 s\n",
      "Tokenize:  0.018949031829833984 s\n",
      "Remove stopwords and Lemmatize:  0.07522296905517578 s\n",
      "\n",
      "Fix bad wording:  0.014961004257202148 s\n",
      "Tokenize:  0.0279233455657959 s\n",
      "Remove stopwords and Lemmatize:  0.08079123497009277 s\n",
      "\n",
      "Fix bad wording:  0.027029037475585938 s\n",
      "Tokenize:  0.03191375732421875 s\n",
      "Remove stopwords and Lemmatize:  0.11968016624450684 s\n",
      "\n",
      "Fix bad wording:  0.003988742828369141 s\n",
      "Tokenize:  0.005994558334350586 s\n",
      "Remove stopwords and Lemmatize:  0.01595020294189453 s\n",
      "\n",
      "Fix bad wording:  0.004012584686279297 s\n",
      "Tokenize:  0.00796651840209961 s\n",
      "Remove stopwords and Lemmatize:  0.020485639572143555 s\n",
      "\n",
      "Fix bad wording:  0.018943309783935547 s\n",
      "Tokenize:  0.025930404663085938 s\n",
      "Remove stopwords and Lemmatize:  0.0857706069946289 s\n",
      "\n",
      "Fix bad wording:  0.027925968170166016 s\n",
      "Tokenize:  0.043883562088012695 s\n",
      "Remove stopwords and Lemmatize:  0.14461231231689453 s\n",
      "\n",
      "Fix bad wording:  0.01994633674621582 s\n",
      "Tokenize:  0.0359044075012207 s\n",
      "Remove stopwords and Lemmatize:  0.057845354080200195 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.0059833526611328125 s\n",
      "Remove stopwords and Lemmatize:  0.018948793411254883 s\n",
      "\n",
      "Fix bad wording:  0.018949508666992188 s\n",
      "Tokenize:  0.041886329650878906 s\n",
      "Remove stopwords and Lemmatize:  0.10671496391296387 s\n",
      "\n",
      "Fix bad wording:  0.03989243507385254 s\n",
      "Tokenize:  0.05585122108459473 s\n",
      "Remove stopwords and Lemmatize:  0.09175586700439453 s\n",
      "\n",
      "Fix bad wording:  0.004984140396118164 s\n",
      "Tokenize:  0.009974241256713867 s\n",
      "Remove stopwords and Lemmatize:  0.04488062858581543 s\n",
      "\n",
      "Fix bad wording:  0.010970115661621094 s\n",
      "Tokenize:  0.012965917587280273 s\n",
      "Remove stopwords and Lemmatize:  0.05684924125671387 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.007976770401000977 s\n",
      "Remove stopwords and Lemmatize:  0.0219423770904541 s\n",
      "\n",
      "Fix bad wording:  0.0019936561584472656 s\n",
      "Tokenize:  0.003988981246948242 s\n",
      "Remove stopwords and Lemmatize:  0.00797891616821289 s\n",
      "\n",
      "Fix bad wording:  0.012965202331542969 s\n",
      "Tokenize:  0.021943330764770508 s\n",
      "Remove stopwords and Lemmatize:  0.05185985565185547 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.001994609832763672 s\n",
      "Remove stopwords and Lemmatize:  0.0049860477447509766 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.0029926300048828125 s\n",
      "Remove stopwords and Lemmatize:  0.013962745666503906 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.023935794830322266 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.003989458084106445 s\n",
      "Remove stopwords and Lemmatize:  0.012966632843017578 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.013962984085083008 s\n",
      "\n",
      "Fix bad wording:  0.008975744247436523 s\n",
      "Tokenize:  0.00997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.04188823699951172 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.006983280181884766 s\n",
      "Remove stopwords and Lemmatize:  0.011966705322265625 s\n",
      "\n",
      "Fix bad wording:  0.004987001419067383 s\n",
      "Tokenize:  0.009971857070922852 s\n",
      "Remove stopwords and Lemmatize:  0.029918909072875977 s\n",
      "\n",
      "Fix bad wording:  0.00099945068359375 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.0029921531677246094 s\n",
      "\n",
      "Fix bad wording:  0.0029900074005126953 s\n",
      "Tokenize:  0.004988193511962891 s\n",
      "Remove stopwords and Lemmatize:  0.013962030410766602 s\n",
      "\n",
      "Fix bad wording:  0.0029914379119873047 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize:  0.0059854984283447266 s\n",
      "Remove stopwords and Lemmatize:  0.029918193817138672 s\n",
      "\n",
      "Fix bad wording:  0.0019953250885009766 s\n",
      "Tokenize:  0.0029990673065185547 s\n",
      "Remove stopwords and Lemmatize:  0.009966611862182617 s\n",
      "\n",
      "Fix bad wording:  0.011983394622802734 s\n",
      "Tokenize:  0.01992964744567871 s\n",
      "Remove stopwords and Lemmatize:  0.032915592193603516 s\n",
      "\n",
      "Fix bad wording:  0.008977890014648438 s\n",
      "Tokenize:  0.02592778205871582 s\n",
      "Remove stopwords and Lemmatize:  0.09873843193054199 s\n",
      "\n",
      "Fix bad wording:  0.03989267349243164 s\n",
      "Tokenize:  0.06781697273254395 s\n",
      "Remove stopwords and Lemmatize:  0.15658140182495117 s\n",
      "\n",
      "Fix bad wording:  0.01695561408996582 s\n",
      "Tokenize:  0.03191733360290527 s\n",
      "Remove stopwords and Lemmatize:  0.06183195114135742 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.003989219665527344 s\n",
      "Remove stopwords and Lemmatize:  0.021941423416137695 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.006982088088989258 s\n",
      "Remove stopwords and Lemmatize:  0.01795172691345215 s\n",
      "\n",
      "Fix bad wording:  0.007978200912475586 s\n",
      "Tokenize:  0.012965202331542969 s\n",
      "Remove stopwords and Lemmatize:  0.028922557830810547 s\n",
      "\n",
      "Fix bad wording:  0.003990888595581055 s\n",
      "Tokenize:  0.007977962493896484 s\n",
      "Remove stopwords and Lemmatize:  0.022939443588256836 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.002991199493408203 s\n",
      "Remove stopwords and Lemmatize:  0.006981849670410156 s\n",
      "\n",
      "Fix bad wording:  0.0019941329956054688 s\n",
      "Tokenize:  0.003989458084106445 s\n",
      "Remove stopwords and Lemmatize:  0.010971307754516602 s\n",
      "\n",
      "Fix bad wording:  0.005982398986816406 s\n",
      "Tokenize:  0.013963460922241211 s\n",
      "Remove stopwords and Lemmatize:  0.03789830207824707 s\n",
      "\n",
      "Fix bad wording:  0.003994464874267578 s\n",
      "Tokenize:  0.006973743438720703 s\n",
      "Remove stopwords and Lemmatize:  0.01795196533203125 s\n",
      "\n",
      "Fix bad wording:  0.006989717483520508 s\n",
      "Tokenize:  0.014950752258300781 s\n",
      "Remove stopwords and Lemmatize:  0.04188847541809082 s\n",
      "\n",
      "Fix bad wording:  0.0019943714141845703 s\n",
      "Tokenize:  0.001995563507080078 s\n",
      "Remove stopwords and Lemmatize:  0.004986286163330078 s\n",
      "\n",
      "Fix bad wording:  0.002993345260620117 s\n",
      "Tokenize:  0.00299072265625 s\n",
      "Remove stopwords and Lemmatize:  0.011968851089477539 s\n",
      "\n",
      "Fix bad wording:  0.0010051727294921875 s\n",
      "Tokenize:  0.001987457275390625 s\n",
      "Remove stopwords and Lemmatize:  0.004987001419067383 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0 s\n",
      "\n",
      "Fix bad wording:  0.004986286163330078 s\n",
      "Tokenize:  0.00997471809387207 s\n",
      "Remove stopwords and Lemmatize:  0.023936748504638672 s\n",
      "\n",
      "Fix bad wording:  0.001993417739868164 s\n",
      "Tokenize:  0.0009992122650146484 s\n",
      "Remove stopwords and Lemmatize:  0.004986286163330078 s\n",
      "\n",
      "Fix bad wording:  0.001993894577026367 s\n",
      "Tokenize:  0.004987239837646484 s\n",
      "Remove stopwords and Lemmatize:  0.011966943740844727 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0029914379119873047 s\n",
      "Remove stopwords and Lemmatize:  0.005985260009765625 s\n",
      "\n",
      "Fix bad wording:  0.0009953975677490234 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.01296544075012207 s\n",
      "\n",
      "Fix bad wording:  0.006982326507568359 s\n",
      "Tokenize:  0.013961553573608398 s\n",
      "Remove stopwords and Lemmatize:  0.04587697982788086 s\n",
      "\n",
      "Fix bad wording:  0.0019953250885009766 s\n",
      "Tokenize:  0.0029916763305664062 s\n",
      "Remove stopwords and Lemmatize:  0.007978200912475586 s\n",
      "\n",
      "Fix bad wording:  0.00797891616821289 s\n",
      "Tokenize:  0.014960050582885742 s\n",
      "Remove stopwords and Lemmatize:  0.045877695083618164 s\n",
      "\n",
      "Fix bad wording:  0.004990816116333008 s\n",
      "Tokenize:  0.009968996047973633 s\n",
      "Remove stopwords and Lemmatize:  0.028923511505126953 s\n",
      "\n",
      "Fix bad wording:  0.0010025501251220703 s\n",
      "Tokenize:  0.0019888877868652344 s\n",
      "Remove stopwords and Lemmatize:  0.002992391586303711 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.01396632194519043 s\n",
      "\n",
      "Fix bad wording:  0.010967016220092773 s\n",
      "Tokenize:  0.01795220375061035 s\n",
      "Remove stopwords and Lemmatize:  0.049866676330566406 s\n",
      "\n",
      "Fix bad wording:  0.013968706130981445 s\n",
      "Tokenize:  0.01894402503967285 s\n",
      "Remove stopwords and Lemmatize:  0.04302573204040527 s\n",
      "\n",
      "Fix bad wording:  0.005984783172607422 s\n",
      "Tokenize:  0.005977153778076172 s\n",
      "Remove stopwords and Lemmatize:  0.01894998550415039 s\n",
      "\n",
      "Fix bad wording:  0.0039882659912109375 s\n",
      "Tokenize:  0.006981611251831055 s\n",
      "Remove stopwords and Lemmatize:  0.019950389862060547 s\n",
      "\n",
      "Fix bad wording:  0.005980014801025391 s\n",
      "Tokenize:  0.016954898834228516 s\n",
      "Remove stopwords and Lemmatize:  0.040891170501708984 s\n",
      "\n",
      "Fix bad wording:  0.0039899349212646484 s\n",
      "Tokenize:  0.007977485656738281 s\n",
      "Remove stopwords and Lemmatize:  0.02792501449584961 s\n",
      "\n",
      "Fix bad wording:  0.015956878662109375 s\n",
      "Tokenize:  0.025931119918823242 s\n",
      "Remove stopwords and Lemmatize:  0.05983924865722656 s\n",
      "\n",
      "Fix bad wording:  0.00997471809387207 s\n",
      "Tokenize:  0.016953229904174805 s\n",
      "Remove stopwords and Lemmatize:  0.04388284683227539 s\n",
      "\n",
      "Fix bad wording:  0.007977724075317383 s\n",
      "Tokenize:  0.00797891616821289 s\n",
      "Remove stopwords and Lemmatize:  0.028921127319335938 s\n",
      "\n",
      "Fix bad wording:  0.015958786010742188 s\n",
      "Tokenize:  0.022939443588256836 s\n",
      "Remove stopwords and Lemmatize:  0.07081031799316406 s\n",
      "\n",
      "Fix bad wording:  0.0029926300048828125 s\n",
      "Tokenize:  0.008976936340332031 s\n",
      "Remove stopwords and Lemmatize:  0.022937536239624023 s\n",
      "\n",
      "Fix bad wording:  0.01595759391784668 s\n",
      "Tokenize:  0.03092050552368164 s\n",
      "Remove stopwords and Lemmatize:  0.09079980850219727 s\n",
      "\n",
      "Fix bad wording:  0.005980491638183594 s\n",
      "Tokenize:  0.00997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.014969348907470703 s\n",
      "\n",
      "Fix bad wording:  0.007976055145263672 s\n",
      "Tokenize:  0.011968851089477539 s\n",
      "Remove stopwords and Lemmatize:  0.0498661994934082 s\n",
      "\n",
      "Fix bad wording:  0.0049860477447509766 s\n",
      "Tokenize:  0.009974241256713867 s\n",
      "Remove stopwords and Lemmatize:  0.025931119918823242 s\n",
      "\n",
      "Fix bad wording:  0.022938251495361328 s\n",
      "Tokenize:  0.024933815002441406 s\n",
      "Remove stopwords and Lemmatize:  0.12067770957946777 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0009975433349609375 s\n",
      "Remove stopwords and Lemmatize:  0.003989458084106445 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.0049860477447509766 s\n",
      "\n",
      "Fix bad wording:  0.003989458084106445 s\n",
      "Tokenize:  0.005984306335449219 s\n",
      "Remove stopwords and Lemmatize:  0.023934125900268555 s\n",
      "\n",
      "Fix bad wording:  0.003989458084106445 s\n",
      "Tokenize:  0.005983591079711914 s\n",
      "Remove stopwords and Lemmatize:  0.015957355499267578 s\n",
      "\n",
      "Fix bad wording:  0.00897526741027832 s\n",
      "Tokenize:  0.00797724723815918 s\n",
      "Remove stopwords and Lemmatize:  0.03391122817993164 s\n",
      "\n",
      "Fix bad wording:  0.007976770401000977 s\n",
      "Tokenize:  0.015958309173583984 s\n",
      "Remove stopwords and Lemmatize:  0.04787015914916992 s\n",
      "\n",
      "Fix bad wording:  0.007977962493896484 s\n",
      "Tokenize:  0.009973287582397461 s\n",
      "Remove stopwords and Lemmatize:  0.04188990592956543 s\n",
      "\n",
      "Fix bad wording:  0.001993894577026367 s\n",
      "Tokenize:  0.0019953250885009766 s\n",
      "Remove stopwords and Lemmatize:  0.0049860477447509766 s\n",
      "\n",
      "Fix bad wording:  0.008980274200439453 s\n",
      "Tokenize:  0.013957977294921875 s\n",
      "Remove stopwords and Lemmatize:  0.03890252113342285 s\n",
      "\n",
      "Fix bad wording:  0.010971307754516602 s\n",
      "Tokenize:  0.01396036148071289 s\n",
      "Remove stopwords and Lemmatize:  0.044881343841552734 s\n",
      "\n",
      "Fix bad wording:  0.0019941329956054688 s\n",
      "Tokenize:  0.003988981246948242 s\n",
      "Remove stopwords and Lemmatize:  0.0109710693359375 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.0009970664978027344 s\n",
      "Remove stopwords and Lemmatize:  0.0039899349212646484 s\n",
      "\n",
      "Fix bad wording:  0.004985332489013672 s\n",
      "Tokenize:  0.01496124267578125 s\n",
      "Remove stopwords and Lemmatize:  0.03789806365966797 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.0069806575775146484 s\n",
      "Remove stopwords and Lemmatize:  0.021943330764770508 s\n",
      "\n",
      "Fix bad wording:  0.011967182159423828 s\n",
      "Tokenize:  0.014960527420043945 s\n",
      "Remove stopwords and Lemmatize:  0.0468747615814209 s\n",
      "\n",
      "Fix bad wording:  0.0029942989349365234 s\n",
      "Tokenize:  0.006983757019042969 s\n",
      "Remove stopwords and Lemmatize:  0.015954971313476562 s\n",
      "\n",
      "Fix bad wording:  0.0009968280792236328 s\n",
      "Tokenize:  0.001993894577026367 s\n",
      "Remove stopwords and Lemmatize:  0.008976459503173828 s\n",
      "\n",
      "Fix bad wording:  0.008976221084594727 s\n",
      "Tokenize:  0.013962745666503906 s\n",
      "Remove stopwords and Lemmatize:  0.02792501449584961 s\n",
      "\n",
      "Fix bad wording:  0.006983280181884766 s\n",
      "Tokenize:  0.009971380233764648 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.026927709579467773 s\n",
      "\n",
      "Fix bad wording:  0.004986763000488281 s\n",
      "Tokenize:  0.005984783172607422 s\n",
      "Remove stopwords and Lemmatize:  0.024932861328125 s\n",
      "\n",
      "Fix bad wording:  0.003989696502685547 s\n",
      "Tokenize:  0.005983591079711914 s\n",
      "Remove stopwords and Lemmatize:  0.01496577262878418 s\n",
      "\n",
      "Fix bad wording:  0.0029859542846679688 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.021941184997558594 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.004986763000488281 s\n",
      "Remove stopwords and Lemmatize:  0.017951250076293945 s\n",
      "\n",
      "Fix bad wording:  0.005984783172607422 s\n",
      "Tokenize:  0.011970043182373047 s\n",
      "Remove stopwords and Lemmatize:  0.03689885139465332 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.0019953250885009766 s\n",
      "Remove stopwords and Lemmatize:  0.00797891616821289 s\n",
      "\n",
      "Fix bad wording:  0.01496124267578125 s\n",
      "Tokenize:  0.02393507957458496 s\n",
      "Remove stopwords and Lemmatize:  0.07280731201171875 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.0010066032409667969 s\n",
      "Remove stopwords and Lemmatize:  0.004991769790649414 s\n",
      "\n",
      "Fix bad wording:  0.007972955703735352 s\n",
      "Tokenize:  0.014960527420043945 s\n",
      "Remove stopwords and Lemmatize:  0.0608372688293457 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0019941329956054688 s\n",
      "Remove stopwords and Lemmatize:  0.005984067916870117 s\n",
      "\n",
      "Fix bad wording:  0.005984783172607422 s\n",
      "Tokenize:  0.008974552154541016 s\n",
      "Remove stopwords and Lemmatize:  0.026934385299682617 s\n",
      "\n",
      "Fix bad wording:  0.0019893646240234375 s\n",
      "Tokenize:  0.001993894577026367 s\n",
      "Remove stopwords and Lemmatize:  0.005984067916870117 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.00399017333984375 s\n",
      "Remove stopwords and Lemmatize:  0.01296544075012207 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.0029926300048828125 s\n",
      "Remove stopwords and Lemmatize:  0.008975744247436523 s\n",
      "\n",
      "Fix bad wording:  0.027925968170166016 s\n",
      "Tokenize:  0.0388948917388916 s\n",
      "Remove stopwords and Lemmatize:  0.15857672691345215 s\n",
      "\n",
      "Fix bad wording:  0.01596355438232422 s\n",
      "Tokenize:  0.032906532287597656 s\n",
      "Remove stopwords and Lemmatize:  0.11469268798828125 s\n",
      "\n",
      "Fix bad wording:  0.003988742828369141 s\n",
      "Tokenize:  0.012965679168701172 s\n",
      "Remove stopwords and Lemmatize:  0.056847333908081055 s\n",
      "\n",
      "Fix bad wording:  0.01695418357849121 s\n",
      "Tokenize:  0.04687643051147461 s\n",
      "Remove stopwords and Lemmatize:  0.09275102615356445 s\n",
      "\n",
      "Fix bad wording:  0.002995729446411133 s\n",
      "Tokenize:  0.004982709884643555 s\n",
      "Remove stopwords and Lemmatize:  0.01795220375061035 s\n",
      "\n",
      "Fix bad wording:  0.005984306335449219 s\n",
      "Tokenize:  0.01495981216430664 s\n",
      "Remove stopwords and Lemmatize:  0.032912254333496094 s\n",
      "\n",
      "Fix bad wording:  0.00698089599609375 s\n",
      "Tokenize:  0.010972261428833008 s\n",
      "Remove stopwords and Lemmatize:  0.024933815002441406 s\n",
      "\n",
      "Fix bad wording:  0.006981611251831055 s\n",
      "Tokenize:  0.007978200912475586 s\n",
      "Remove stopwords and Lemmatize:  0.033908843994140625 s\n",
      "\n",
      "Fix bad wording:  0.012964487075805664 s\n",
      "Tokenize:  0.01894974708557129 s\n",
      "Remove stopwords and Lemmatize:  0.06283116340637207 s\n",
      "\n",
      "Fix bad wording:  0.012966156005859375 s\n",
      "Tokenize:  0.019946813583374023 s\n",
      "Remove stopwords and Lemmatize:  0.055849313735961914 s\n",
      "\n",
      "Fix bad wording:  0.004987001419067383 s\n",
      "Tokenize:  0.012965917587280273 s\n",
      "Remove stopwords and Lemmatize:  0.027924299240112305 s\n",
      "\n",
      "Fix bad wording:  0.0029854774475097656 s\n",
      "Tokenize:  0.005983829498291016 s\n",
      "Remove stopwords and Lemmatize:  0.017952680587768555 s\n",
      "\n",
      "Fix bad wording:  0.007977724075317383 s\n",
      "Tokenize:  0.024934053421020508 s\n",
      "Remove stopwords and Lemmatize:  0.05684781074523926 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0019941329956054688 s\n",
      "Remove stopwords and Lemmatize:  0.003988504409790039 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0009980201721191406 s\n",
      "Remove stopwords and Lemmatize:  0.0029914379119873047 s\n",
      "\n",
      "Fix bad wording:  0.008975982666015625 s\n",
      "Tokenize:  0.01595759391784668 s\n",
      "Remove stopwords and Lemmatize:  0.027924776077270508 s\n",
      "\n",
      "Fix bad wording:  0.008976459503173828 s\n",
      "Tokenize:  0.016954898834228516 s\n",
      "Remove stopwords and Lemmatize:  0.04588484764099121 s\n",
      "\n",
      "Fix bad wording:  0.0029935836791992188 s\n",
      "Tokenize:  0.0059814453125 s\n",
      "Remove stopwords and Lemmatize:  0.02593231201171875 s\n",
      "\n",
      "Fix bad wording:  0.010970115661621094 s\n",
      "Tokenize:  0.018949031829833984 s\n",
      "Remove stopwords and Lemmatize:  0.05186104774475098 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.0029914379119873047 s\n",
      "\n",
      "Fix bad wording:  0.0009984970092773438 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.002991199493408203 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0019948482513427734 s\n",
      "Remove stopwords and Lemmatize:  0.009971380233764648 s\n",
      "\n",
      "Fix bad wording:  0.010971307754516602 s\n",
      "Tokenize:  0.011968851089477539 s\n",
      "Remove stopwords and Lemmatize:  0.02892303466796875 s\n",
      "\n",
      "Fix bad wording:  0.00299072265625 s\n",
      "Tokenize:  0.0049860477447509766 s\n",
      "Remove stopwords and Lemmatize:  0.015958786010742188 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.002999544143676758 s\n",
      "\n",
      "Fix bad wording:  0.000989675521850586 s\n",
      "Tokenize:  0.0019948482513427734 s\n",
      "Remove stopwords and Lemmatize:  0.006979703903198242 s\n",
      "\n",
      "Fix bad wording:  0.005983829498291016 s\n",
      "Tokenize:  0.011968135833740234 s\n",
      "Remove stopwords and Lemmatize:  0.03789949417114258 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.009974241256713867 s\n",
      "\n",
      "Fix bad wording:  0.011966705322265625 s\n",
      "Tokenize:  0.02593088150024414 s\n",
      "Remove stopwords and Lemmatize:  0.08976030349731445 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0029921531677246094 s\n",
      "Remove stopwords and Lemmatize:  0.007978439331054688 s\n",
      "\n",
      "Fix bad wording:  0.011967658996582031 s\n",
      "Tokenize:  0.018949270248413086 s\n",
      "Remove stopwords and Lemmatize:  0.07280635833740234 s\n",
      "\n",
      "Fix bad wording:  0.003989696502685547 s\n",
      "Tokenize:  0.006981611251831055 s\n",
      "Remove stopwords and Lemmatize:  0.02094292640686035 s\n",
      "\n",
      "Fix bad wording:  0.015957117080688477 s\n",
      "Tokenize:  0.02692723274230957 s\n",
      "Remove stopwords and Lemmatize:  0.08278012275695801 s\n",
      "\n",
      "Fix bad wording:  0.014958620071411133 s\n",
      "Tokenize:  0.02892303466796875 s\n",
      "Remove stopwords and Lemmatize:  0.11369562149047852 s\n",
      "\n",
      "Fix bad wording:  0.002992391586303711 s\n",
      "Tokenize:  0.004986763000488281 s\n",
      "Remove stopwords and Lemmatize:  0.013962745666503906 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.006981611251831055 s\n",
      "Remove stopwords and Lemmatize:  0.02892279624938965 s\n",
      "\n",
      "Fix bad wording:  0.004987001419067383 s\n",
      "Tokenize:  0.008976221084594727 s\n",
      "Remove stopwords and Lemmatize:  0.021941661834716797 s\n",
      "\n",
      "Fix bad wording:  0.0009984970092773438 s\n",
      "Tokenize:  0.0009951591491699219 s\n",
      "Remove stopwords and Lemmatize:  0.00299835205078125 s\n",
      "\n",
      "Fix bad wording:  0.0059773921966552734 s\n",
      "Tokenize:  0.005983829498291016 s\n",
      "Remove stopwords and Lemmatize:  0.01795196533203125 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.007978439331054688 s\n",
      "Remove stopwords and Lemmatize:  0.01595759391784668 s\n",
      "\n",
      "Fix bad wording:  0.001995563507080078 s\n",
      "Tokenize:  0.001996278762817383 s\n",
      "Remove stopwords and Lemmatize:  0.008975505828857422 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0019953250885009766 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0010056495666503906 s\n",
      "\n",
      "Fix bad wording:  0.006980419158935547 s\n",
      "Tokenize:  0.012964963912963867 s\n",
      "Remove stopwords and Lemmatize:  0.0498659610748291 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0019943714141845703 s\n",
      "\n",
      "Fix bad wording:  0.014962196350097656 s\n",
      "Tokenize:  0.022937774658203125 s\n",
      "Remove stopwords and Lemmatize:  0.0718071460723877 s\n",
      "\n",
      "Fix bad wording:  0.016954421997070312 s\n",
      "Tokenize:  0.030918121337890625 s\n",
      "Remove stopwords and Lemmatize:  0.05884432792663574 s\n",
      "\n",
      "Fix bad wording:  0.001993417739868164 s\n",
      "Tokenize:  0.003988981246948242 s\n",
      "Remove stopwords and Lemmatize:  0.009973526000976562 s\n",
      "\n",
      "Fix bad wording:  0.019947052001953125 s\n",
      "Tokenize:  0.046874284744262695 s\n",
      "Remove stopwords and Lemmatize:  0.11070442199707031 s\n",
      "\n",
      "Fix bad wording:  0.009972810745239258 s\n",
      "Tokenize:  0.017951250076293945 s\n",
      "Remove stopwords and Lemmatize:  0.050865888595581055 s\n",
      "\n",
      "Fix bad wording:  0.011968135833740234 s\n",
      "Tokenize:  0.02094578742980957 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.05485343933105469 s\n",
      "\n",
      "Fix bad wording:  0.0029888153076171875 s\n",
      "Tokenize:  0.006981849670410156 s\n",
      "Remove stopwords and Lemmatize:  0.012965679168701172 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.005984067916870117 s\n",
      "Remove stopwords and Lemmatize:  0.016954660415649414 s\n",
      "\n",
      "Fix bad wording:  0.0049860477447509766 s\n",
      "Tokenize:  0.01596236228942871 s\n",
      "Remove stopwords and Lemmatize:  0.027927160263061523 s\n",
      "\n",
      "Fix bad wording:  0.001995563507080078 s\n",
      "Tokenize:  0.0039882659912109375 s\n",
      "Remove stopwords and Lemmatize:  0.016954421997070312 s\n",
      "\n",
      "Fix bad wording:  0.005984783172607422 s\n",
      "Tokenize:  0.010970830917358398 s\n",
      "Remove stopwords and Lemmatize:  0.04288506507873535 s\n",
      "\n",
      "Fix bad wording:  0.015956640243530273 s\n",
      "Tokenize:  0.02892327308654785 s\n",
      "Remove stopwords and Lemmatize:  0.08178162574768066 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0019948482513427734 s\n",
      "Remove stopwords and Lemmatize:  0.003989696502685547 s\n",
      "\n",
      "Fix bad wording:  0.003988504409790039 s\n",
      "Tokenize:  0.004987478256225586 s\n",
      "Remove stopwords and Lemmatize:  0.017952442169189453 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.0019943714141845703 s\n",
      "\n",
      "Fix bad wording:  0.0109710693359375 s\n",
      "Tokenize:  0.016953468322753906 s\n",
      "Remove stopwords and Lemmatize:  0.05684804916381836 s\n",
      "\n",
      "Fix bad wording:  0.003988504409790039 s\n",
      "Tokenize:  0.006980419158935547 s\n",
      "Remove stopwords and Lemmatize:  0.02294015884399414 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.004986763000488281 s\n",
      "Remove stopwords and Lemmatize:  0.01496124267578125 s\n",
      "\n",
      "Fix bad wording:  0.001993417739868164 s\n",
      "Tokenize:  0.0039904117584228516 s\n",
      "Remove stopwords and Lemmatize:  0.006982088088989258 s\n",
      "\n",
      "Fix bad wording:  0.003986835479736328 s\n",
      "Tokenize:  0.0069811344146728516 s\n",
      "Remove stopwords and Lemmatize:  0.019947528839111328 s\n",
      "\n",
      "Fix bad wording:  0.011967897415161133 s\n",
      "Tokenize:  0.010969400405883789 s\n",
      "Remove stopwords and Lemmatize:  0.057845354080200195 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.003988742828369141 s\n",
      "\n",
      "Fix bad wording:  0.013962745666503906 s\n",
      "Tokenize:  0.01894998550415039 s\n",
      "Remove stopwords and Lemmatize:  0.06482815742492676 s\n",
      "\n",
      "Fix bad wording:  0.0009877681732177734 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.006981372833251953 s\n",
      "\n",
      "Fix bad wording:  0.008975744247436523 s\n",
      "Tokenize:  0.009972333908081055 s\n",
      "Remove stopwords and Lemmatize:  0.03291201591491699 s\n",
      "\n",
      "Fix bad wording:  0.011977195739746094 s\n",
      "Tokenize:  0.017942428588867188 s\n",
      "Remove stopwords and Lemmatize:  0.05684852600097656 s\n",
      "\n",
      "Fix bad wording:  0.007979154586791992 s\n",
      "Tokenize:  0.013962984085083008 s\n",
      "Remove stopwords and Lemmatize:  0.039893388748168945 s\n",
      "\n",
      "Fix bad wording:  0.004987001419067383 s\n",
      "Tokenize:  0.007978200912475586 s\n",
      "Remove stopwords and Lemmatize:  0.022939205169677734 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.0029921531677246094 s\n",
      "\n",
      "Fix bad wording:  0.014959573745727539 s\n",
      "Tokenize:  0.02493429183959961 s\n",
      "Remove stopwords and Lemmatize:  0.07978606224060059 s\n",
      "\n",
      "Fix bad wording:  0.012966156005859375 s\n",
      "Tokenize:  0.022476673126220703 s\n",
      "Remove stopwords and Lemmatize:  0.07880091667175293 s\n",
      "\n",
      "Fix bad wording:  0.003976345062255859 s\n",
      "Tokenize:  0.0049877166748046875 s\n",
      "Remove stopwords and Lemmatize:  0.0229337215423584 s\n",
      "\n",
      "Fix bad wording:  0.002991914749145508 s\n",
      "Tokenize:  0.004986763000488281 s\n",
      "Remove stopwords and Lemmatize:  0.016958236694335938 s\n",
      "\n",
      "Fix bad wording:  0.007971763610839844 s\n",
      "Tokenize:  0.01595759391784668 s\n",
      "Remove stopwords and Lemmatize:  0.042886972427368164 s\n",
      "\n",
      "Fix bad wording:  0.017945528030395508 s\n",
      "Tokenize:  0.028922557830810547 s\n",
      "Remove stopwords and Lemmatize:  0.09174776077270508 s\n",
      "\n",
      "Fix bad wording:  0.006989240646362305 s\n",
      "Tokenize:  0.010962724685668945 s\n",
      "Remove stopwords and Lemmatize:  0.04488229751586914 s\n",
      "\n",
      "Fix bad wording:  0.005983829498291016 s\n",
      "Tokenize:  0.011968135833740234 s\n",
      "Remove stopwords and Lemmatize:  0.03390955924987793 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.0049860477447509766 s\n",
      "Remove stopwords and Lemmatize:  0.013963699340820312 s\n",
      "\n",
      "Fix bad wording:  0.009971857070922852 s\n",
      "Tokenize:  0.017954111099243164 s\n",
      "Remove stopwords and Lemmatize:  0.04587554931640625 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.007977962493896484 s\n",
      "\n",
      "Fix bad wording:  0.004987478256225586 s\n",
      "Tokenize:  0.012972116470336914 s\n",
      "Remove stopwords and Lemmatize:  0.058840274810791016 s\n",
      "\n",
      "Fix bad wording:  0.004007816314697266 s\n",
      "Tokenize:  0.0049724578857421875 s\n",
      "Remove stopwords and Lemmatize:  0.01695537567138672 s\n",
      "\n",
      "Fix bad wording:  0.0020067691802978516 s\n",
      "Tokenize:  0.003988742828369141 s\n",
      "Remove stopwords and Lemmatize:  0.013955354690551758 s\n",
      "\n",
      "Fix bad wording:  0.04487967491149902 s\n",
      "Tokenize:  0.08078980445861816 s\n",
      "Remove stopwords and Lemmatize:  0.2662839889526367 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.006982088088989258 s\n",
      "Remove stopwords and Lemmatize:  0.017951488494873047 s\n",
      "\n",
      "Fix bad wording:  0.008973836898803711 s\n",
      "Tokenize:  0.021940946578979492 s\n",
      "Remove stopwords and Lemmatize:  0.05983924865722656 s\n",
      "\n",
      "Fix bad wording:  0.002991199493408203 s\n",
      "Tokenize:  0.005984783172607422 s\n",
      "Remove stopwords and Lemmatize:  0.014958620071411133 s\n",
      "\n",
      "Fix bad wording:  0.008977174758911133 s\n",
      "Tokenize:  0.010969400405883789 s\n",
      "Remove stopwords and Lemmatize:  0.04089188575744629 s\n",
      "\n",
      "Fix bad wording:  0.006983757019042969 s\n",
      "Tokenize:  0.011965036392211914 s\n",
      "Remove stopwords and Lemmatize:  0.03390932083129883 s\n",
      "\n",
      "Fix bad wording:  0.008975982666015625 s\n",
      "Tokenize:  0.01495981216430664 s\n",
      "Remove stopwords and Lemmatize:  0.03889727592468262 s\n",
      "\n",
      "Fix bad wording:  0.000997781753540039 s\n",
      "Tokenize:  0.001993894577026367 s\n",
      "Remove stopwords and Lemmatize:  0.004987001419067383 s\n",
      "\n",
      "Fix bad wording:  0.00698089599609375 s\n",
      "Tokenize:  0.011966466903686523 s\n",
      "Remove stopwords and Lemmatize:  0.03789925575256348 s\n",
      "\n",
      "Fix bad wording:  0.013962268829345703 s\n",
      "Tokenize:  0.010968685150146484 s\n",
      "Remove stopwords and Lemmatize:  0.04886960983276367 s\n",
      "\n",
      "Fix bad wording:  0.0019941329956054688 s\n",
      "Tokenize:  0.003989219665527344 s\n",
      "Remove stopwords and Lemmatize:  0.014963865280151367 s\n",
      "\n",
      "Fix bad wording:  0.000993490219116211 s\n",
      "Tokenize:  0.0009975433349609375 s\n",
      "Remove stopwords and Lemmatize:  0.0019941329956054688 s\n",
      "\n",
      "Fix bad wording:  0.009972572326660156 s\n",
      "Tokenize:  0.017956972122192383 s\n",
      "Remove stopwords and Lemmatize:  0.07480430603027344 s\n",
      "\n",
      "Fix bad wording:  0.022933244705200195 s\n",
      "Tokenize:  0.04787325859069824 s\n",
      "Remove stopwords and Lemmatize:  0.14261746406555176 s\n",
      "\n",
      "Fix bad wording:  0.027927875518798828 s\n",
      "Tokenize:  0.034906864166259766 s\n",
      "Remove stopwords and Lemmatize:  0.0917518138885498 s\n",
      "\n",
      "Fix bad wording:  0.001997232437133789 s\n",
      "Tokenize:  0.003988742828369141 s\n",
      "Remove stopwords and Lemmatize:  0.006979227066040039 s\n",
      "\n",
      "Fix bad wording:  0.0029935836791992188 s\n",
      "Tokenize:  0.0039882659912109375 s\n",
      "Remove stopwords and Lemmatize:  0.022936344146728516 s\n",
      "\n",
      "Fix bad wording:  0.013960599899291992 s\n",
      "Tokenize:  0.016954898834228516 s\n",
      "Remove stopwords and Lemmatize:  0.037899017333984375 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.0019943714141845703 s\n",
      "\n",
      "Fix bad wording:  0.013962507247924805 s\n",
      "Tokenize:  0.02593088150024414 s\n",
      "Remove stopwords and Lemmatize:  0.08676719665527344 s\n",
      "\n",
      "Fix bad wording:  0.005984067916870117 s\n",
      "Tokenize:  0.008977174758911133 s\n",
      "Remove stopwords and Lemmatize:  0.031914710998535156 s\n",
      "\n",
      "Fix bad wording:  0.018949270248413086 s\n",
      "Tokenize:  0.02792644500732422 s\n",
      "Remove stopwords and Lemmatize:  0.07978558540344238 s\n",
      "\n",
      "Fix bad wording:  0.007978200912475586 s\n",
      "Tokenize:  0.013962745666503906 s\n",
      "Remove stopwords and Lemmatize:  0.04887080192565918 s\n",
      "\n",
      "Fix bad wording:  0.003989458084106445 s\n",
      "Tokenize:  0.006981849670410156 s\n",
      "Remove stopwords and Lemmatize:  0.018948793411254883 s\n",
      "\n",
      "Fix bad wording:  0.028922319412231445 s\n",
      "Tokenize:  0.03789949417114258 s\n",
      "Remove stopwords and Lemmatize:  0.07978510856628418 s\n",
      "\n",
      "Fix bad wording:  0.006982088088989258 s\n",
      "Tokenize:  0.013961553573608398 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.041889190673828125 s\n",
      "\n",
      "Fix bad wording:  0.020943880081176758 s\n",
      "Tokenize:  0.03191423416137695 s\n",
      "Remove stopwords and Lemmatize:  0.0608367919921875 s\n",
      "\n",
      "Fix bad wording:  0.01296544075012207 s\n",
      "Tokenize:  0.023936748504638672 s\n",
      "Remove stopwords and Lemmatize:  0.09674239158630371 s\n",
      "\n",
      "Fix bad wording:  0.006979703903198242 s\n",
      "Tokenize:  0.014960765838623047 s\n",
      "Remove stopwords and Lemmatize:  0.041887521743774414 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.0029921531677246094 s\n",
      "Remove stopwords and Lemmatize:  0.00598454475402832 s\n",
      "\n",
      "Fix bad wording:  0.0039882659912109375 s\n",
      "Tokenize:  0.0069811344146728516 s\n",
      "Remove stopwords and Lemmatize:  0.02094292640686035 s\n",
      "\n",
      "Fix bad wording:  0.007979393005371094 s\n",
      "Tokenize:  0.012965679168701172 s\n",
      "Remove stopwords and Lemmatize:  0.029919862747192383 s\n",
      "\n",
      "Fix bad wording:  0.008976459503173828 s\n",
      "Tokenize:  0.016954421997070312 s\n",
      "Remove stopwords and Lemmatize:  0.04488039016723633 s\n",
      "\n",
      "Fix bad wording:  0.0049860477447509766 s\n",
      "Tokenize:  0.010970592498779297 s\n",
      "Remove stopwords and Lemmatize:  0.04986834526062012 s\n",
      "\n",
      "Fix bad wording:  0.0049860477447509766 s\n",
      "Tokenize:  0.006981849670410156 s\n",
      "Remove stopwords and Lemmatize:  0.01994609832763672 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.003991127014160156 s\n",
      "Remove stopwords and Lemmatize:  0.011966943740844727 s\n",
      "\n",
      "Fix bad wording:  0.008977890014648438 s\n",
      "Tokenize:  0.013962030410766602 s\n",
      "Remove stopwords and Lemmatize:  0.03989124298095703 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0010304450988769531 s\n",
      "Remove stopwords and Lemmatize:  0.0009646415710449219 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.0019943714141845703 s\n",
      "Remove stopwords and Lemmatize:  0.005984067916870117 s\n",
      "\n",
      "Fix bad wording:  0.009972095489501953 s\n",
      "Tokenize:  0.01695561408996582 s\n",
      "Remove stopwords and Lemmatize:  0.034906864166259766 s\n",
      "\n",
      "Fix bad wording:  0.003989458084106445 s\n",
      "Tokenize:  0.007977485656738281 s\n",
      "Remove stopwords and Lemmatize:  0.014960289001464844 s\n",
      "\n",
      "Fix bad wording:  0.00099945068359375 s\n",
      "Tokenize:  0.0009946823120117188 s\n",
      "Remove stopwords and Lemmatize:  0.002991199493408203 s\n",
      "\n",
      "Fix bad wording:  0.009972810745239258 s\n",
      "Tokenize:  0.01695418357849121 s\n",
      "Remove stopwords and Lemmatize:  0.06383109092712402 s\n",
      "\n",
      "Fix bad wording:  0.004986286163330078 s\n",
      "Tokenize:  0.008978605270385742 s\n",
      "Remove stopwords and Lemmatize:  0.024931907653808594 s\n",
      "\n",
      "Fix bad wording:  0.009974479675292969 s\n",
      "Tokenize:  0.015956640243530273 s\n",
      "Remove stopwords and Lemmatize:  0.04488372802734375 s\n",
      "\n",
      "Fix bad wording:  0.0059795379638671875 s\n",
      "Tokenize:  0.008976936340332031 s\n",
      "Remove stopwords and Lemmatize:  0.02692866325378418 s\n",
      "\n",
      "Fix bad wording:  0.018959522247314453 s\n",
      "Tokenize:  0.033898115158081055 s\n",
      "Remove stopwords and Lemmatize:  0.05585145950317383 s\n",
      "\n",
      "Fix bad wording:  0.005984783172607422 s\n",
      "Tokenize:  0.00997161865234375 s\n",
      "Remove stopwords and Lemmatize:  0.0438838005065918 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.00897669792175293 s\n",
      "Remove stopwords and Lemmatize:  0.028922319412231445 s\n",
      "\n",
      "Fix bad wording:  0.002993345260620117 s\n",
      "Tokenize:  0.0039899349212646484 s\n",
      "Remove stopwords and Lemmatize:  0.0049855709075927734 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.0049860477447509766 s\n",
      "\n",
      "Fix bad wording:  0.001005411148071289 s\n",
      "Tokenize:  0.0019867420196533203 s\n",
      "Remove stopwords and Lemmatize:  0.003988981246948242 s\n",
      "\n",
      "Fix bad wording:  0.008976221084594727 s\n",
      "Tokenize:  0.010970592498779297 s\n",
      "Remove stopwords and Lemmatize:  0.030917644500732422 s\n",
      "\n",
      "Fix bad wording:  0.010970354080200195 s\n",
      "Tokenize:  0.018949031829833984 s\n",
      "Remove stopwords and Lemmatize:  0.07579731941223145 s\n",
      "\n",
      "Fix bad wording:  0.001995563507080078 s\n",
      "Tokenize:  0.0009965896606445312 s\n",
      "Remove stopwords and Lemmatize:  0.003989458084106445 s\n",
      "\n",
      "Fix bad wording:  0.005983591079711914 s\n",
      "Tokenize:  0.010970830917358398 s\n",
      "Remove stopwords and Lemmatize:  0.050864458084106445 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0 s\n",
      "Remove stopwords and Lemmatize:  0.0019948482513427734 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.0069828033447265625 s\n",
      "Remove stopwords and Lemmatize:  0.02194070816040039 s\n",
      "\n",
      "Fix bad wording:  0.009972333908081055 s\n",
      "Tokenize:  0.01795220375061035 s\n",
      "Remove stopwords and Lemmatize:  0.0578455924987793 s\n",
      "\n",
      "Fix bad wording:  0.003989219665527344 s\n",
      "Tokenize:  0.008975744247436523 s\n",
      "Remove stopwords and Lemmatize:  0.024933576583862305 s\n",
      "\n",
      "Fix bad wording:  0.011967182159423828 s\n",
      "Tokenize:  0.020943880081176758 s\n",
      "Remove stopwords and Lemmatize:  0.05285954475402832 s\n",
      "\n",
      "Fix bad wording:  0.004987478256225586 s\n",
      "Tokenize:  0.007979393005371094 s\n",
      "Remove stopwords and Lemmatize:  0.018948793411254883 s\n",
      "\n",
      "Fix bad wording:  0.01096963882446289 s\n",
      "Tokenize:  0.013962268829345703 s\n",
      "Remove stopwords and Lemmatize:  0.05086350440979004 s\n",
      "\n",
      "Fix bad wording:  0.0069811344146728516 s\n",
      "Tokenize:  0.010970830917358398 s\n",
      "Remove stopwords and Lemmatize:  0.048868417739868164 s\n",
      "\n",
      "Fix bad wording:  0.010971307754516602 s\n",
      "Tokenize:  0.019943952560424805 s\n",
      "Remove stopwords and Lemmatize:  0.05485415458679199 s\n",
      "\n",
      "Fix bad wording:  0.008975982666015625 s\n",
      "Tokenize:  0.009973764419555664 s\n",
      "Remove stopwords and Lemmatize:  0.02692580223083496 s\n",
      "\n",
      "Fix bad wording:  0.003988027572631836 s\n",
      "Tokenize:  0.004987239837646484 s\n",
      "Remove stopwords and Lemmatize:  0.009974002838134766 s\n",
      "\n",
      "Fix bad wording:  0.003988504409790039 s\n",
      "Tokenize:  0.00399017333984375 s\n",
      "Remove stopwords and Lemmatize:  0.013962268829345703 s\n",
      "\n",
      "Fix bad wording:  0.006982564926147461 s\n",
      "Tokenize:  0.012967348098754883 s\n",
      "Remove stopwords and Lemmatize:  0.06482458114624023 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.001995086669921875 s\n",
      "Remove stopwords and Lemmatize:  0.0069806575775146484 s\n",
      "\n",
      "Fix bad wording:  0.0009953975677490234 s\n",
      "Tokenize:  0.0009982585906982422 s\n",
      "Remove stopwords and Lemmatize:  0.0029938220977783203 s\n",
      "\n",
      "Fix bad wording:  0.004984617233276367 s\n",
      "Tokenize:  0.010971307754516602 s\n",
      "Remove stopwords and Lemmatize:  0.015958309173583984 s\n",
      "\n",
      "Fix bad wording:  0.0019948482513427734 s\n",
      "Tokenize:  0.004984140396118164 s\n",
      "Remove stopwords and Lemmatize:  0.006983280181884766 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.004986286163330078 s\n",
      "Remove stopwords and Lemmatize:  0.007980823516845703 s\n",
      "\n",
      "Fix bad wording:  0.0019927024841308594 s\n",
      "Tokenize:  0.004990100860595703 s\n",
      "Remove stopwords and Lemmatize:  0.010972976684570312 s\n",
      "\n",
      "Fix bad wording:  0.009968042373657227 s\n",
      "Tokenize:  0.012964963912963867 s\n",
      "Remove stopwords and Lemmatize:  0.02792644500732422 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.001993894577026367 s\n",
      "Remove stopwords and Lemmatize:  0.0049860477447509766 s\n",
      "\n",
      "Fix bad wording:  0.006981372833251953 s\n",
      "Tokenize:  0.01595783233642578 s\n",
      "Remove stopwords and Lemmatize:  0.031914472579956055 s\n",
      "\n",
      "Fix bad wording:  0.005984306335449219 s\n",
      "Tokenize:  0.017953157424926758 s\n",
      "Remove stopwords and Lemmatize:  0.029918432235717773 s\n",
      "\n",
      "Fix bad wording:  0.003991842269897461 s\n",
      "Tokenize:  0.009970903396606445 s\n",
      "Remove stopwords and Lemmatize:  0.01994776725769043 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.0029916763305664062 s\n",
      "Remove stopwords and Lemmatize:  0.007978200912475586 s\n",
      "\n",
      "Fix bad wording:  0.0329129695892334 s\n",
      "Tokenize:  0.06482577323913574 s\n",
      "Remove stopwords and Lemmatize:  0.09076070785522461 s\n",
      "\n",
      "Fix bad wording:  0.004983186721801758 s\n",
      "Tokenize:  0.012964963912963867 s\n",
      "Remove stopwords and Lemmatize:  0.037897586822509766 s\n",
      "\n",
      "Fix bad wording:  0.005984783172607422 s\n",
      "Tokenize:  0.016954660415649414 s\n",
      "Remove stopwords and Lemmatize:  0.04089093208312988 s\n",
      "\n",
      "Fix bad wording:  0.004987478256225586 s\n",
      "Tokenize:  0.00997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.016954898834228516 s\n",
      "\n",
      "Fix bad wording:  0.008974552154541016 s\n",
      "Tokenize:  0.019947052001953125 s\n",
      "Remove stopwords and Lemmatize:  0.03191685676574707 s\n",
      "\n",
      "Fix bad wording:  0.011966228485107422 s\n",
      "Tokenize:  0.03390908241271973 s\n",
      "Remove stopwords and Lemmatize:  0.08876395225524902 s\n",
      "\n",
      "Fix bad wording:  0.00498652458190918 s\n",
      "Tokenize:  0.008975505828857422 s\n",
      "Remove stopwords and Lemmatize:  0.023936986923217773 s\n",
      "\n",
      "Fix bad wording:  0.013962030410766602 s\n",
      "Tokenize:  0.03091740608215332 s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove stopwords and Lemmatize:  0.05186152458190918 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.008976459503173828 s\n",
      "Remove stopwords and Lemmatize:  0.01296377182006836 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.001995086669921875 s\n",
      "Remove stopwords and Lemmatize:  0.004986763000488281 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.0009970664978027344 s\n",
      "Remove stopwords and Lemmatize:  0.0019960403442382812 s\n",
      "\n",
      "Fix bad wording:  0.008974790573120117 s\n",
      "Tokenize:  0.026927709579467773 s\n",
      "Remove stopwords and Lemmatize:  0.051860809326171875 s\n",
      "\n",
      "Fix bad wording:  0.0059833526611328125 s\n",
      "Tokenize:  0.014961481094360352 s\n",
      "Remove stopwords and Lemmatize:  0.04687380790710449 s\n",
      "\n",
      "Fix bad wording:  0.003989696502685547 s\n",
      "Tokenize:  0.010970592498779297 s\n",
      "Remove stopwords and Lemmatize:  0.020943641662597656 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.004987001419067383 s\n",
      "Remove stopwords and Lemmatize:  0.01197504997253418 s\n",
      "\n",
      "Fix bad wording:  0.0009903907775878906 s\n",
      "Tokenize:  0.0029914379119873047 s\n",
      "Remove stopwords and Lemmatize:  0.007978677749633789 s\n",
      "\n",
      "Fix bad wording:  0.012965202331542969 s\n",
      "Tokenize:  0.03889656066894531 s\n",
      "Remove stopwords and Lemmatize:  0.07380151748657227 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.012965917587280273 s\n",
      "Remove stopwords and Lemmatize:  0.03291153907775879 s\n",
      "\n",
      "Fix bad wording:  0.000997304916381836 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.0019948482513427734 s\n",
      "\n",
      "Fix bad wording:  0.0029921531677246094 s\n",
      "Tokenize:  0.00698089599609375 s\n",
      "Remove stopwords and Lemmatize:  0.01695418357849121 s\n",
      "\n",
      "Fix bad wording:  0.036902427673339844 s\n",
      "Tokenize:  0.08676767349243164 s\n",
      "Remove stopwords and Lemmatize:  0.21941304206848145 s\n",
      "\n",
      "Fix bad wording:  0.011968135833740234 s\n",
      "Tokenize:  0.025931358337402344 s\n",
      "Remove stopwords and Lemmatize:  0.04487919807434082 s\n",
      "\n",
      "Fix bad wording:  0.003988981246948242 s\n",
      "Tokenize:  0.00897669792175293 s\n",
      "Remove stopwords and Lemmatize:  0.020944595336914062 s\n",
      "\n",
      "Fix bad wording:  0.012964725494384766 s\n",
      "Tokenize:  0.030916929244995117 s\n",
      "Remove stopwords and Lemmatize:  0.07081079483032227 s\n",
      "\n",
      "Fix bad wording:  0.0009975433349609375 s\n",
      "Tokenize:  0.0029914379119873047 s\n",
      "Remove stopwords and Lemmatize:  0.007979154586791992 s\n",
      "\n",
      "Fix bad wording:  0.0019936561584472656 s\n",
      "Tokenize:  0.003990888595581055 s\n",
      "Remove stopwords and Lemmatize:  0.007977724075317383 s\n",
      "\n",
      "Fix bad wording:  0.010970830917358398 s\n",
      "Tokenize:  0.022938966751098633 s\n",
      "Remove stopwords and Lemmatize:  0.059839725494384766 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.007978677749633789 s\n",
      "Remove stopwords and Lemmatize:  0.01795220375061035 s\n",
      "\n",
      "Fix bad wording:  0.0029916763305664062 s\n",
      "Tokenize:  0.004987478256225586 s\n",
      "Remove stopwords and Lemmatize:  0.016955852508544922 s\n",
      "\n",
      "Fix bad wording:  0.0009980201721191406 s\n",
      "Tokenize:  0.0009968280792236328 s\n",
      "Remove stopwords and Lemmatize:  0.0019958019256591797 s\n",
      "\n",
      "Fix bad wording:  0.010971784591674805 s\n",
      "Tokenize:  0.015955448150634766 s\n",
      "Remove stopwords and Lemmatize:  0.02792835235595703 s\n",
      "\n",
      "Fix bad wording:  0.008976221084594727 s\n",
      "Tokenize:  0.012972116470336914 s\n",
      "Remove stopwords and Lemmatize:  0.020936250686645508 s\n",
      "\n",
      "Fix bad wording:  0.0009970664978027344 s\n",
      "Tokenize:  0.000997304916381836 s\n",
      "Remove stopwords and Lemmatize:  0.002992391586303711 s\n",
      "\n",
      "Fix bad wording:  0.0 s\n",
      "Tokenize:  0.0009970664978027344 s\n",
      "Remove stopwords and Lemmatize:  0.007978677749633789 s\n",
      "\n",
      "Fix bad wording:  0.011967658996582031 s\n",
      "Tokenize:  0.021941423416137695 s\n",
      "Remove stopwords and Lemmatize:  0.040889739990234375 s\n",
      "\n",
      "Fix bad wording:  0.012970924377441406 s\n",
      "Tokenize:  0.013956785202026367 s\n",
      "Remove stopwords and Lemmatize:  0.06482720375061035 s\n",
      "\n",
      "Fix bad wording:  0.005983591079711914 s\n",
      "Tokenize:  0.01695418357849121 s\n",
      "Remove stopwords and Lemmatize:  0.05485272407531738 s\n",
      "\n",
      "Fix bad wording:  0.001995086669921875 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.011968851089477539 s\n",
      "\n",
      "Fix bad wording:  0.016954421997070312 s\n",
      "Tokenize:  0.026926755905151367 s\n",
      "Remove stopwords and Lemmatize:  0.060837745666503906 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.002991914749145508 s\n",
      "Remove stopwords and Lemmatize:  0.0069828033447265625 s\n",
      "\n",
      "Fix bad wording:  0.00398707389831543 s\n",
      "Tokenize:  0.006982088088989258 s\n",
      "Remove stopwords and Lemmatize:  0.018954992294311523 s\n",
      "\n",
      "Fix bad wording:  0.0009922981262207031 s\n",
      "Tokenize:  0.0009975433349609375 s\n",
      "Remove stopwords and Lemmatize:  0.004987001419067383 s\n",
      "\n",
      "Fix bad wording:  0.003988504409790039 s\n",
      "Tokenize:  0.007978677749633789 s\n",
      "Remove stopwords and Lemmatize:  0.026929140090942383 s\n",
      "\n",
      "Fix bad wording:  0.015956878662109375 s\n",
      "Tokenize:  0.0279238224029541 s\n",
      "Remove stopwords and Lemmatize:  0.08876323699951172 s\n",
      "\n",
      "Fix bad wording:  0.008975744247436523 s\n",
      "Tokenize:  0.014960289001464844 s\n",
      "Remove stopwords and Lemmatize:  0.054852962493896484 s\n",
      "\n",
      "Fix bad wording:  0.0039899349212646484 s\n",
      "Tokenize:  0.007978200912475586 s\n",
      "Remove stopwords and Lemmatize:  0.017952919006347656 s\n",
      "\n",
      "Fix bad wording:  0.010965585708618164 s\n",
      "Tokenize:  0.027925729751586914 s\n",
      "Remove stopwords and Lemmatize:  0.045876264572143555 s\n",
      "\n",
      "Fix bad wording:  0.013960838317871094 s\n",
      "Tokenize:  0.02592945098876953 s\n",
      "Remove stopwords and Lemmatize:  0.05286407470703125 s\n",
      "\n",
      "Fix bad wording:  0.013961315155029297 s\n",
      "Tokenize:  0.036902427673339844 s\n",
      "Remove stopwords and Lemmatize:  0.1107032299041748 s\n",
      "\n",
      "Fix bad wording:  0.006982564926147461 s\n",
      "Tokenize:  0.007977485656738281 s\n",
      "Remove stopwords and Lemmatize:  0.01595926284790039 s\n",
      "\n",
      "Fix bad wording:  0.001994609832763672 s\n",
      "Tokenize:  0.004985332489013672 s\n",
      "Remove stopwords and Lemmatize:  0.013962745666503906 s\n",
      "\n",
      "Fix bad wording:  0.0029926300048828125 s\n",
      "Tokenize:  0.0039899349212646484 s\n",
      "Remove stopwords and Lemmatize:  0.013962984085083008 s\n",
      "\n",
      "Fix bad wording:  0.0029914379119873047 s\n",
      "Tokenize:  0.0049860477447509766 s\n",
      "Remove stopwords and Lemmatize:  0.019945859909057617 s\n",
      "\n",
      "Fix bad wording:  0.003987550735473633 s\n",
      "Tokenize:  0.006981849670410156 s\n",
      "Remove stopwords and Lemmatize:  0.01795363426208496 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#https://github.com/bulentozel/OpenMaker/blob/master/Semantics/data/corpuses/schwartz.json\n",
    "# schwartz.json or pruned_schwartz.json\n",
    "filepath = 'pruned_schwartz.json'\n",
    "\n",
    "data = read_data(filepath)\n",
    "# corpus = extract_corpus(data)\n",
    "# corpusPP = preprocess_corpus(corpus)\n",
    "\n",
    "corpusPP = list(data.text.apply(tp.clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdetok = MosesDetokenizer()\n",
    "\n",
    "brown_files_sent = []\n",
    "for fid in brown.fileids():\n",
    "    brown_files_sent.append([mdetok.detokenize(' '.join(sent).replace('``', '\"').replace(\"''\", '\"').replace('`', \"'\").split(), return_str=True)  for sent in brown.sents(fid)])\n",
    "    \n",
    "brown_natural = [' '.join(bfs) for bfs in brown_files_sent]\n",
    "brown_naturalPP = preprocess_corpus(brown_natural)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "50000\n",
      "done in 13.28s.\n",
      "Fitting NMF for universalism\n",
      "done in 11.24s.\n",
      "Fitting NMF for hedonism\n",
      "done in 9.11s.\n",
      "Fitting NMF for achievement\n",
      "done in 9.13s.\n",
      "Fitting NMF for power\n",
      "done in 5.92s.\n",
      "Fitting NMF for self-direction\n",
      "done in 5.56s.\n",
      "Fitting NMF for benevolence\n",
      "done in 8.53s.\n",
      "Fitting NMF for conformity\n",
      "done in 8.25s.\n",
      "Fitting NMF for tradition\n",
      "done in 5.27s.\n",
      "Fitting NMF for stimulation\n",
      "done in 5.25s.\n",
      "Fitting NMF for security\n",
      "done in 5.26s.\n"
     ]
    }
   ],
   "source": [
    "nmf_list, W_list, tfidf, tfidf_vectorizer = train_corpus(corpusPP, data, [], n_topics=3, betaloss = 'kullback-leibler', bckg_brown = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in NMF model:\n",
      "\u001b[96m\u001b[1muniversalism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0menvironmental - movement - state - marriage - social\n",
      "\u001b[1mTopic #1: \u001b[0mright - environmental - law - social - human\n",
      "\u001b[1mTopic #2: \u001b[0menergy - ecology - peace - use - human\n",
      "\u001b[1mTopic #3: \u001b[0mone - social - may - also - use\n",
      "\n",
      "\u001b[96m\u001b[1mhedonism\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mpain - love - orgasm - one - empathy\n",
      "\u001b[1mTopic #1: \u001b[0mone - happiness - pleasure - social - desire\n",
      "\u001b[1mTopic #2: \u001b[0mmay - one - experience - also - emotion\n",
      "\u001b[1mTopic #3: \u001b[0msocial - one - use - state - also\n",
      "\n",
      "\u001b[96m\u001b[1machievement\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msocial - class - capital - society - labour\n",
      "\u001b[1mTopic #1: \u001b[0mwork - hour - individual - social - goal\n",
      "\u001b[1mTopic #2: \u001b[0mcapital - status - social - human - need\n",
      "\u001b[1mTopic #3: \u001b[0mone - social - state - use - also\n",
      "\n",
      "\u001b[96m\u001b[1mpower\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mpower - use - experiment - milgram - make\n",
      "\u001b[1mTopic #1: \u001b[0mtime - state - wealth - power - collapse\n",
      "\u001b[1mTopic #2: \u001b[0mauthority - power - veto - bill - social\n",
      "\u001b[1mTopic #3: \u001b[0mone - social - state - use - also\n",
      "\n",
      "\u001b[96m\u001b[1mself-direction\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mcreativity - play - creative - intelligence - new\n",
      "\u001b[1mTopic #1: \u001b[0minnovation - idea - unite - intelligence - territory\n",
      "\u001b[1mTopic #2: \u001b[0myes - independence - invention - bully - positive\n",
      "\u001b[1mTopic #3: \u001b[0msocial - one - use - state - also\n",
      "\n",
      "\u001b[96m\u001b[1mbenevolence\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mlaw - truth - ethic - forgiveness - theory\n",
      "\u001b[1mTopic #1: \u001b[0mgood - evil - one - justice - pardon\n",
      "\u001b[1mTopic #2: \u001b[0mmiss - little - little miss - one - moral\n",
      "\u001b[1mTopic #3: \u001b[0msocial - one - use - state - also\n",
      "\n",
      "\u001b[96m\u001b[1mconformity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mgod - church - commandment - politeness - use\n",
      "\u001b[1mTopic #1: \u001b[0mchild - group - behavior - discipline - one\n",
      "\u001b[1mTopic #2: \u001b[0mgod - one - use - name - commandment\n",
      "\u001b[1mTopic #3: \u001b[0msocial - one - state - use - also\n",
      "\n",
      "\u001b[96m\u001b[1mtradition\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mfolklore - christian - sin - one - group\n",
      "\u001b[1mTopic #1: \u001b[0mvirtue - tradition - humility - one - practice\n",
      "\u001b[1mTopic #2: \u001b[0mtemperance - virtue - one - moral - character\n",
      "\u001b[1mTopic #3: \u001b[0msocial - one - state - use - also\n",
      "\n",
      "\u001b[96m\u001b[1mstimulation\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0mfiction - genre - story - often - music\n",
      "\u001b[1mTopic #1: \u001b[0mtourism - travel - million - tourist - international\n",
      "\u001b[1mTopic #2: \u001b[0msport - travel - adventure - exploration - use\n",
      "\u001b[1mTopic #3: \u001b[0msocial - one - state - use - also\n",
      "\n",
      "\u001b[96m\u001b[1msecurity\u001b[0m\n",
      "\u001b[1mTopic #0: \u001b[0msecurity - risk - human - peace - social\n",
      "\u001b[1mTopic #1: \u001b[0mwaste - hygiene - pollution - use - norm\n",
      "\u001b[1mTopic #2: \u001b[0msecurity - reciprocity - social - state - national\n",
      "\u001b[1mTopic #3: \u001b[0mone - social - state - use - also\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# new\n",
    "print(\"\\nTopics in NMF model:\")\n",
    "for i in range(10):\n",
    "    print_top_words(nmf_list, i, tfidf_vectorizer, n_top_words=5, n_topics=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>universalism (0) - word</th>\n",
       "      <th>universalism (0) - score</th>\n",
       "      <th>universalism (1) - word</th>\n",
       "      <th>universalism (1) - score</th>\n",
       "      <th>universalism (2) - word</th>\n",
       "      <th>universalism (2) - score</th>\n",
       "      <th>benevolence (0) - word</th>\n",
       "      <th>benevolence (0) - score</th>\n",
       "      <th>benevolence (1) - word</th>\n",
       "      <th>benevolence (1) - score</th>\n",
       "      <th>...</th>\n",
       "      <th>stimulation (1) - word</th>\n",
       "      <th>stimulation (1) - score</th>\n",
       "      <th>stimulation (2) - word</th>\n",
       "      <th>stimulation (2) - score</th>\n",
       "      <th>self-direction (0) - word</th>\n",
       "      <th>self-direction (0) - score</th>\n",
       "      <th>self-direction (1) - word</th>\n",
       "      <th>self-direction (1) - score</th>\n",
       "      <th>self-direction (2) - word</th>\n",
       "      <th>self-direction (2) - score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>environmental</td>\n",
       "      <td>49.667</td>\n",
       "      <td>right</td>\n",
       "      <td>45.119</td>\n",
       "      <td>energy</td>\n",
       "      <td>36.749</td>\n",
       "      <td>law</td>\n",
       "      <td>33.988</td>\n",
       "      <td>good</td>\n",
       "      <td>22.219</td>\n",
       "      <td>...</td>\n",
       "      <td>tourism</td>\n",
       "      <td>34.625</td>\n",
       "      <td>sport</td>\n",
       "      <td>31.517</td>\n",
       "      <td>creativity</td>\n",
       "      <td>48.524</td>\n",
       "      <td>innovation</td>\n",
       "      <td>20.310</td>\n",
       "      <td>yes</td>\n",
       "      <td>22.895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>movement</td>\n",
       "      <td>40.511</td>\n",
       "      <td>environmental</td>\n",
       "      <td>40.540</td>\n",
       "      <td>ecology</td>\n",
       "      <td>33.340</td>\n",
       "      <td>truth</td>\n",
       "      <td>29.938</td>\n",
       "      <td>evil</td>\n",
       "      <td>21.605</td>\n",
       "      <td>...</td>\n",
       "      <td>travel</td>\n",
       "      <td>13.601</td>\n",
       "      <td>travel</td>\n",
       "      <td>8.751</td>\n",
       "      <td>play</td>\n",
       "      <td>22.123</td>\n",
       "      <td>idea</td>\n",
       "      <td>15.863</td>\n",
       "      <td>independence</td>\n",
       "      <td>14.820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>state</td>\n",
       "      <td>39.030</td>\n",
       "      <td>law</td>\n",
       "      <td>33.303</td>\n",
       "      <td>peace</td>\n",
       "      <td>32.374</td>\n",
       "      <td>ethic</td>\n",
       "      <td>26.576</td>\n",
       "      <td>one</td>\n",
       "      <td>19.961</td>\n",
       "      <td>...</td>\n",
       "      <td>million</td>\n",
       "      <td>7.945</td>\n",
       "      <td>adventure</td>\n",
       "      <td>7.758</td>\n",
       "      <td>creative</td>\n",
       "      <td>21.152</td>\n",
       "      <td>unite</td>\n",
       "      <td>14.226</td>\n",
       "      <td>invention</td>\n",
       "      <td>11.506</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>marriage</td>\n",
       "      <td>35.007</td>\n",
       "      <td>social</td>\n",
       "      <td>31.894</td>\n",
       "      <td>use</td>\n",
       "      <td>28.147</td>\n",
       "      <td>forgiveness</td>\n",
       "      <td>26.164</td>\n",
       "      <td>justice</td>\n",
       "      <td>18.050</td>\n",
       "      <td>...</td>\n",
       "      <td>tourist</td>\n",
       "      <td>7.608</td>\n",
       "      <td>exploration</td>\n",
       "      <td>7.689</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>11.612</td>\n",
       "      <td>intelligence</td>\n",
       "      <td>12.747</td>\n",
       "      <td>bully</td>\n",
       "      <td>10.333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>social</td>\n",
       "      <td>34.964</td>\n",
       "      <td>human</td>\n",
       "      <td>30.453</td>\n",
       "      <td>human</td>\n",
       "      <td>27.457</td>\n",
       "      <td>theory</td>\n",
       "      <td>24.849</td>\n",
       "      <td>pardon</td>\n",
       "      <td>18.045</td>\n",
       "      <td>...</td>\n",
       "      <td>international</td>\n",
       "      <td>7.487</td>\n",
       "      <td>use</td>\n",
       "      <td>6.662</td>\n",
       "      <td>new</td>\n",
       "      <td>10.511</td>\n",
       "      <td>territory</td>\n",
       "      <td>12.589</td>\n",
       "      <td>positive</td>\n",
       "      <td>9.789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>party</td>\n",
       "      <td>33.618</td>\n",
       "      <td>peace</td>\n",
       "      <td>30.217</td>\n",
       "      <td>think</td>\n",
       "      <td>25.110</td>\n",
       "      <td>good</td>\n",
       "      <td>23.249</td>\n",
       "      <td>lie</td>\n",
       "      <td>17.507</td>\n",
       "      <td>...</td>\n",
       "      <td>country</td>\n",
       "      <td>7.279</td>\n",
       "      <td>include</td>\n",
       "      <td>6.363</td>\n",
       "      <td>process</td>\n",
       "      <td>10.126</td>\n",
       "      <td>state</td>\n",
       "      <td>11.556</td>\n",
       "      <td>task</td>\n",
       "      <td>9.681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>samesex</td>\n",
       "      <td>33.164</td>\n",
       "      <td>war</td>\n",
       "      <td>28.521</td>\n",
       "      <td>system</td>\n",
       "      <td>24.941</td>\n",
       "      <td>one</td>\n",
       "      <td>21.965</td>\n",
       "      <td>trust</td>\n",
       "      <td>16.348</td>\n",
       "      <td>...</td>\n",
       "      <td>billion</td>\n",
       "      <td>6.202</td>\n",
       "      <td>game</td>\n",
       "      <td>6.120</td>\n",
       "      <td>theory</td>\n",
       "      <td>10.048</td>\n",
       "      <td>new</td>\n",
       "      <td>11.161</td>\n",
       "      <td>individual</td>\n",
       "      <td>9.346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>green</td>\n",
       "      <td>30.595</td>\n",
       "      <td>state</td>\n",
       "      <td>28.432</td>\n",
       "      <td>one</td>\n",
       "      <td>23.574</td>\n",
       "      <td>natural</td>\n",
       "      <td>19.912</td>\n",
       "      <td>individual</td>\n",
       "      <td>15.752</td>\n",
       "      <td>...</td>\n",
       "      <td>world</td>\n",
       "      <td>5.996</td>\n",
       "      <td>may</td>\n",
       "      <td>6.010</td>\n",
       "      <td>work</td>\n",
       "      <td>9.819</td>\n",
       "      <td>group</td>\n",
       "      <td>11.093</td>\n",
       "      <td>emotion</td>\n",
       "      <td>9.025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>woman</td>\n",
       "      <td>26.753</td>\n",
       "      <td>use</td>\n",
       "      <td>26.748</td>\n",
       "      <td>social</td>\n",
       "      <td>23.401</td>\n",
       "      <td>may</td>\n",
       "      <td>15.696</td>\n",
       "      <td>moral</td>\n",
       "      <td>15.039</td>\n",
       "      <td>...</td>\n",
       "      <td>destination</td>\n",
       "      <td>5.189</td>\n",
       "      <td>also</td>\n",
       "      <td>5.840</td>\n",
       "      <td>also</td>\n",
       "      <td>9.063</td>\n",
       "      <td>curiosity</td>\n",
       "      <td>10.226</td>\n",
       "      <td>yes yes</td>\n",
       "      <td>8.915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>right</td>\n",
       "      <td>26.690</td>\n",
       "      <td>specie</td>\n",
       "      <td>26.609</td>\n",
       "      <td>theory</td>\n",
       "      <td>22.065</td>\n",
       "      <td>natural law</td>\n",
       "      <td>14.742</td>\n",
       "      <td>social</td>\n",
       "      <td>14.720</td>\n",
       "      <td>...</td>\n",
       "      <td>unite</td>\n",
       "      <td>5.111</td>\n",
       "      <td>explorer</td>\n",
       "      <td>5.780</td>\n",
       "      <td>state</td>\n",
       "      <td>8.952</td>\n",
       "      <td>music</td>\n",
       "      <td>9.654</td>\n",
       "      <td>performance</td>\n",
       "      <td>8.694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  60 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  universalism (0) - word  universalism (0) - score universalism (1) - word  \\\n",
       "0           environmental                    49.667                   right   \n",
       "1                movement                    40.511           environmental   \n",
       "2                   state                    39.030                     law   \n",
       "3                marriage                    35.007                  social   \n",
       "4                  social                    34.964                   human   \n",
       "5                   party                    33.618                   peace   \n",
       "6                 samesex                    33.164                     war   \n",
       "7                   green                    30.595                   state   \n",
       "8                   woman                    26.753                     use   \n",
       "9                   right                    26.690                  specie   \n",
       "\n",
       "   universalism (1) - score universalism (2) - word  universalism (2) - score  \\\n",
       "0                    45.119                  energy                    36.749   \n",
       "1                    40.540                 ecology                    33.340   \n",
       "2                    33.303                   peace                    32.374   \n",
       "3                    31.894                     use                    28.147   \n",
       "4                    30.453                   human                    27.457   \n",
       "5                    30.217                   think                    25.110   \n",
       "6                    28.521                  system                    24.941   \n",
       "7                    28.432                     one                    23.574   \n",
       "8                    26.748                  social                    23.401   \n",
       "9                    26.609                  theory                    22.065   \n",
       "\n",
       "  benevolence (0) - word  benevolence (0) - score benevolence (1) - word  \\\n",
       "0                    law                   33.988                   good   \n",
       "1                  truth                   29.938                   evil   \n",
       "2                  ethic                   26.576                    one   \n",
       "3            forgiveness                   26.164                justice   \n",
       "4                 theory                   24.849                 pardon   \n",
       "5                   good                   23.249                    lie   \n",
       "6                    one                   21.965                  trust   \n",
       "7                natural                   19.912             individual   \n",
       "8                    may                   15.696                  moral   \n",
       "9            natural law                   14.742                 social   \n",
       "\n",
       "   benevolence (1) - score             ...             stimulation (1) - word  \\\n",
       "0                   22.219             ...                            tourism   \n",
       "1                   21.605             ...                             travel   \n",
       "2                   19.961             ...                            million   \n",
       "3                   18.050             ...                            tourist   \n",
       "4                   18.045             ...                      international   \n",
       "5                   17.507             ...                            country   \n",
       "6                   16.348             ...                            billion   \n",
       "7                   15.752             ...                              world   \n",
       "8                   15.039             ...                        destination   \n",
       "9                   14.720             ...                              unite   \n",
       "\n",
       "   stimulation (1) - score stimulation (2) - word  stimulation (2) - score  \\\n",
       "0                   34.625                  sport                   31.517   \n",
       "1                   13.601                 travel                    8.751   \n",
       "2                    7.945              adventure                    7.758   \n",
       "3                    7.608            exploration                    7.689   \n",
       "4                    7.487                    use                    6.662   \n",
       "5                    7.279                include                    6.363   \n",
       "6                    6.202                   game                    6.120   \n",
       "7                    5.996                    may                    6.010   \n",
       "8                    5.189                   also                    5.840   \n",
       "9                    5.111               explorer                    5.780   \n",
       "\n",
       "  self-direction (0) - word  self-direction (0) - score  \\\n",
       "0                creativity                      48.524   \n",
       "1                      play                      22.123   \n",
       "2                  creative                      21.152   \n",
       "3              intelligence                      11.612   \n",
       "4                       new                      10.511   \n",
       "5                   process                      10.126   \n",
       "6                    theory                      10.048   \n",
       "7                      work                       9.819   \n",
       "8                      also                       9.063   \n",
       "9                     state                       8.952   \n",
       "\n",
       "  self-direction (1) - word  self-direction (1) - score  \\\n",
       "0                innovation                      20.310   \n",
       "1                      idea                      15.863   \n",
       "2                     unite                      14.226   \n",
       "3              intelligence                      12.747   \n",
       "4                 territory                      12.589   \n",
       "5                     state                      11.556   \n",
       "6                       new                      11.161   \n",
       "7                     group                      11.093   \n",
       "8                 curiosity                      10.226   \n",
       "9                     music                       9.654   \n",
       "\n",
       "  self-direction (2) - word  self-direction (2) - score  \n",
       "0                       yes                      22.895  \n",
       "1              independence                      14.820  \n",
       "2                 invention                      11.506  \n",
       "3                     bully                      10.333  \n",
       "4                  positive                       9.789  \n",
       "5                      task                       9.681  \n",
       "6                individual                       9.346  \n",
       "7                   emotion                       9.025  \n",
       "8                   yes yes                       8.915  \n",
       "9               performance                       8.694  \n",
       "\n",
       "[10 rows x 60 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_pretrained_words(nmf_list, tfidf_vectorizer, word_count=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_pretrained_excel(nmf_list, tfidf_vectorizer, \"ssnmf_theme_words_t3_1109.xlsx\", word_count=-1, anti=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( [nmf_list, tfidf_vectorizer], open( \"pretrained_v3_t3_h10_1409.p\", \"wb\" ) )\n",
    "#pickle.dump( [nmf_list, tfidf_vectorizer], open( \"nmf2_pretrained_pruned_brown.p\", \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
